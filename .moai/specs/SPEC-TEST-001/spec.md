# SPEC-TEST-001: RAG 테스팅 자동화 시스템

## TAG BLOCK
```yaml
spec_id: SPEC-TEST-001
title: RAG 테스팅 자동화 시스템
status: Planned
priority: High
created: 2025-01-24
assigned: workflow-spec
domain: TEST
related_specs: []
estimated_effort: 6주
```

## 환경 (Environment)

### 시스템 개요
대학 규정 관리 시스템의 RAG(검색 증강 생성) 컴포넌트를 자동으로 테스트하고, 다양한 사용자 페르소나를 시뮬레이션하여 답변 품질을 지속적으로 개선하는 자동화 시스템입니다.

### 기술 제약사항
- Python 3.11+ 환경
- Clean Architecture 4계층 유지 (interface/application/domain/infrastructure)
- 기존 RAG 시스템(src/rag/)과 호환
- Claude AI API (anthropic) 통합
- 결과 저장: JSON + 마크다운

### 통합 범위
- 기존 CLI 인터페이스와 연동
- 기존 RAG 파이프라인 활용
- 새로운 자동화 계층 추가

## 가정 (Assumptions)

### 사용자 가정
1. 테스터는 RAG 테스트 프레임워크(.github/prompts/rag-testing.prompt.md)의 내용을 이해하고 있습니다
2. LLM API(Claude)가 안정적으로 제공됩니다
3. 규정 데이터(data/output/*.json)가 최신 상태입니다

### 기술적 가정
1. 기존 RAG 시스템이 정상적으로 동작합니다
2. Clean Architecture의 domain 계층은 순수 Python으로 유지합니다
3. LLM API 호출 비용이 예산 범위 내입니다

### 검증 가능성 가정
1. 모든 테스트 결과는 정량적 메트릭으로 측정 가능합니다
2. 팩트체크는 자동화 가능합니다
3. 의도 추론 정확도는 평가 기준이 명확합니다

## 요구사항 (Requirements) - EARS 형식

### Ubiquitous Requirements (항상 활성)

**REQ-U-001:** 시스템은 모든 테스트 실행 결과를 영구 저장소(data/output/test_sessions/)에 기록해야 한다.

**REQ-U-002:** 시스템은 모든 생성된 페르소나와 쿼리를 추적 가능한 형식으로 저장해야 한다.

**REQ-U-003:** 시스템은 모든 RAG 컴포넌트 동작 로그를 기록해야 한다.

### Event-Driven Requirements (이벤트 기반)

**REQ-E-001:** WHEN 테스트 실행 명령을 수신하면, 시스템은 지정된 페르소나 유형을 선택하고 해당 페르소나에 맞는 쿼리 세트를 생성해야 한다.

**REQ-E-002:** WHEN 쿼리 실행이 완료되면, 시스템은 RAG 컴포넌트별 기여도를 분석하고 기록해야 한다.

**REQ-E-003:** WHEN 팩트체크가 실패하면, 시스템은 재생성을 트리거하고 최대 3회까지 시도해야 한다.

**REQ-E-004:** WHEN 답변 품질 평가가 완료되면, 시스템은 개선 제안을 생성하고 data/output/improvement_plan.json에 저장해야 한다.

**REQ-E-005:** WHEN 멀티턴 대화에서 맥락 단절이 감지되면, 시스템은 이를 실패로 기록하고 근본 원인을 분석해야 한다.

**REQ-E-006:** WHEN 테스트 세션이 완료되면, 시스템은 종합 보고서를 마크다운 형식으로 생성해야 한다.

### State-Driven Requirements (상태 기반)

**REQ-S-001:** IF 의도 추론 정확도가 85% 미만이면, 시스템은 Query Analyzer 개선 제안을 생성해야 한다.

**REQ-S-002:** IF 팩트체크 수행률이 100%가 아니면, 시스템은 이를 실패로 기록해야 한다.

**REQ-S-003:** IF 동적 쿼리 성공률이 80% 미만이면, 시스템은 Phase 5(개선 적용)로 이동해야 한다.

**REQ-S-004:** IF 동일한 실패 유형이 2회 이상 발생하면, 시스템은 5-Why 분석을 수행해야 한다.

**REQ-S-005:** IF 후속 질문 성공률이 85% 미만이면, 시스템은 맥락 관리 로직을 점검해야 한다.

### Unwanted Requirements (금지 사항)

**REQ-W-001:** 시스템은 답변에 "대학마다 다를 수 있습니다" 같은 회피성 표현이 포함된 경우를 성공으로 간주해서는 안 된다.

**REQ-W-002:** 시스템은 팩트체크 없이 답변의 정확성을 단정해서는 안 된다.

**REQ-W-003:** 시스템은 단발성 쿼리로만 테스트하고 멀티턴 대화 테스트를 건너뛰어서는 안 된다.

**REQ-W-004:** 시스템은 domain 계층에 외부 라이브러리(LLM 클라이언트 등)를 직접 의존하게 해서는 안 된다.

### Optional Requirements (선택 사항)

**REQ-O-001:** WHERE 가능하면, 시스템은 테스트 실행을 병렬화하여 실행 시간을 단축해야 한다.

**REQ-O-002:** WHERE 가능하면, 시스템은 이전 테스트 세션의 결과를 참조하여 중복 테스트를 회피해야 한다.

**REQ-O-003:** WHERE 가능하면, 시스템은 웹 대시보드를 통해 테스트 결과를 시각화해야 한다.

## 상세 명세 (Specifications)

### 1. 페르소나 생성자 (PersonaGenerator)

**목적:** 다양한 사용자 유형을 자동으로 생성하여 RAG 시스템의 사용자 커버리지를 극대화

**페르소나 유형 (10종):**
1. 신입생: 학교 시스템에 익숙하지 않음, 비공식적 표현
2. 재학생 (3학년): 구체적 정보 필요, 졸업 준비
3. 대학원생: 연구/논문 중심, 전문적 질문
4. 신임 교수: 제도 파악 필요, 공식적 표현
5. 정교수: 세부 규정 확인, 권리 주장
6. 신입 직원: 복무규정 파악, 혜택 문의
7. 과장급 직원: 부서 운영, 예산 관련
8. 학부모: 자녀 관련 정보, 외부 시선
9. 어려운 상황의 학생: 감정적, 급한 상황
10. 불만있는 구성원: 권리 주장, 신고 의향

**난이도 분포:**
- 쉬움 30%: 단일 규정, 명확한 키워드
- 중간 40%: 여러 규정 연계, 조건부 답변 필요
- 어려움 30%: 모호한 표현, 감정적, 복합 질문

**쿼리 유형:**
- 사실 확인, 절차 질문, 자격 확인, 비교 질문
- 모호한 질문, 감정 표현, 복합 질문, 은어/축약어

### 2. 쿼리 생성자 (QueryGenerator)

**목적:** 페르소나별로 다양한 난이도와 유형의 쿼리를 자동 생성

**생성 규칙:**
- 각 페르소나당 최소 3개 쿼리 (쉬움 1개, 중간 1개, 어려움 1개)
- 멀티턴 시나리오당 최소 5턴 후속 질문
- 의도 추론 검증을 위한 3단계 의도 분석 포함

**의도 추론 3단계:**
1. 표면적 의도: 쿼리에서 직접 표현된 것
2. 숨겨진 의도: 쿼리 뒤에 있는 실제 니즈
3. 행동 의도: 사용자가 궁극적으로 하고 싶은 행동

### 3. 멀티턴 시뮬레이터 (MultiTurnSimulator)

**목적:** 최소 3턴 이상의 후속 질문 시나리오를 자동으로 생성하고 실행

**후속 질문 유형:**
- 구체화: 더 자세한 정보 요청
- 관련 확장: 연관 주제로 확장
- 예외 확인: 특수 상황 문의
- 절차 심화: 구체적 절차 문의
- 조건 변경: 다른 조건에서의 적용
- 확인 질문: 이해 확인
- 되돌아가기: 이전 주제로 복귀
- 비교 요청: 옵션 비교

**맥락 유지 검증:**
- 각 Turn별 이전 맥락 연결 확인
- 의도 진화 추적
- 암묵적 정보 기억 확인
- 핵심 니즈 파악 확인

### 4. 자동 팩트체커 (AutoFactChecker)

**목적:** 모든 답변의 핵심 주장을 자동으로 검증

**팩트체크 절차:**
1. 답변에서 핵심 주장 3개 추출
2. 각 주장에 대해 검증 쿼리 자동 생성
3. 검색 결과와 답변 내용 대조
4. 검증 결과 기록

**검증 기준:**
- 규정 조항 번호 정확성
- 수치/기한 정확성
- 출처 존재 여부
- 할루시네이션 여부

### 5. 품질 평가자 (QualityEvaluator)

**목적:** 답변 품질을 다차원으로 평가하고 점수화

**평가 항목 (배점 5.0 만점):**
- 정확성 1.0: 규정 내용과 일치 (팩트체크 필수)
- 완전성 1.0: 질문의 모든 측면에 답변
- 관련성 1.0: 질문 의도에 맞는 답변
- 출처 명시 1.0: 규정명/조항 인용
- 실용성 0.5: 기한/서류/담당부서 포함
- 행동 가능성 0.5: 사용자가 바로 행동 가능

**성공/실패 판정:**
- 성공: 점수 ≥ 4.0 AND 팩트체크 모두 통과
- 부분성공(=실패): 점수 3.0~3.9 OR 팩트체크 1개 실패
- 실패: 점수 < 3.0 OR 팩트체크 2개 이상 실패

### 6. 개선 적용자 (ImprovementApplier)

**목적:** 실패 분석 결과를 바탕으로 자동으로 개선 제안을 적용

**제안 유형별 처리:**
- intent: data/config/intents.json 패치
- synonym: data/config/synonyms.json 패치
- code_pattern: src/rag/infrastructure/query_analyzer.py 수정
- code_weight: 가중치 조정
- hyde_condition: HyDE 발동 조건 수정
- query_expansion: 쿼리 확장 규칙/프롬프트 수정

**5-Why 분석:**
실패한 쿼리에 대해 근본 원인을 5단계로 분석하여 RAG 컴포넌트별 개선 방향 도출

### 7. 테스트 리포트 생성자 (TestReportGenerator)

**목적:** 테스트 세션 결과를 종합하여 마크다운 보고서 생성

**보고서 항목:**
1. 시작 상태 (정적 테스트 통과율, 시스템 상태, RAG 설정)
2. RAG 컴포넌트 단위 테스트 결과
3. 동적 테스트 결과 (페르소나, 난이도 분포, 성공률)
4. 멀티턴 테스트 결과 (시나리오 수, 맥락 유지율)
5. 답변 품질 점수 (항목별 평균, 실패 유형 분포)
6. RAG 컴포넌트 기여도 종합
7. 적용된 개선 내역
8. 최종 상태 및 다음 단계 권장사항

### 8. RAG 컴포넌트 기여도 분석

**분석 대상 컴포넌트:**
- Self-RAG: 검색 필요성 판단, 결과 관련성 평가
- HyDE: 모호한 쿼리 가상 문서 생성
- Corrective RAG: 검색 결과 품질 평가 및 재검색
- Hybrid Search: BM25 + Dense 융합
- BGE Reranker: 검색 결과 재정렬
- Query Analyzer: 인텐트 분석 및 쿼리 확장
- Dynamic Query Expansion: LLM 기반 동적 확장
- Fact Check: 답변 팩트체크 및 재생성

**기여도 평가 기준:**
- 긍정적 기여: 품질 개선에 기여
- 부정적 영향: 품질 저하 원인
- 무영향: 발동 안됨 또는 효과 없음

## 추적성 (Traceability)

**관련 문서:**
- 프로젝트 개요: .moai/project/product.md
- 프로젝트 구조: .moai/project/structure.md
- 기술 스택: .moai/project/tech.md
- RAG 테스트 프롬프트: .github/prompts/rag-testing.prompt.md

**구현 범위:**
- 새로운 자동화 계층: src/rag/automation/
- 인터페이스 확장: src/rag/interface/automation_cli.py
- 테스트 결과 저장소: data/output/test_sessions/

**통합 지점:**
- 기존 SearchUseCase와 통합
- 기존 LLMClient 활용
- 기존 QueryAnalyzer, FactChecker와 연동
