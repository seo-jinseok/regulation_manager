"""
Keyword Extractor for Regulation Documents.

Automatically extracts key terms from regulation JSON files
to improve search accuracy and build regulation_keywords.json.
"""

import json
import re
from collections import Counter
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional


@dataclass
class RegulationKeywords:
    """Keywords extracted from a single regulation."""

    rule_code: str
    name: str
    keywords: List[str] = field(default_factory=list)
    context: str = "general"  # student, employee, general
    chapter_keywords: Dict[str, List[str]] = field(default_factory=dict)


@dataclass
class ExtractionResult:
    """Result of keyword extraction."""

    total_regulations: int
    total_keywords: int
    regulations: Dict[str, RegulationKeywords] = field(default_factory=dict)


class KeywordExtractor:
    """
    Extracts keywords from regulation JSON files.

    Identifies important terms from regulation names, chapter titles,
    and article titles to improve search accuracy.
    """

    # Context detection patterns
    STUDENT_PATTERNS = [
        r"í•™ìƒ",
        r"í•™ì¹™",
        r"í•™ì‚¬",
        r"ë“±ë¡",
        r"ì¡¸ì—…",
        r"íœ´í•™",
        r"ìž¥í•™",
        r"ìˆ˜ê°•",
        r"ì„±ì ",
        r"í•™ìœ„",
        r"ìž…í•™",
        r"ìž¬í•™",
        r"í•™ë…„",
    ]
    EMPLOYEE_PATTERNS = [
        r"êµì›",
        r"ì§ì›",
        r"ì¸ì‚¬",
        r"ë³´ìˆ˜",
        r"ê¸‰ì—¬",
        r"í‡´ì§",
        r"ë³µë¬´",
        r"ì—°êµ¬ë…„",
        r"ìŠ¹ì§„",
        r"í˜¸ë´‰",
        r"ê·¼ë¡œ",
        r"ë…¸ì‚¬",
    ]

    # Stopwords to exclude
    STOPWORDS = {
        "ì œ",
        "ì¡°",
        "í•­",
        "í˜¸",
        "ëª©",
        "ë‹¤ìŒ",
        "ê°",
        "í•´ë‹¹",
        "ê²½ìš°",
        "ê·œì •",
        "ê´€í•œ",
        "ìœ„í•œ",
        "ë”°ë¥¸",
        "ëŒ€í•œ",
        "ì˜í•œ",
        "ìžˆëŠ”",
        "í•˜ëŠ”",
        "ë˜ëŠ”",
        "í•œë‹¤",
        "ìžˆë‹¤",
        "ëœë‹¤",
        "ìˆ˜",
        "ê²ƒ",
        "ë“±",
        "ë°",
        "ë˜ëŠ”",
        "ì´",
        "ê·¸",
        "ì €",
        "ìœ„",
        "ì•„ëž˜",
        "ê¸°íƒ€",
    }

    def __init__(
        self,
        json_path: Optional[str] = None,
        output_path: Optional[str] = None,
    ):
        """
        Initialize keyword extractor.

        Args:
            json_path: Path to regulation JSON file.
            output_path: Path to save extracted keywords.
        """
        self._json_path = json_path or self._default_json_path()
        self._output_path = output_path or self._default_output_path()

    def _default_json_path(self) -> str:
        """Get default regulation JSON path."""
        from ..config import get_config
        return str(get_config().json_path_resolved)

    def _default_output_path(self) -> str:
        """Get default output path for extracted keywords."""
        from ..config import get_config
        return str(get_config().regulation_keywords_path_resolved)

    def extract_keywords(self) -> ExtractionResult:
        """
        Extract keywords from regulation JSON.

        Returns:
            ExtractionResult with extracted keywords.
        """
        path = Path(self._json_path)
        if not path.exists():
            raise FileNotFoundError(f"Regulation JSON not found: {path}")

        data = json.loads(path.read_text(encoding="utf-8"))
        # SCHEMA_REFERENCE.mdì— ë”°ë¥´ë©´ ìµœìƒìœ„ ë¦¬ìŠ¤íŠ¸ëŠ” 'docs'
        docs = data.get("docs", [])

        result = ExtractionResult(
            total_regulations=len(
                [d for d in docs if d.get("doc_type") == "regulation"]
            ),
            total_keywords=0,
        )

        for doc in docs:
            # ê·œì • íƒ€ìž…ì´ ì•„ë‹Œ ê²ƒì€ ê±´ë„ˆëœ€ (ëª©ì°¨, ìƒ‰ì¸ ë“±)
            if doc.get("doc_type") != "regulation":
                continue

            # metadataì—ì„œ rule_code ì¶”ì¶œ
            metadata = doc.get("metadata", {})
            rule_code = metadata.get("rule_code", "")
            title = doc.get("title", "")

            if not rule_code or not title:
                continue

            # Extract keywords from regulation
            keywords = self._extract_from_regulation(doc)
            context = self._detect_context(title, keywords)

            reg_keywords = RegulationKeywords(
                rule_code=rule_code,
                name=title,
                keywords=keywords,
                context=context,
            )

            result.regulations[rule_code] = reg_keywords
            result.total_keywords += len(keywords)

        return result

    def _extract_from_regulation(self, doc: dict) -> List[str]:
        """Extract keywords from a single regulation (Document object)."""
        all_terms: List[str] = []

        # From title
        title = doc.get("title", "")
        all_terms.extend(self._extract_nouns(title))

        # Recursive extraction from nodes in 'content' and 'addenda'
        def extract_from_nodes(nodes: List[dict]):
            for node in nodes:
                node_title = node.get("title", "")
                if node_title:
                    all_terms.extend(self._extract_nouns(node_title))

                # Also consider text if it's short (like a summary)
                text = node.get("text", "")
                if text and len(text) < 100:
                    all_terms.extend(self._extract_nouns(text))

                children = node.get("children", [])
                if children:
                    extract_from_nodes(children)

        extract_from_nodes(doc.get("content", []))
        extract_from_nodes(doc.get("addenda", []))

        # Count and filter
        counter = Counter(all_terms)
        keywords = [
            term
            for term, count in counter.most_common(30)
            if count >= 1 and len(term) >= 2
        ]

        return keywords[:20]  # Top 20 keywords

    def _extract_nouns(self, text: str) -> List[str]:
        """Extract noun-like terms from text."""
        if not text:
            return []

        # Remove parentheses content
        text = re.sub(r"\([^)]*\)", "", text)
        text = re.sub(r"ã€Œ[^ã€]*ã€", "", text)
        text = re.sub(r"ì œ\d+ì¡°(ì˜\d+)?", "", text)
        text = re.sub(r"ì œ\d+[í•­í˜¸ëª©ìž¥íŽ¸ì ˆ]", "", text)

        # Split by non-Korean characters
        tokens = re.findall(r"[ê°€-íž£]+", text)

        # Filter stopwords and short tokens
        filtered = [t for t in tokens if t not in self.STOPWORDS and len(t) >= 2]

        return filtered

    def _detect_context(self, name: str, keywords: List[str]) -> str:
        """Detect context (student/employee/general) from keywords."""
        text = name + " " + " ".join(keywords)

        student_score = sum(
            1 for pattern in self.STUDENT_PATTERNS if re.search(pattern, text)
        )
        employee_score = sum(
            1 for pattern in self.EMPLOYEE_PATTERNS if re.search(pattern, text)
        )

        if student_score > employee_score:
            return "student"
        elif employee_score > student_score:
            return "employee"
        return "general"

    def save_keywords(self, result: ExtractionResult) -> str:
        """
        Save extracted keywords to JSON file.

        Args:
            result: ExtractionResult to save.

        Returns:
            Path to saved file.
        """
        output = {
            "version": "1.0.0",
            "total_regulations": result.total_regulations,
            "total_keywords": result.total_keywords,
            "regulations": {
                rule_code: {
                    "name": reg.name,
                    "keywords": reg.keywords,
                    "context": reg.context,
                }
                for rule_code, reg in result.regulations.items()
            },
        }

        path = Path(self._output_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(
            json.dumps(output, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

        return str(path)

    def format_summary(self, result: ExtractionResult) -> str:
        """Format extraction result as readable string."""
        lines = [
            "=" * 60,
            "ê·œì • í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼",
            "=" * 60,
            f"ì´ ê·œì • ìˆ˜: {result.total_regulations}",
            f"ì´ í‚¤ì›Œë“œ ìˆ˜: {result.total_keywords}",
            "-" * 60,
        ]

        # Context breakdown
        contexts = {"student": 0, "employee": 0, "general": 0}
        for reg in result.regulations.values():
            contexts[reg.context] = contexts.get(reg.context, 0) + 1

        lines.append(f"í•™ìƒ ê´€ë ¨: {contexts['student']}ê°œ ê·œì •")
        lines.append(f"êµì§ì› ê´€ë ¨: {contexts['employee']}ê°œ ê·œì •")
        lines.append(f"ì¼ë°˜: {contexts['general']}ê°œ ê·œì •")
        lines.append("=" * 60)

        return "\n".join(lines)

    def format_details(self, result: ExtractionResult, limit: int = 10) -> str:
        """Format detailed keyword list."""
        lines = []
        for i, (rule_code, reg) in enumerate(result.regulations.items()):
            if i >= limit:
                lines.append(f"\n... and {len(result.regulations) - limit} more")
                break
            context_icon = {"student": "ðŸŽ“", "employee": "ðŸ‘”", "general": "ðŸ“‹"}.get(
                reg.context, "ðŸ“‹"
            )
            lines.append(f"\n{context_icon} [{rule_code}] {reg.name}")
            lines.append(f"   í‚¤ì›Œë“œ: {', '.join(reg.keywords[:10])}")
        return "\n".join(lines)
