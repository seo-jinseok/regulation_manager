"""
Unified CLI Interface for Regulation Manager.

Provides a single entry point for all regulation management tasks:
- convert: HWP â†’ JSON conversion
- sync: Database synchronization
- search: Regulation search
- ask: LLM-powered Q&A
- status: Sync status check
- reset: Database reset
- serve: Start Web UI or MCP Server

Usage:
    uv run regulation convert "ê·œì •ì§‘.hwp"
    uv run regulation search "êµì› ì—°êµ¬ë…„"
    uv run regulation ask "íœ´í•™ ì ˆì°¨"
    uv run regulation serve --web
"""

import argparse
import os
import sys
from typing import Optional

# Load .env file
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass


def _get_default_llm_settings():
    """Get default LLM settings from centralized config."""
    from ..config import get_config

    config = get_config()
    return (
        config.llm_providers,
        config.llm_provider,
        config.llm_model,
        config.llm_base_url,
    )


def _add_convert_parser(subparsers):
    """Add convert subcommand parser."""
    convert_providers = [
        "openai",
        "gemini",
        "openrouter",
        "ollama",
        "lmstudio",
        "local",
        "mlx",
    ]
    default_provider = os.getenv("LLM_PROVIDER") or "openai"
    if default_provider not in convert_providers:
        default_provider = "openai"
    default_model = os.getenv("LLM_MODEL") or None
    default_base_url = os.getenv("LLM_BASE_URL") or None

    parser = subparsers.add_parser(
        "convert",
        help="HWPX íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜",
        description="HWPX ê·œì •ì§‘ì„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "input_path",
        type=str,
        help="HWPX íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/output",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/output)",
    )
    parser.add_argument(
        "--use-llm",
        action="store_true",
        help="LLM ì „ì²˜ë¦¬ í™œì„±í™”",
    )
    parser.add_argument(
        "--provider",
        type=str,
        default=default_provider,
        choices=convert_providers,
        help="LLM í”„ë¡œë°”ì´ë”",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=default_model,
        help="LLM ëª¨ë¸",
    )
    parser.add_argument(
        "--base-url",
        type=str,
        default=default_base_url,
        help="LLM API URL (ë¡œì»¬ ì„œë²„ìš©)",
    )
    parser.add_argument(
        "--allow-llm-fallback",
        action="store_true",
        help="LLM ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ í´ë°± í—ˆìš©",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ë³€í™˜",
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default=".cache",
        help="ìºì‹œ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥",
    )
    parser.add_argument(
        "--no-enhance-rag",
        action="store_false",
        dest="enhance_rag",
        help="RAG ìµœì í™” ë¹„í™œì„±í™”",
    )
    parser.add_argument(
        "--hwpx",
        action="store_true",
        help="HWPX ì§ì ‘ íŒŒì‹± ì‚¬ìš© (HTML/Markdown ë³€í™˜ ê³¼ì •ì„ ê±´ë„ˆë›°ì–´ ì •í™•ë„ í–¥ìƒ)",
    )
    parser.set_defaults(enhance_rag=True)


def _add_sync_parser(subparsers):
    """Add sync subcommand parser."""
    parser = subparsers.add_parser(
        "sync",
        help="ê·œì • ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™”",
        description="JSON íŒŒì¼ì„ ChromaDBì— ë™ê¸°í™”í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "json_path",
        type=str,
        help="ê·œì •ì§‘ JSON íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="ì „ì²´ ì¬ë™ê¸°í™” (ê¸°ë³¸: ì¦ë¶„ ë™ê¸°í™”)",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )
    parser.add_argument(
        "--extract-keywords",
        action="store_true",
        help="ë™ê¸°í™” í›„ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ (regulation_keywords.json ê°±ì‹ )",
    )


def _add_search_parser(subparsers):
    """Add search subcommand parser."""

    # Get default settings
    providers, default_provider, default_model, default_base_url = (
        _get_default_llm_settings()
    )

    parser = subparsers.add_parser(
        "search",
        help="ê·œì • ê²€ìƒ‰ (ìë™ìœ¼ë¡œ ë‹µë³€ ìƒì„± ë˜ëŠ” ë¬¸ì„œ ê²€ìƒ‰)",
        description="ì§ˆë¬¸ì´ë©´ AI ë‹µë³€ì„, í‚¤ì›Œë“œë©´ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. (í˜¹ì€ -a/-q ì˜µì…˜ìœ¼ë¡œ ê°•ì œ)",
    )
    parser.add_argument(
        "query",
        type=str,
        nargs="?",
        help="ê²€ìƒ‰ ì¿¼ë¦¬ ë˜ëŠ” ì§ˆë¬¸",
    )
    parser.add_argument(
        "-n",
        "--top-k",
        type=int,
        default=5,
        help="ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸: 5)",
    )
    parser.add_argument(
        "--include-abolished",
        action="store_true",
        help="íì§€ ê·œì • í¬í•¨ (ê²€ìƒ‰ ëª¨ë“œì¼ ë•Œë§Œ ìœ íš¨)",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )
    parser.add_argument(
        "--no-rerank",
        action="store_true",
        help="BGE Reranker ë¹„í™œì„±í™”",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ì •ë³´ ì¶œë ¥",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥",
    )
    parser.add_argument(
        "--feedback",
        action="store_true",
        help="ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°± ë‚¨ê¸°ê¸° (ì¸í„°ë™í‹°ë¸Œ)",
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="ëŒ€í™”í˜• ëª¨ë“œë¡œ ì—°ì† ì§ˆì˜",
    )

    # Unified specific arguments
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        "-a",
        "--answer",
        action="store_true",
        help="AI ë‹µë³€ ìƒì„± ê°•ì œ (Ask ëª¨ë“œ)",
    )
    mode_group.add_argument(
        "-q",
        "--quick",
        action="store_true",
        help="ë¬¸ì„œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰ (Search ëª¨ë“œ)",
    )

    # LLM options (for answer mode)
    parser.add_argument(
        "--provider",
        type=str,
        default=default_provider,
        choices=providers,
        help="LLM í”„ë¡œë°”ì´ë” (ë‹µë³€ ìƒì„± ì‹œ)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=default_model,
        help="ëª¨ë¸ ì´ë¦„",
    )
    parser.add_argument(
        "--base-url",
        type=str,
        default=default_base_url,
        help="ë¡œì»¬ ì„œë²„ URL",
    )
    parser.add_argument(
        "--show-sources",
        action="store_true",
        help="ê´€ë ¨ ê·œì • ì „ë¬¸ ì¶œë ¥ (ë‹µë³€ ìƒì„± ì‹œ)",
    )

    # Tool calling is now DEFAULT (FunctionGemma for routing, base LLM for answers)
    parser.add_argument(
        "--no-tools",
        action="store_true",
        help="Tool Calling ë¹„í™œì„±í™” (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©)",
    )
    parser.add_argument(
        "--tool-mode",
        type=str,
        choices=["auto", "mlx", "openai", "ollama"],
        default="auto",
        help="Tool Calling ë°±ì—”ë“œ (auto: OpenAI API ìš°ì„ , mlx: Apple Silicon Experimental)",
    )


def _add_ask_parser(subparsers):
    """Add ask subcommand parser (Legacy Wrapper)."""
    providers, default_provider, default_model, default_base_url = (
        _get_default_llm_settings()
    )

    parser = subparsers.add_parser(
        "ask",
        help="ê·œì • ì§ˆë¬¸ (search -aì™€ ë™ì¼)",
        description="LLMì„ ì‚¬ìš©í•˜ì—¬ ê·œì •ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "question",
        type=str,
        help="ì§ˆë¬¸",
    )
    parser.add_argument(
        "-n",
        "--top-k",
        type=int,
        default=5,
        help="ì°¸ê³  ê·œì • ìˆ˜",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )
    parser.add_argument(
        "--provider",
        type=str,
        default=default_provider,
        choices=providers,
        help="LLM í”„ë¡œë°”ì´ë”",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=default_model,
        help="ëª¨ë¸ ì´ë¦„",
    )
    parser.add_argument(
        "--base-url",
        type=str,
        default=default_base_url,
        help="ë¡œì»¬ ì„œë²„ URL",
    )
    parser.add_argument(
        "--show-sources",
        action="store_true",
        help="ê´€ë ¨ ê·œì • ì „ë¬¸ ì¶œë ¥",
    )
    parser.add_argument(
        "--no-rerank",
        action="store_true",
        help="BGE Reranker ë¹„í™œì„±í™”",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ì •ë³´ ì¶œë ¥",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥",
    )
    parser.add_argument(
        "--feedback",
        action="store_true",
        help="ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°± ë‚¨ê¸°ê¸° (ì¸í„°ë™í‹°ë¸Œ)",
    )


def _add_status_parser(subparsers):
    """Add status subcommand parser."""
    parser = subparsers.add_parser(
        "status",
        help="ë™ê¸°í™” ìƒíƒœ í™•ì¸",
        description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” ìƒíƒœë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )


def _add_reset_parser(subparsers):
    """Add reset subcommand parser."""
    parser = subparsers.add_parser(
        "reset",
        help="ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”",
        description="ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--confirm",
        action="store_true",
        required=True,
        help="ì´ˆê¸°í™” í™•ì¸ (í•„ìˆ˜)",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )


def _add_serve_parser(subparsers):
    """Add serve subcommand parser."""
    parser = subparsers.add_parser(
        "serve",
        help="ì„œë²„ ì‹œì‘ (Web UI ë˜ëŠ” MCP)",
        description="Gradio Web UI ë˜ëŠ” MCP Serverë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.",
    )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--web",
        action="store_true",
        help="Gradio Web UI ì‹œì‘",
    )
    group.add_argument(
        "--mcp",
        action="store_true",
        help="MCP Server ì‹œì‘",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=7860,
        help="Web UI í¬íŠ¸ (ê¸°ë³¸: 7860)",
    )
    parser.add_argument(
        "--share",
        action="store_true",
        help="Web UI ê³µê°œ ë§í¬ ìƒì„± (Gradio share)",
    )


def _add_evaluate_parser(subparsers):
    """Add evaluate subcommand parser."""
    parser = subparsers.add_parser(
        "evaluate",
        help="RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€",
        description="í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="data/config/evaluation_dataset.json",
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ",
    )
    parser.add_argument(
        "--category",
        type=str,
        default=None,
        help="íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í‰ê°€",
    )
    parser.add_argument(
        "-n",
        "--top-k",
        type=int,
        default=5,
        help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 5)",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ê²°ê³¼ ì¶œë ¥",
    )


def _add_extract_keywords_parser(subparsers):
    """Add extract-keywords subcommand parser."""
    parser = subparsers.add_parser(
        "extract-keywords",
        help="ê·œì •ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ",
        description="ê·œì • JSONì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--json-path",
        type=str,
        default="data/output/ê·œì •ì§‘.json",
        help="ê·œì • JSON íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="data/config/regulation_keywords.json",
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì €ì¥í•˜ì§€ ì•Šê³  ê²°ê³¼ë§Œ í‘œì‹œ",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ê²°ê³¼ ì¶œë ¥",
    )


def _add_feedback_parser(subparsers):
    """Add feedback subcommand parser."""
    parser = subparsers.add_parser(
        "feedback",
        help="í”¼ë“œë°± í†µê³„ í™•ì¸",
        description="ìˆ˜ì§‘ëœ í”¼ë“œë°± í†µê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--clear",
        action="store_true",
        help="ëª¨ë“  í”¼ë“œë°± ì‚­ì œ",
    )


def _add_analyze_parser(subparsers):
    """Add analyze subcommand parser."""
    parser = subparsers.add_parser(
        "analyze",
        help="í”¼ë“œë°± ê¸°ë°˜ ê°œì„  ì œì•ˆ",
        description="í”¼ë“œë°±ì„ ë¶„ì„í•˜ì—¬ ê°œì„  ì‚¬í•­ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ê²°ê³¼ ì¶œë ¥",
    )


def _add_quality_parser(subparsers):
    """Add quality subcommand parser for RAGAS-based evaluation."""
    parser = subparsers.add_parser(
        "quality",
        help="RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ (RAGAS LLM-as-Judge)",
        description="RAGAS í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•œ LLM-as-Judge í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--judge-model",
        default=os.getenv("RAG_JUDGE_MODEL", "gpt-4o"),
        help="Judge LLM ëª¨ë¸ (ê¸°ë³¸: gpt-4o)",
    )
    parser.add_argument(
        "--no-ragas",
        action="store_true",
        help="RAGAS ì‚¬ìš© ì•ˆ í•¨ (ëª¨ì˜ í‰ê°€)",
    )
    parser.add_argument(
        "--output-dir",
        default="data/evaluations",
        help="í‰ê°€ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í„°ë¦¬",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )

    # Subcommands for quality
    quality_subparsers = parser.add_subparsers(dest="quality_cmd", title="í‰ê°€ ëª…ë ¹ì–´")

    # quality baseline
    baseline_parser = quality_subparsers.add_parser(
        "baseline",
        help="ê¸°ì¤€ì„  í‰ê°€ ì‹¤í–‰ (ëª¨ë“  í˜ë¥´ì†Œë‚˜)",
    )
    baseline_parser.add_argument(
        "--queries-per-persona",
        type=int,
        default=5,
        help="í˜ë¥´ì†Œë‚˜ë‹¹ ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸: 5)",
    )
    baseline_parser.add_argument(
        "--topic",
        help="íŠ¹ì • ì£¼ì œë¡œë§Œ í…ŒìŠ¤íŠ¸",
    )
    baseline_parser.add_argument(
        "-n",
        "--top-k",
        type=int,
        default=5,
        help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸: 5)",
    )

    # quality persona
    persona_parser = quality_subparsers.add_parser(
        "persona",
        help="íŠ¹ì • í˜ë¥´ì†Œë‚˜ë¡œ í‰ê°€",
    )
    persona_parser.add_argument(
        "--id",
        required=True,
        choices=[
            "freshman",
            "graduate",
            "professor",
            "staff",
            "parent",
            "international",
        ],
        help="í˜ë¥´ì†Œë‚˜ ID",
    )
    persona_parser.add_argument(
        "--count",
        type=int,
        default=10,
        help="ìƒì„±í•  ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸: 10)",
    )
    persona_parser.add_argument("--topic", help="íŠ¹ì • ì£¼ì œ")
    persona_parser.add_argument(
        "-n",
        "--top-k",
        type=int,
        default=5,
        help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜",
    )

    # quality synthetic
    synthetic_parser = quality_subparsers.add_parser(
        "synthetic",
        help="í•©ì„± ë°ì´í„° ìƒì„±",
    )
    synthetic_parser.add_argument(
        "--count",
        type=int,
        default=50,
        help="ìƒì„±í•  ì§ˆë¬¸ ìˆ˜ (ê¸°ë³¸: 50)",
    )
    synthetic_parser.add_argument(
        "--difficulty",
        choices=["easy", "medium", "hard", "mixed"],
        default="mixed",
        help="ë‚œì´ë„ (ê¸°ë³¸: mixed)",
    )
    synthetic_parser.add_argument(
        "--scenarios",
        action="store_true",
        help="ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ëª¨ë“œ",
    )
    synthetic_parser.add_argument(
        "--regulation",
        default="í•™ì¹™",
        help="ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±í•  ê·œì • (ê¸°ë³¸: í•™ì¹™)",
    )

    # quality stats
    stats_parser = quality_subparsers.add_parser(
        "stats",
        help="í‰ê°€ í†µê³„ í™•ì¸",
    )
    stats_parser.add_argument(
        "--days",
        type=int,
        help="ìµœê·¼ Nì¼ê°„ í†µê³„ë§Œ",
    )

    # quality dashboard
    quality_subparsers.add_parser(
        "dashboard",
        help="Gradio í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ì‹¤í–‰",
    )

    # P2: quality run - ì „ì²´ í‰ê°€ ì‹¤í–‰ (BatchEvaluationExecutor + ProgressReporter)
    run_parser = quality_subparsers.add_parser(
        "run",
        help="ì „ì²´ í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬, ì§„í–‰ë¥  ì¶”ì )",
        description="BatchEvaluationExecutorë¥¼ ì‚¬ìš©í•œ ì „ì²´ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.",
    )
    run_parser.add_argument(
        "--personas",
        "-p",
        nargs="+",
        help="íŠ¹ì • í˜ë¥´ì†Œë‚˜ë§Œ í…ŒìŠ¤íŠ¸ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
    )
    run_parser.add_argument(
        "--queries-per-persona",
        "-q",
        type=int,
        default=25,
        help="í˜ë¥´ì†Œë‚˜ë‹¹ ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸: 25)",
    )
    run_parser.add_argument(
        "--batch-size",
        "-b",
        type=int,
        default=5,
        help="API ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 5)",
    )
    run_parser.add_argument(
        "--session-id",
        "-s",
        help="ì„¸ì…˜ ID (ì§€ì •í•˜ë©´ í•´ë‹¹ ì„¸ì…˜ ì¬ê°œ)",
    )
    run_parser.add_argument(
        "--output",
        "-o",
        help="í‰ê°€ ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼",
    )
    run_parser.add_argument(
        "--no-checkpoint",
        action="store_true",
        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹„í™œì„±í™”",
    )

    # P2: quality resume - ì¤‘ë‹¨ëœ ì„¸ì…˜ ì¬ê°œ
    resume_parser = quality_subparsers.add_parser(
        "resume",
        help="ì¤‘ë‹¨ëœ í‰ê°€ ì„¸ì…˜ ì¬ê°œ",
        description="CheckpointManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë‹¨ëœ í‰ê°€ë¥¼ ì¬ê°œí•©ë‹ˆë‹¤.",
    )
    resume_parser.add_argument(
        "--session-id",
        "-s",
        help="ì¬ê°œí•  ì„¸ì…˜ ID (ìƒëµ ì‹œ ê°€ì¥ ìµœê·¼ ì¤‘ë‹¨ ì„¸ì…˜)",
    )
    resume_parser.add_argument(
        "--list",
        "-l",
        action="store_true",
        help="ì¬ê°œ ê°€ëŠ¥í•œ ì„¸ì…˜ ëª©ë¡ í‘œì‹œ",
    )

    # P2: quality generate-spec - ì‹¤íŒ¨ íŒ¨í„´ì—ì„œ SPEC ìƒì„±
    spec_parser = quality_subparsers.add_parser(
        "generate-spec",
        help="í‰ê°€ ì‹¤íŒ¨ íŒ¨í„´ì—ì„œ SPEC ë¬¸ì„œ ìƒì„±",
        description="FailureClassifier + SPECGeneratorë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œì„  SPECì„ ìƒì„±í•©ë‹ˆë‹¤.",
    )
    spec_parser.add_argument(
        "--session-id",
        "-s",
        help="íŠ¹ì • ì„¸ì…˜ì˜ ì‹¤íŒ¨ íŒ¨í„´ ì‚¬ìš©",
    )
    spec_parser.add_argument(
        "--output",
        "-o",
        help="SPEC ë¬¸ì„œ ì¶œë ¥ íŒŒì¼",
    )
    spec_parser.add_argument(
        "--threshold",
        type=float,
        default=0.6,
        help="ì‹¤íŒ¨ë¡œ ê°„ì£¼í•  ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸: 0.6)",
    )

    # P2: quality status - ì„¸ì…˜ ìƒíƒœ í™•ì¸
    status_parser = quality_subparsers.add_parser(
        "status",
        help="í‰ê°€ ì„¸ì…˜ ìƒíƒœ í™•ì¸",
        description="ì§„í–‰ ì¤‘ì¸ í‰ê°€ ì„¸ì…˜ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
    )
    status_parser.add_argument(
        "--session-id",
        "-s",
        help="íŠ¹ì • ì„¸ì…˜ ìƒíƒœ í™•ì¸",
    )
    status_parser.add_argument(
        "--all",
        "-a",
        action="store_true",
        help="ëª¨ë“  ì„¸ì…˜ í‘œì‹œ",
    )
    status_parser.add_argument(
        "--cleanup",
        action="store_true",
        help="ì˜¤ë˜ëœ ì™„ë£Œ ì„¸ì…˜ ì •ë¦¬",
    )


def _add_synonym_parser(subparsers):
    """Add synonym subcommand parser."""
    # Get LLM settings for suggest command
    providers, default_provider, default_model, default_base_url = (
        _get_default_llm_settings()
    )

    parser = subparsers.add_parser(
        "synonym",
        help="ë™ì˜ì–´ ê´€ë¦¬ (LLM ê¸°ë°˜ ìë™ ìƒì„± ë° ìˆ˜ë™ ê´€ë¦¬)",
        description="ë™ì˜ì–´ ì‚¬ì „ì„ ê´€ë¦¬í•©ë‹ˆë‹¤. LLMìœ¼ë¡œ ë™ì˜ì–´ë¥¼ ìë™ ìƒì„±í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€/ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    )
    synonym_subparsers = parser.add_subparsers(dest="synonym_cmd")

    # synonym suggest <term>
    suggest_parser = synonym_subparsers.add_parser(
        "suggest",
        help="LLMìœ¼ë¡œ ë™ì˜ì–´ í›„ë³´ ìƒì„±",
    )
    suggest_parser.add_argument("term", help="ë™ì˜ì–´ë¥¼ ìƒì„±í•  ìš©ì–´")
    suggest_parser.add_argument(
        "--context",
        default="ëŒ€í•™ ê·œì •",
        help="ìš©ì–´ ë§¥ë½ (ê¸°ë³¸: ëŒ€í•™ ê·œì •)",
    )
    suggest_parser.add_argument(
        "--auto-add",
        action="store_true",
        help="ê²€í†  ì—†ì´ ë°”ë¡œ ì¶”ê°€",
    )
    suggest_parser.add_argument(
        "--provider",
        type=str,
        default=default_provider,
        choices=providers,
        help="LLM í”„ë¡œë°”ì´ë”",
    )
    suggest_parser.add_argument(
        "--model",
        type=str,
        default=default_model,
        help="ëª¨ë¸ëª…",
    )
    suggest_parser.add_argument(
        "--base-url",
        type=str,
        default=default_base_url,
        help="ë¡œì»¬ ì„œë²„ URL",
    )

    # synonym add <term> <synonym>
    add_parser = synonym_subparsers.add_parser("add", help="ë™ì˜ì–´ ìˆ˜ë™ ì¶”ê°€")
    add_parser.add_argument("term", help="ê¸°ì¤€ ìš©ì–´")
    add_parser.add_argument("synonym", help="ì¶”ê°€í•  ë™ì˜ì–´")

    # synonym remove <term> <synonym>
    remove_parser = synonym_subparsers.add_parser("remove", help="ë™ì˜ì–´ ì œê±°")
    remove_parser.add_argument("term", help="ê¸°ì¤€ ìš©ì–´")
    remove_parser.add_argument("synonym", help="ì œê±°í•  ë™ì˜ì–´")

    # synonym list [term]
    list_parser = synonym_subparsers.add_parser("list", help="ë™ì˜ì–´ ëª©ë¡ ì¡°íšŒ")
    list_parser.add_argument("term", nargs="?", help="íŠ¹ì • ìš©ì–´ë§Œ ì¡°íšŒ (ìƒëµ ì‹œ ì „ì²´)")


def _add_reparse_parser(subparsers):
    """Add reparse subcommand parser for HWPX full reparse."""
    parser = subparsers.add_parser(
        "reparse",
        help="HWPX íŒŒì¼ ì¼ê´„ ì¬íŒŒì‹± ë° í’ˆì§ˆ ë¶„ì„",
        description="ëª¨ë“  HWPX íŒŒì¼ì„ ì¬íŒŒì‹±í•˜ê³  í’ˆì§ˆ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "-i",
        "--input-dir",
        type=str,
        default="data/input",
        help="HWPX íŒŒì¼ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/input)",
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        type=str,
        default="data/output",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/output)",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ì¶œë ¥",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ íŒŒì¼ ìƒì„± ì—†ì´ ë¯¸ë¦¬ë³´ê¸°",
    )


def create_parser() -> argparse.ArgumentParser:
    """Create main argument parser with all subcommands."""
    parser = argparse.ArgumentParser(
        prog="regulation",
        description="ëŒ€í•™ ê·œì • ê´€ë¦¬ ì‹œìŠ¤í…œ - HWPX ë³€í™˜, RAG ê²€ìƒ‰, AI Q&A",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  regulation convert "ê·œì •ì§‘.hwpx"      HWPX â†’ JSON ë³€í™˜
  regulation sync data/output/ê·œì •ì§‘.json  DB ë™ê¸°í™”
  regulation search "êµì› ì—°êµ¬ë…„"        ê·œì • ê²€ìƒ‰
  regulation ask "íœ´í•™ ì ˆì°¨"             AI ì§ˆë¬¸
  regulation status                      ìƒíƒœ í™•ì¸
  regulation serve --web                 Web UI ì‹œì‘
""",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        dest="global_debug",
        help="ì „ì—­ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”",
    )
    parser.add_argument(
        "--version",
        action="version",
        version="%(prog)s 0.1.0",
    )

    subparsers = parser.add_subparsers(
        dest="command",
        title="commands",
        description="ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´",
        metavar="<command>",
    )

    # Add all subcommands
    _add_convert_parser(subparsers)
    _add_sync_parser(subparsers)
    _add_search_parser(subparsers)
    _add_ask_parser(subparsers)
    _add_status_parser(subparsers)
    _add_reset_parser(subparsers)
    _add_serve_parser(subparsers)
    _add_evaluate_parser(subparsers)
    _add_extract_keywords_parser(subparsers)
    _add_feedback_parser(subparsers)
    _add_analyze_parser(subparsers)
    _add_quality_parser(subparsers)
    _add_synonym_parser(subparsers)
    _add_reparse_parser(subparsers)

    return parser


# =============================================================================
# Command Handlers
# =============================================================================


def cmd_convert(args) -> int:
    """Execute convert command - HWP to JSON conversion."""

    # Convert argument names to match main.py expectations
    # (unified CLI uses kebab-case, main.py uses snake_case)
    class ConvertArgs:
        def __init__(self, args):
            self.input_path = args.input_path
            self.output_dir = getattr(args, "output_dir", "data/output")
            self.use_llm = getattr(args, "use_llm", False)
            self.provider = getattr(args, "provider", "openai")
            self.model = getattr(args, "model", None)
            self.base_url = getattr(args, "base_url", None)
            self.allow_llm_fallback = getattr(args, "allow_llm_fallback", False)
            self.force = getattr(args, "force", False)
            self.cache_dir = getattr(args, "cache_dir", ".cache")
            self.verbose = getattr(args, "verbose", False)
            self.enhance_rag = getattr(args, "enhance_rag", True)
            self.hwpx = getattr(args, "hwpx", False)

    from ...main import run_pipeline

    return run_pipeline(ConvertArgs(args))


def cmd_sync(args) -> int:
    """Execute sync command."""
    from .cli import cmd_sync as _cmd_sync

    return _cmd_sync(args)


def cmd_search(args) -> int:
    """Execute search command."""
    from .cli import cmd_search as _cmd_search

    return _cmd_search(args)


def cmd_ask(args) -> int:
    """Execute ask command."""
    from .cli import cmd_ask as _cmd_ask

    return _cmd_ask(args)


def cmd_status(args) -> int:
    """Execute status command."""
    from .cli import cmd_status as _cmd_status

    return _cmd_status(args)


def cmd_reset(args) -> int:
    """Execute reset command."""
    from .cli import cmd_reset as _cmd_reset

    return _cmd_reset(args)


def cmd_reparse(args) -> int:
    """Execute reparse command - HWPX full reparse with quality analysis."""
    from ...commands.reparse_hwpx import main as reparse_main

    # Build argv for reparse_hwpx
    argv = []
    if args.input_dir:
        argv.extend(["--input-dir", args.input_dir])
    if args.output_dir:
        argv.extend(["--output-dir", args.output_dir])
    if args.verbose:
        argv.append("--verbose")
    if args.dry_run:
        argv.append("--dry-run")

    return reparse_main(argv)


def cmd_serve(args) -> int:
    """Execute serve command - start Web UI or MCP Server."""
    import os

    # Enable warmup for server modes
    os.environ["WARMUP_ON_INIT"] = "true"

    if args.web:
        import gradio as gr

        from .gradio_app import CUSTOM_CSS, create_app

        app = create_app(db_path=args.db_path)
        app.launch(
            server_port=args.port,
            share=args.share,
            show_error=True,
            css=CUSTOM_CSS,
            theme=gr.themes.Soft(
                primary_hue=gr.themes.colors.emerald,
                neutral_hue=gr.themes.colors.neutral,
            ).set(
                body_background_fill="#0f0f0f",
                body_background_fill_dark="#0f0f0f",
                block_background_fill="#1a1a1a",
                block_background_fill_dark="#1a1a1a",
                border_color_primary="rgba(255,255,255,0.06)",
                border_color_primary_dark="rgba(255,255,255,0.06)",
            ),
        )
        return 0
    elif args.mcp:
        from .mcp_server import mcp

        mcp.run()
        return 0
    return 1


def cmd_evaluate(args) -> int:
    """Execute evaluate command - run quality evaluation."""
    from rich.console import Console

    from ..application.evaluate import EvaluationUseCase
    from ..application.search_usecase import SearchUseCase
    from ..infrastructure.chroma_store import ChromaVectorStore
    from ..infrastructure.llm_adapter import LLMClientAdapter

    console = Console()

    # Initialize components
    store = ChromaVectorStore(persist_directory=args.db_path)

    # Get default settings for LLM
    _, provider, model, base_url = _get_default_llm_settings()

    llm_client = LLMClientAdapter(
        provider=provider,
        model=model,
        base_url=base_url,
    )
    search_usecase = SearchUseCase(
        store=store,
        llm_client=llm_client,
        use_reranker=True,
    )

    # Run evaluation
    eval_usecase = EvaluationUseCase(
        search_usecase=search_usecase,
        dataset_path=args.dataset,
    )

    console.print("[bold]ğŸ” í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...[/bold]")
    test_cases = eval_usecase.load_dataset()
    console.print(f"[dim]ì´ {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤[/dim]\n")

    console.print("[bold]ğŸ§ª í‰ê°€ ì‹¤í–‰ ì¤‘...[/bold]")
    summary = eval_usecase.run_evaluation(
        top_k=args.top_k,
        category=args.category,
    )

    # Print results
    console.print(eval_usecase.format_summary(summary))

    if args.verbose:
        console.print(eval_usecase.format_details(summary))

    return 0 if summary.pass_rate >= 0.8 else 1


def cmd_extract_keywords(args) -> int:
    """Execute extract-keywords command."""
    from rich.console import Console

    from ..infrastructure.keyword_extractor import KeywordExtractor

    console = Console()

    extractor = KeywordExtractor(
        json_path=args.json_path,
        output_path=args.output,
    )

    console.print("[bold]ğŸ“š ê·œì • í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...[/bold]")
    result = extractor.extract_keywords()

    console.print(extractor.format_summary(result))

    if args.verbose:
        console.print(extractor.format_details(result))

    if not args.dry_run:
        output_path = extractor.save_keywords(result)
        console.print(f"\n[green]âœ… ì €ì¥ë¨: {output_path}[/green]")

    return 0


def cmd_feedback(args) -> int:
    """Execute feedback command."""
    from rich.console import Console

    from ..infrastructure.feedback import FeedbackCollector

    console = Console()
    collector = FeedbackCollector()

    if args.clear:
        collector.clear_feedback()
        console.print("[yellow]ğŸ—‘ï¸ ëª¨ë“  í”¼ë“œë°±ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.[/yellow]")
        return 0

    stats = collector.get_statistics()
    console.print(collector.format_statistics(stats))

    return 0


def cmd_analyze(args) -> int:
    """Execute analyze command - analyze feedback for improvements."""
    from rich.console import Console

    from ..application.auto_learn import AutoLearnUseCase
    from ..infrastructure.feedback import FeedbackCollector

    console = Console()

    collector = FeedbackCollector()
    auto_learn = AutoLearnUseCase(feedback_collector=collector)

    console.print("[bold]ğŸ§  í”¼ë“œë°± ë¶„ì„ ì¤‘...[/bold]")
    result = auto_learn.analyze_feedback()

    console.print(auto_learn.format_suggestions(result))

    return 0


def cmd_synonym(args) -> int:
    """Execute synonym management commands."""
    from .cli import cmd_synonym as _cmd_synonym

    return _cmd_synonym(args)


def cmd_quality(args) -> int:
    """Execute quality command - RAGAS-based RAG quality evaluation."""
    from rich.console import Console

    from ..domain.evaluation import RAGQualityEvaluator
    from ..domain.evaluation.personas import PersonaManager
    from ..domain.evaluation.synthetic_data import SyntheticDataGenerator
    from ..infrastructure.json_loader import JSONDocumentLoader
    from ..infrastructure.storage.evaluation_store import EvaluationStore

    console = Console()

    # Initialize components
    evaluator = RAGQualityEvaluator(
        judge_model=args.judge_model,
        use_ragas=not args.no_ragas,
    )
    store = EvaluationStore(storage_dir=args.output_dir)
    persona_mgr = PersonaManager()
    loader = JSONDocumentLoader()

    # Initialize RAG system for answer generation
    from ..application.search_usecase import SearchUseCase
    from ..infrastructure.chroma_store import ChromaVectorStore
    from ..infrastructure.llm_adapter import LLMClientAdapter

    vector_store = ChromaVectorStore(persist_directory=args.db_path)
    _, provider, model, base_url = _get_default_llm_settings()

    # For quality evaluation, prefer local Ollama for reliability
    # Override with env var if explicitly set for evaluation
    import os

    eval_provider = os.getenv("EVAL_LLM_PROVIDER", "ollama")
    eval_model = os.getenv("EVAL_LLM_MODEL", "llama3.2:latest")
    eval_base_url = os.getenv("EVAL_LLM_BASE_URL", "http://localhost:11434")

    console.print(
        f"[dim]Using LLM: {eval_provider} ({eval_model}) at {eval_base_url}[/dim]"
    )

    llm_client = LLMClientAdapter(
        provider=eval_provider, model=eval_model, base_url=eval_base_url
    )
    search_usecase = SearchUseCase(
        store=vector_store, llm_client=llm_client, use_reranker=True
    )

    # Subcommand handling
    if args.quality_cmd == "baseline":
        console.print("[bold]ğŸ” ê¸°ì¤€ì„  í‰ê°€ ì‹œì‘...[/bold]")
        results = []

        for persona_id in persona_mgr.list_personas():
            console.print(f"[dim]í˜ë¥´ì†Œë‚˜ {persona_id} í…ŒìŠ¤íŠ¸ ì¤‘...[/dim]")
            queries = persona_mgr.generate_queries(
                persona_id,
                count=args.queries_per_persona,
                topics=[args.topic] if args.topic else None,
            )

            for query in queries:
                try:
                    # RAG ì‹œìŠ¤í…œ ì‹¤í–‰
                    search_results = search_usecase.search(
                        query_text=query,
                        top_k=args.top_k,
                    )
                    contexts = (
                        [r.chunk.text for r in search_results] if search_results else []
                    )

                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                    if not contexts:
                        console.print(
                            f"[yellow]  âš  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query[:40]}...[/yellow]"
                        )
                        continue

                    # ë‹µë³€ ìƒì„±
                    from ..infrastructure.tool_executor import ToolExecutor

                    tool_executor = ToolExecutor(
                        search_usecase=search_usecase,
                        llm_client=llm_client,
                    )

                    # LLM í´ë¼ì´ì–¸íŠ¸ í™•ì¸
                    if not llm_client:
                        console.print("[red]  âŒ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨[/red]")
                        continue

                    answer = tool_executor._handle_generate_answer(
                        {"question": query, "context": "\n\n".join(contexts)}
                    )

                    # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                    if not answer or answer.strip() == "":
                        console.print(
                            f"[yellow]  âš  ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {query[:40]}...[/yellow]"
                        )
                        continue

                    # í‰ê°€ ì‹¤í–‰
                    result = evaluator.evaluate_single_turn(query, contexts, answer)
                    result.persona = persona_id
                    results.append(result)
                    store.save_evaluation(result)

                    console.print(
                        f"[dim]  Query: {query[:40]}... Score: {result.overall_score:.2f}[/dim]"
                    )
                except Exception as e:
                    console.print(f"[red]í‰ê°€ ì‹¤íŒ¨: {e}[/red]")
                    import traceback

                    console.print(f"[dim]{traceback.format_exc()[:200]}[/dim]")
                    continue

        # í†µê³„ ì¶œë ¥
        stats = store.get_statistics()
        console.print("\n[bold]ê¸°ì¤€ì„  í‰ê°€ ê²°ê³¼[/bold]")
        console.print(f"ì „ì²´ í‰ê°€: {stats.total_evaluations}")
        console.print(f"í‰ê·  ì ìˆ˜: {stats.avg_overall_score:.2f}")
        console.print(f"í•©ê²©ë¥ : {stats.pass_rate:.1%}")
        console.print(f"ì¶”ì„¸: {stats.trend}")
        console.print("\n[bold]ë©”íŠ¸ë¦­ë³„ ì ìˆ˜:[/bold]")
        console.print(f"  Faithfulness: {stats.avg_faithfulness:.2f}")
        console.print(f"  Answer Relevancy: {stats.avg_answer_relevancy:.2f}")
        console.print(f"  Contextual Precision: {stats.avg_contextual_precision:.2f}")
        console.print(f"  Contextual Recall: {stats.avg_contextual_recall:.2f}")

    elif args.quality_cmd == "persona":
        console.print(f"[bold]ğŸ” í˜ë¥´ì†Œë‚˜ {args.id} í…ŒìŠ¤íŠ¸ ì‹œì‘...[/bold]")
        queries = persona_mgr.generate_queries(
            args.id, count=args.count, topics=[args.topic] if args.topic else None
        )
        console.print(f"[dim]{len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ[/dim]")

        for query in queries:
            try:
                search_results = search_usecase.search(
                    query_text=query, top_k=args.top_k
                )
                contexts = (
                    [r.chunk.text for r in search_results] if search_results else []
                )

                from ..infrastructure.tool_executor import ToolExecutor

                tool_executor = ToolExecutor(
                    search_usecase=search_usecase,
                    llm_client=llm_client,
                )
                answer = tool_executor._handle_generate_answer(
                    {"question": query, "context": "\n\n".join(contexts)}
                )

                result = evaluator.evaluate_single_turn(query, contexts, answer)
                result.persona = args.id
                store.save_evaluation(result)

                console.print(
                    f"Score: {result.overall_score:.2f} | Query: {query[:50]}..."
                )
            except Exception as e:
                console.print(f"[red]í‰ê°€ ì‹¤íŒ¨: {e}[/red]")

    elif args.quality_cmd == "synthetic":
        console.print("[bold]ğŸ“ í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹œì‘...[/bold]")
        generator = SyntheticDataGenerator(loader)

        if args.scenarios:
            scenarios = generator.generate_scenarios_from_regulations(
                regulation=args.regulation, num_scenarios=args.count
            )
            console.print(f"[green]âœ… {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì™„ë£Œ[/green]")
        else:
            queries = generator.generate_queries_from_documents(
                num_questions=args.count, difficulty=args.difficulty
            )
            console.print(f"[green]âœ… {len(queries)}ê°œ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ[/green]")

    elif args.quality_cmd == "stats":
        if args.days:
            stats = store.get_statistics(days=args.days)
        else:
            stats = store.get_statistics()

        console.print("\n[bold]í‰ê°€ í†µê³„[/bold]")
        console.print(f"ì „ì²´ í‰ê°€: {stats.total_evaluations}")
        console.print(f"í‰ê·  ì ìˆ˜: {stats.avg_overall_score:.2f}")
        console.print(f"í•©ê²©ë¥ : {stats.pass_rate:.1%}")
        console.print(f"ìµœì € ì ìˆ˜: {stats.min_score:.2f}")
        console.print(f"ìµœê³  ì ìˆ˜: {stats.max_score:.2f}")
        console.print(f"í‘œì¤€ í¸ì°¨: {stats.std_deviation:.2f}")
        console.print(f"ì¶”ì„¸: {stats.trend}")
        console.print("\n[bold]ë©”íŠ¸ë¦­ë³„ í‰ê· :[/bold]")
        console.print(f"  Faithfulness: {stats.avg_faithfulness:.2f}")
        console.print(f"  Answer Relevancy: {stats.avg_answer_relevancy:.2f}")
        console.print(f"  Contextual Precision: {stats.avg_contextual_precision:.2f}")
        console.print(f"  Contextual Recall: {stats.avg_contextual_recall:.2f}")

    elif args.quality_cmd == "dashboard":
        console.print("[bold]ğŸš€ Gradio í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ì‹œì‘...[/bold]")

        from .web.quality_dashboard import app as quality_app

        quality_app.launch(
            server_port=7861,
            share=False,
            show_error=True,
        )

    # P2: quality run - ì „ì²´ í‰ê°€ ì‹¤í–‰
    elif args.quality_cmd == "run":
        return _cmd_quality_run(args, console, evaluator, store, persona_mgr, search_usecase, llm_client)

    # P2: quality resume - ì¤‘ë‹¨ëœ ì„¸ì…˜ ì¬ê°œ
    elif args.quality_cmd == "resume":
        return _cmd_quality_resume(args, console, store)

    # P2: quality generate-spec - ì‹¤íŒ¨ íŒ¨í„´ì—ì„œ SPEC ìƒì„±
    elif args.quality_cmd == "generate-spec":
        return _cmd_quality_generate_spec(args, console, store)

    # P2: quality status - ì„¸ì…˜ ìƒíƒœ í™•ì¸
    elif args.quality_cmd == "status":
        return _cmd_quality_status(args, console, store)

    return 0


def _cmd_quality_run(args, console, evaluator, store, persona_mgr, search_usecase, llm_client) -> int:
    """Execute quality run command - batch evaluation with progress tracking."""
    from ..application.evaluation import (
        CheckpointManager,
        ProgressReporter,
    )

    checkpoint_dir = "data/checkpoints"
    if not args.no_checkpoint:
        checkpoint_mgr = CheckpointManager(checkpoint_dir=checkpoint_dir)
    else:
        checkpoint_mgr = None

    # Create session or resume existing
    session_id = args.session_id
    if session_id:
        progress_data = checkpoint_mgr.load_checkpoint(session_id) if checkpoint_mgr else None
        if progress_data is None:
            console.print(f"[red]ì„¸ì…˜ {session_id}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
            return 1
        console.print(f"[bold]ì„¸ì…˜ {session_id} ì¬ê°œ ì¤‘...[/bold]")
    else:
        # Create new session
        import uuid
        session_id = f"eval-{uuid.uuid4().hex[:8]}"
        personas = list(args.personas) if args.personas else persona_mgr.list_personas()
        total_queries = len(personas) * args.queries_per_persona

        if checkpoint_mgr:
            checkpoint_mgr.create_session(
                session_id=session_id,
                total_queries=total_queries,
                personas=personas,
            )
        console.print(f"[bold]ìƒˆ í‰ê°€ ì„¸ì…˜ ì‹œì‘: {session_id}[/bold]")
        console.print(f"[dim]í˜ë¥´ì†Œë‚˜: {', '.join(personas)}[/dim]")
        console.print(f"[dim]ì´ ì¿¼ë¦¬ ìˆ˜: {total_queries}[/dim]")

    # Initialize progress reporter
    personas_for_progress = list(args.personas) if args.personas else persona_mgr.list_personas()
    persona_counts = {p: args.queries_per_persona for p in personas_for_progress}
    total_for_reporter = sum(persona_counts.values())
    reporter = ProgressReporter(total_queries=total_for_reporter, persona_counts=persona_counts)

    # Initialize batch executor
    # Note: BatchEvaluationExecutor requires an evaluator callable
    # For now, we'll skip the batch executor and process queries directly
    # batch_executor = BatchEvaluationExecutor(
    #     evaluator=evaluator.evaluate_single_turn,
    #     batch_size=args.batch_size,
    # )

    # Run evaluation
    results = []
    personas = list(args.personas) if args.personas else persona_mgr.list_personas()

    try:
        for persona_id in personas:
            queries = persona_mgr.generate_queries(
                persona_id,
                count=args.queries_per_persona,
            )

            for i, query in enumerate(queries):
                try:
                    # Search
                    search_results = search_usecase.search(query_text=query, top_k=5)
                    contexts = [r.chunk.text for r in search_results] if search_results else []

                    if not contexts:
                        console.print(f"[yellow]  âš  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query[:40]}...[/yellow]")
                        continue

                    # Generate answer
                    from ..infrastructure.tool_executor import ToolExecutor
                    tool_executor = ToolExecutor(
                        search_usecase=search_usecase,
                        llm_client=llm_client,
                    )
                    answer = tool_executor._handle_generate_answer(
                        {"question": query, "context": "\n\n".join(contexts)}
                    )

                    if not answer or answer.strip() == "":
                        continue

                    # Evaluate
                    result = evaluator.evaluate_single_turn(query, contexts, answer)
                    result.persona = persona_id
                    results.append(result)
                    store.save_evaluation(result)

                    # Update progress
                    reporter.update(
                        completed=1,
                        persona=persona_id,
                        query=query,
                        score=result.overall_score,
                    )

                    # Save checkpoint
                    if checkpoint_mgr and not args.no_checkpoint:
                        checkpoint_mgr.update_progress(
                            session_id=session_id,
                            persona=persona_id,
                            query_id=f"q_{i}",
                            result={"score": result.overall_score, "query": query},
                        )

                    # Show progress
                    progress_info = reporter.get_progress()
                    eta = reporter.get_eta()
                    console.print(
                        f"[dim]  [{progress_info.completed}/{progress_info.total}] "
                        f"Score: {result.overall_score:.2f} | ETA: {eta:.0f}s | {query[:30]}...[/dim]"
                    )

                except Exception as e:
                    console.print(f"[red]í‰ê°€ ì‹¤íŒ¨: {e}[/red]")
                    if checkpoint_mgr and not args.no_checkpoint:
                        checkpoint_mgr.update_progress(
                            session_id=session_id,
                            persona=persona_id,
                            query_id=f"q_{i}",
                            error=str(e),
                        )

    except KeyboardInterrupt:
        console.print("\n[yellow]í‰ê°€ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.[/yellow]")
        if checkpoint_mgr and not args.no_checkpoint:
            checkpoint_mgr.pause_session(session_id)
            console.print(f"[yellow]ì„¸ì…˜ ì €ì¥ë¨: {session_id}[/yellow]")
            console.print(f"[yellow]ì¬ê°œ ëª…ë ¹: regulation quality resume -s {session_id}[/yellow]")
        return 130

    # Final statistics
    stats = store.get_statistics()
    console.print("\n[bold green]âœ… í‰ê°€ ì™„ë£Œ![/bold green]")
    console.print(f"ì„¸ì…˜ ID: {session_id}")
    console.print(f"í‰ê°€ëœ ì¿¼ë¦¬: {len(results)}")
    console.print(f"í‰ê·  ì ìˆ˜: {stats.avg_overall_score:.2f}")
    console.print(f"í•©ê²©ë¥ : {stats.pass_rate:.1%}")

    # Save report
    if args.output:
        import json
        report = {
            "session_id": session_id,
            "total_queries": len(results),
            "stats": {
                "avg_score": stats.avg_overall_score,
                "pass_rate": stats.pass_rate,
                "avg_faithfulness": stats.avg_faithfulness,
                "avg_answer_relevancy": stats.avg_answer_relevancy,
                "avg_contextual_precision": stats.avg_contextual_precision,
                "avg_contextual_recall": stats.avg_contextual_recall,
            },
        }
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        console.print(f"[dim]ë³´ê³ ì„œ ì €ì¥ë¨: {args.output}[/dim]")

    return 0


def _cmd_quality_resume(args, console, store) -> int:
    """Execute quality resume command - resume interrupted session."""
    from ..application.evaluation import CheckpointManager, ResumeController

    checkpoint_dir = "data/checkpoints"
    checkpoint_mgr = CheckpointManager(checkpoint_dir=checkpoint_dir)
    resume_ctrl = ResumeController(checkpoint_manager=checkpoint_mgr)

    # List sessions if requested
    if args.list:
        sessions = resume_ctrl.find_interrupted_sessions()
        if not sessions:
            console.print("[yellow]ì¬ê°œ ê°€ëŠ¥í•œ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return 0

        console.print("[bold]ì¬ê°œ ê°€ëŠ¥í•œ ì„¸ì…˜:[/bold]")
        for session in sessions:
            console.print(
                f"  - {session['session_id']}: "
                f"{session['completion_rate']:.0f}% ì™„ë£Œ, "
                f"ì—…ë°ì´íŠ¸: {session['updated_at']}"
            )
        return 0

    # Get session to resume
    session_id = args.session_id
    if not session_id:
        session_id = resume_ctrl.get_resume_recommendation()
        if not session_id:
            console.print("[yellow]ì¬ê°œí•  ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return 1
        console.print(f"[dim]ê°€ì¥ ìµœê·¼ ì¤‘ë‹¨ ì„¸ì…˜ ì„ íƒ: {session_id}[/dim]")

    # Check if can resume
    can_resume, reason = resume_ctrl.can_resume(session_id)
    if not can_resume:
        console.print(f"[red]ì„¸ì…˜ {session_id}ì„(ë¥¼) ì¬ê°œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {reason}[/red]")
        return 1

    # Get resume context
    context = resume_ctrl.get_resume_context(session_id)
    if not context:
        console.print(f"[red]ì„¸ì…˜ {session_id}ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
        return 1

    console.print(f"[bold]ì„¸ì…˜ {session_id} ì¬ê°œ ì •ë³´:[/bold]")
    console.print(f"  ì™„ë£Œìœ¨: {context.completion_rate:.1f}%")
    console.print(f"  ì™„ë£Œëœ ì¿¼ë¦¬: {context.completed_count}/{context.total_count}")
    console.print(f"  ì‹¤íŒ¨í•œ ì¿¼ë¦¬: {context.failed_count}")
    console.print(f"  ë‚¨ì€ í˜ë¥´ì†Œë‚˜: {', '.join(context.remaining_personas) or 'ì—†ìŒ'}")

    console.print("\n[green]ì„¸ì…˜ ì¬ê°œ ëª…ë ¹:[/green]")
    console.print(f"  regulation quality run -s {session_id}")

    return 0


def _cmd_quality_generate_spec(args, console, store) -> int:
    """Execute quality generate-spec command - generate SPEC from failures."""
    from ..domain.evaluation import (
        FailureClassifier,
        RecommendationEngine,
        SPECGenerator,
    )
    from ..infrastructure.storage.evaluation_store import EvaluationStore

    # Get evaluations from store
    eval_store = EvaluationStore(storage_dir=args.output_dir if hasattr(args, 'output_dir') else "data/evaluations")

    # Get recent evaluations below threshold
    evaluations = eval_store.get_evaluations(
        max_score=args.threshold,
        limit=100,
    )

    if not evaluations:
        console.print(f"[yellow]ì„ê³„ê°’ {args.threshold} ë¯¸ë§Œì˜ ì‹¤íŒ¨í•œ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
        return 0

    console.print(f"[bold]ë¶„ì„ ì¤‘: {len(evaluations)}ê°œ ì‹¤íŒ¨ í‰ê°€[/bold]")

    # Classify failures
    classifier = FailureClassifier()
    failure_summaries = classifier.classify_batch(evaluations)

    console.print("\n[bold]ì‹¤íŒ¨ ìœ í˜• ë¶„ì„:[/bold]")
    for summary in failure_summaries:
        console.print(
            f"  - {summary.failure_type.value}: {summary.count}ê±´ "
            f"(í‰ê·  ì ìˆ˜: {summary.avg_score:.2f})"
        )

    # Generate recommendations
    engine = RecommendationEngine()
    failure_counts = {s.failure_type: s.count for s in failure_summaries}
    recommendations = engine.generate_recommendations(failure_counts, threshold=1)

    console.print(f"\n[bold]ìƒì„±ëœ ê¶Œì¥ì‚¬í•­: {len(recommendations)}ê°œ[/bold]")

    # Generate SPEC
    spec_generator = SPECGenerator()
    spec = spec_generator.generate_spec(
        failures=failure_summaries,
        recommendations=recommendations,
    )

    # Output SPEC
    if args.output:
        spec_path = spec_generator.save_spec(spec, path=args.output)
        console.print(f"\n[green]âœ… SPEC ë¬¸ì„œ ìƒì„± ì™„ë£Œ: {spec_path}[/green]")
    else:
        # Print to console
        console.print("\n" + "=" * 60)
        console.print(spec.to_markdown())
        console.print("=" * 60)

    # Show action plan
    if recommendations:
        plan = engine.get_action_plan(recommendations)
        console.print("\n[bold]ì•¡ì…˜ í”Œëœ:[/bold]")
        console.print(f"  ì¦‰ì‹œ ì¡°ì¹˜: {len(plan['immediate_actions'])}ê°œ")
        console.print(f"  ë‹¨ê¸° ì¡°ì¹˜: {len(plan['short_term_actions'])}ê°œ")
        console.print(f"  ì¥ê¸° ì¡°ì¹˜: {len(plan['long_term_actions'])}ê°œ")

    return 0


def _cmd_quality_status(args, console, store) -> int:
    """Execute quality status command - check session status."""
    from ..application.evaluation import CheckpointManager

    checkpoint_dir = "data/checkpoints"
    checkpoint_mgr = CheckpointManager(checkpoint_dir=checkpoint_dir)

    # Cleanup if requested
    if args.cleanup:
        cleaned = checkpoint_mgr.cleanup_completed_sessions(keep_days=7)
        console.print(f"[green]ì •ë¦¬ëœ ì„¸ì…˜: {cleaned}ê°œ[/green]")
        return 0

    # Show specific session
    if args.session_id:
        progress = checkpoint_mgr.load_checkpoint(args.session_id)
        if not progress:
            console.print(f"[red]ì„¸ì…˜ {args.session_id}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
            return 1

        console.print(f"[bold]ì„¸ì…˜: {progress.session_id}[/bold]")
        console.print(f"  ìƒíƒœ: {progress.status}")
        console.print(f"  ì‹œì‘: {progress.started_at}")
        console.print(f"  ì—…ë°ì´íŠ¸: {progress.updated_at}")
        console.print(f"  ì§„í–‰ë¥ : {progress.completed_queries}/{progress.total_queries}")
        console.print(f"  ì™„ë£Œìœ¨: {progress.completion_rate:.1f}%")

        console.print("\n[bold]í˜ë¥´ì†Œë‚˜ë³„ ì§„í–‰:[/bold]")
        for persona, persona_progress in progress.personas.items():
            console.print(
                f"  - {persona}: {persona_progress.completed_queries}/{persona_progress.total_queries} "
                f"(ì‹¤íŒ¨: {persona_progress.failed_queries})"
            )
        return 0

    # Show all sessions
    sessions = checkpoint_mgr.list_sessions()

    if not sessions:
        console.print("[yellow]ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
        return 0

    if not args.all:
        # Show only recent/active sessions
        sessions = [s for s in sessions if s.get("status") != "completed"][:5]

    console.print(f"[bold]í‰ê°€ ì„¸ì…˜ ({len(sessions)}ê°œ):[/bold]\n")

    for session in sessions:
        status_color = {
            "running": "green",
            "paused": "yellow",
            "completed": "blue",
            "failed": "red",
        }.get(session.get("status"), "white")

        console.print(
            f"  [{status_color}]{session['session_id']}[/{status_color}] "
            f"- {session['status']} "
            f"- {session['completion_rate']:.0f}% "
            f"- {session['updated_at']}"
        )

    return 0


# =============================================================================
# Entry Point
# =============================================================================


def main(argv: Optional[list] = None) -> int:
    """Main entry point for the unified CLI."""
    parser = create_parser()
    args = parser.parse_args(argv)

    # Global debug flag handling
    if hasattr(args, "global_debug") and args.global_debug:
        args.debug = True

    # ì»¤ë§¨ë“œ ì—†ì´ ì‹¤í–‰í•˜ë©´ interactive ëª¨ë“œë¡œ ì‹œì‘
    if not args.command:
        # ê¸°ë³¸ê°’ ì„¤ì •
        args.command = "search"
        args.query = None
        args.interactive = True
        args.top_k = 5
        args.include_abolished = False
        args.db_path = "data/chroma_db"
        args.no_rerank = False
        args.verbose = False
        args.debug = False
        args.feedback = False
        args.answer = False
        args.quick = False
        args.show_sources = False
        # LLM ê¸°ë³¸ê°’
        providers, provider, model, base_url = _get_default_llm_settings()
        args.provider = provider
        args.model = model
        args.base_url = base_url
        return cmd_search(args)

    commands = {
        "convert": cmd_convert,
        "sync": cmd_sync,
        "search": cmd_search,
        "ask": cmd_ask,
        "status": cmd_status,
        "reset": cmd_reset,
        "reparse": cmd_reparse,
        "serve": cmd_serve,
        "evaluate": cmd_evaluate,
        "extract-keywords": cmd_extract_keywords,
        "feedback": cmd_feedback,
        "analyze": cmd_analyze,
        "synonym": cmd_synonym,
        "quality": cmd_quality,
    }

    if args.command in commands:
        try:
            return commands[args.command](args)
        except KeyboardInterrupt:
            print("\nAborted.")
            return 130

    parser.print_help()
    return 1


if __name__ == "__main__":
    sys.exit(main())
