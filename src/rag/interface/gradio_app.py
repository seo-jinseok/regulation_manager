"""
Gradio Web UI for Regulation RAG System.

Provides a user-friendly web interface for:
- Searching regulations
- Asking questions with LLM-generated answers
- Viewing sync status

Usage:
    uv run python -m src.rag.interface.gradio_app
"""

import os
from typing import List, Optional, Tuple

try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

from ..infrastructure.chroma_store import ChromaVectorStore
from ..infrastructure.json_loader import JSONDocumentLoader
from ..infrastructure.llm_client import OpenAIClient, MockLLMClient
from ..application.sync_usecase import SyncUseCase
from ..application.search_usecase import SearchUseCase
from ..domain.value_objects import SearchFilter
from ..domain.entities import RegulationStatus


# Default paths
DEFAULT_DB_PATH = "data/chroma_db"
DEFAULT_JSON_PATH = "data/output/ê·œì •ì§‘-test01.json"


def create_app(
    db_path: str = DEFAULT_DB_PATH,
    use_mock_llm: bool = False,
) -> "gr.Blocks":
    """
    Create Gradio app instance.

    Args:
        db_path: Path to ChromaDB storage.
        use_mock_llm: Use mock LLM for testing without API key.

    Returns:
        Gradio Blocks app.
    """
    if not GRADIO_AVAILABLE:
        raise ImportError("gradio is required. Install with: uv add gradio")

    # Initialize components
    store = ChromaVectorStore(persist_directory=db_path)
    loader = JSONDocumentLoader()

    # LLM client (use mock if no API key)
    llm_client = None
    llm_status = "âŒ LLM ë¹„í™œì„± (OPENAI_API_KEY ë¯¸ì„¤ì •)"

    if not use_mock_llm and os.getenv("OPENAI_API_KEY"):
        try:
            llm_client = OpenAIClient()
            llm_status = "âœ… OpenAI GPT-4o-mini ì—°ê²°ë¨"
        except Exception as e:
            llm_status = f"âŒ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}"
    elif use_mock_llm:
        llm_client = MockLLMClient()
        llm_status = "âš ï¸ Mock LLM (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)"

    search_usecase = SearchUseCase(store, llm_client)
    sync_usecase = SyncUseCase(loader, store)

    # Get initial status
    def get_status_text() -> str:
        status = sync_usecase.get_sync_status()
        return f"""**ë™ê¸°í™” ìƒíƒœ**
- ë§ˆì§€ë§‰ ë™ê¸°í™”: {status['last_sync'] or 'ì—†ìŒ'}
- JSON íŒŒì¼: {status['json_file'] or 'ì—†ìŒ'}
- ì¸ë±ì‹±ëœ ê·œì •: {status['store_regulations']}ê°œ
- ì²­í¬ ìˆ˜: {status['store_chunks']}ê°œ
- LLM: {llm_status}
"""

    # Search function
    def search_regulations(
        query: str,
        top_k: int,
        include_abolished: bool,
    ) -> Tuple[str, str]:
        """Execute search and return formatted results."""
        if not query.strip():
            return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""

        if store.count() == 0:
            return "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ë™ê¸°í™”ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.", ""

        results = search_usecase.search(
            query,
            top_k=top_k,
            include_abolished=include_abolished,
        )

        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", ""

        # Format results as markdown table
        table_rows = ["| # | ê·œì • | ì¡°í•­ | ì ìˆ˜ |", "|---|------|------|------|"]
        for i, r in enumerate(results, 1):
            path = " > ".join(r.chunk.parent_path[-2:]) if r.chunk.parent_path else r.chunk.title
            table_rows.append(f"| {i} | {r.chunk.rule_code} | {path[:30]} | {r.score:.2f} |")

        table = "\n".join(table_rows)

        # Top result detail
        top = results[0]
        detail = f"""### 1ìœ„ ê²°ê³¼: {top.chunk.rule_code}
**ê²½ë¡œ:** {' > '.join(top.chunk.parent_path)}

{top.chunk.text[:500]}{'...' if len(top.chunk.text) > 500 else ''}
"""

        return table, detail

    # Ask function (with LLM)
    def ask_question(
        question: str,
        top_k: int,
        include_abolished: bool,
    ) -> Tuple[str, str]:
        """Ask question and get LLM answer."""
        if not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""

        if not llm_client:
            return "LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.", ""

        if store.count() == 0:
            return "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", ""

        filter = None
        if not include_abolished:
            filter = SearchFilter(status=RegulationStatus.ACTIVE)

        answer = search_usecase.ask(
            question,
            filter=filter,
            top_k=top_k,
            include_abolished=include_abolished,
        )

        # Format sources
        sources = []
        for i, r in enumerate(answer.sources, 1):
            path = " > ".join(r.chunk.parent_path[-2:]) if r.chunk.parent_path else ""
            sources.append(f"{i}. [{r.chunk.rule_code}] {path}")

        sources_text = "\n".join(sources) if sources else "ì¶œì²˜ ì—†ìŒ"

        return answer.text, f"### ì°¸ê³  ê·œì •\n{sources_text}\n\n*ì‹ ë¢°ë„: {answer.confidence:.0%}*"

    # Sync function
    def run_sync(json_path: str, full_sync: bool) -> str:
        """Run synchronization."""
        if not json_path.strip():
            return "JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        try:
            if full_sync:
                result = sync_usecase.full_sync(json_path)
            else:
                result = sync_usecase.incremental_sync(json_path)

            if result.has_errors:
                return f"âŒ ì˜¤ë¥˜ ë°œìƒ:\n" + "\n".join(result.errors)

            return f"""âœ… ë™ê¸°í™” ì™„ë£Œ!
- ì¶”ê°€: {result.added}ê°œ
- ìˆ˜ì •: {result.modified}ê°œ
- ì‚­ì œ: {result.removed}ê°œ
- ë³€ê²½ì—†ìŒ: {result.unchanged}ê°œ
- ì´ ì²­í¬: {store.count()}ê°œ
"""
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜: {str(e)}"

    # Build UI
    with gr.Blocks(
        title="ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A",
        theme=gr.themes.Soft(),
    ) as app:
        gr.Markdown("# ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A ì‹œìŠ¤í…œ")

        with gr.Tabs():
            # Tab 1: Search
            with gr.TabItem("ğŸ” ê²€ìƒ‰"):
                with gr.Row():
                    with gr.Column(scale=3):
                        search_query = gr.Textbox(
                            label="ê²€ìƒ‰ì–´",
                            placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ìê²©",
                            lines=1,
                        )
                    with gr.Column(scale=1):
                        search_top_k = gr.Slider(
                            minimum=1, maximum=20, value=5, step=1,
                            label="ê²°ê³¼ ìˆ˜",
                        )
                        search_abolished = gr.Checkbox(
                            label="íì§€ ê·œì • í¬í•¨",
                            value=False,
                        )

                search_btn = gr.Button("ê²€ìƒ‰", variant="primary")

                search_results = gr.Markdown(label="ê²€ìƒ‰ ê²°ê³¼")
                search_detail = gr.Markdown(label="ìƒì„¸ ë‚´ìš©")

                search_btn.click(
                    fn=search_regulations,
                    inputs=[search_query, search_top_k, search_abolished],
                    outputs=[search_results, search_detail],
                )

            # Tab 2: Ask (Q&A)
            with gr.TabItem("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°"):
                with gr.Row():
                    with gr.Column(scale=3):
                        ask_question_input = gr.Textbox(
                            label="ì§ˆë¬¸",
                            placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ì‹ ì²­ ìê²©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                            lines=2,
                        )
                    with gr.Column(scale=1):
                        ask_top_k = gr.Slider(
                            minimum=1, maximum=10, value=5, step=1,
                            label="ì°¸ê³  ê·œì • ìˆ˜",
                        )
                        ask_abolished = gr.Checkbox(
                            label="íì§€ ê·œì • í¬í•¨",
                            value=False,
                        )

                ask_btn = gr.Button("ì§ˆë¬¸í•˜ê¸°", variant="primary")

                ask_answer = gr.Markdown(label="ë‹µë³€")
                ask_sources = gr.Markdown(label="ì°¸ê³  ê·œì •")

                ask_btn.click(
                    fn=ask_question,
                    inputs=[ask_question_input, ask_top_k, ask_abolished],
                    outputs=[ask_answer, ask_sources],
                )

            # Tab 3: Sync
            with gr.TabItem("âš™ï¸ ì„¤ì •"):
                gr.Markdown(get_status_text())

                with gr.Row():
                    sync_json_path = gr.Textbox(
                        label="JSON íŒŒì¼ ê²½ë¡œ",
                        value=DEFAULT_JSON_PATH,
                    )
                    sync_full = gr.Checkbox(
                        label="ì „ì²´ ë™ê¸°í™”",
                        value=False,
                    )

                sync_btn = gr.Button("ë™ê¸°í™” ì‹¤í–‰", variant="secondary")
                sync_result = gr.Markdown(label="ê²°ê³¼")

                sync_btn.click(
                    fn=run_sync,
                    inputs=[sync_json_path, sync_full],
                    outputs=[sync_result],
                )

    return app


def main():
    """Launch Gradio app."""
    import argparse

    parser = argparse.ArgumentParser(description="ê·œì •ì§‘ RAG ì›¹ UI")
    parser.add_argument("--port", type=int, default=7860, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--share", action="store_true", help="ê³µê°œ ë§í¬ ìƒì„±")
    parser.add_argument("--mock-llm", action="store_true", help="Mock LLM ì‚¬ìš©")
    parser.add_argument("--db-path", default=DEFAULT_DB_PATH, help="DB ê²½ë¡œ")

    args = parser.parse_args()

    app = create_app(db_path=args.db_path, use_mock_llm=args.mock_llm)
    app.launch(
        server_port=args.port,
        share=args.share,
        show_error=True,
    )


if __name__ == "__main__":
    main()
