"""
Gradio Web UI for Regulation RAG System - ChatGPT Style.

Provides a modern chat-style interface for:
- Searching regulations (auto-detected)
- Asking questions with LLM-generated answers (auto-detected)
- Viewing full regulation text

Usage:
    uv run python -m src.rag.interface.gradio_app
"""

import argparse
import os
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass

try:
    import gradio as gr

    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

from ..application.full_view_usecase import FullViewUseCase, TableMatch
from ..application.search_usecase import QueryRewriteInfo, SearchUseCase
from ..application.sync_usecase import SyncUseCase
from ..domain.entities import RegulationStatus
from ..domain.value_objects import SearchFilter
from ..infrastructure.chroma_store import ChromaVectorStore
from ..infrastructure.json_loader import JSONDocumentLoader
from ..infrastructure.llm_adapter import LLMClientAdapter
from ..infrastructure.llm_client import MockLLMClient
from ..infrastructure.query_analyzer import Audience, QueryAnalyzer

try:
    from ..infrastructure.function_gemma_adapter import FunctionGemmaAdapter

    FUNCTION_GEMMA_AVAILABLE = True
except ImportError:
    FUNCTION_GEMMA_AVAILABLE = False
    FunctionGemmaAdapter = None

from .chat_logic import (
    format_clarification,
)
from .formatters import (
    clean_path_segments,
    filter_by_relevance,
    format_search_result_with_explanation,
    get_relevance_label_combined,
    infer_attachment_label,
    normalize_markdown_emphasis,
    normalize_markdown_table,
    normalize_relevance_scores,
    strip_path_prefix,
)
from .link_formatter import extract_and_format_references, format_as_markdown_links
from .query_handler import QueryContext, QueryHandler, QueryOptions, QueryResult

# Default paths
DEFAULT_DB_PATH = "data/chroma_db"
DEFAULT_JSON_PATH = "data/output/ê·œì •ì§‘-test01.json"
LLM_PROVIDERS = ["ollama", "lmstudio", "mlx", "local", "openai", "gemini", "openrouter"]
DEFAULT_LLM_PROVIDER = os.getenv("LLM_PROVIDER") or "ollama"
if DEFAULT_LLM_PROVIDER not in LLM_PROVIDERS:
    DEFAULT_LLM_PROVIDER = "ollama"
DEFAULT_LLM_MODEL = os.getenv("LLM_MODEL") or ""
DEFAULT_LLM_BASE_URL = os.getenv("LLM_BASE_URL") or ""


# Load custom CSS from external file for better maintainability
def _load_custom_css() -> str:
    """Load CSS from external file, with fallback to minimal styles."""
    css_path = Path(__file__).parent / "styles.css"
    if css_path.exists():
        return css_path.read_text(encoding="utf-8")
    # Fallback minimal CSS if file not found
    return """
    .gradio-container { background: #0f0f0f !important; }
    .chatbot { border-radius: 16px !important; }
    """


CUSTOM_CSS = _load_custom_css()


def _format_query_rewrite_debug(info: Optional[QueryRewriteInfo]) -> str:
    if not info:
        return ""

    lines = ["### ğŸ”„ ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼"]

    if not info.used:
        lines.append("- **ìƒíƒœ**: ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ë¯¸ì ìš©")
        lines.append(f"- **ì›ë³¸ ì¿¼ë¦¬**: `{info.original}`")
        return "\n".join(lines)

    # ë°©ë²• í‘œì‹œ
    if info.method == "llm":
        method_label = "ğŸ¤– LLM ê¸°ë°˜ ë¦¬ë¼ì´íŒ…"
    elif info.method == "rules":
        method_label = "ğŸ“‹ ê·œì¹™ ê¸°ë°˜ í™•ì¥ (ë™ì˜ì–´/ì¸í…íŠ¸)"
    else:
        method_label = "â“ ì•Œìˆ˜ì—†ìŒ"

    # ì¶”ê°€ ìƒíƒœ í‘œì‹œ
    status_tags = []
    if info.from_cache:
        status_tags.append("ğŸ“¦ ìºì‹œ íˆíŠ¸")
    if info.fallback:
        status_tags.append("âš ï¸ LLM ì‹¤íŒ¨â†’í´ë°±")
    status_text = " | ".join(status_tags) if status_tags else ""

    lines.append(f"**ë°©ë²•**: {method_label}")
    if status_text:
        lines.append(f"**ìƒíƒœ**: {status_text}")

    # ì¿¼ë¦¬ ë³€í™˜ ê²°ê³¼
    lines.append("")
    lines.append("#### ì¿¼ë¦¬ ë³€í™˜")
    lines.append(f"- **ì›ë³¸**: `{info.original}`")
    if info.original == info.rewritten:
        lines.append("- **ê²°ê³¼**: (ë³€ê²½ ì—†ìŒ)")
    else:
        lines.append(f"- **ë³€í™˜**: `{info.rewritten}`")

    # ë™ì˜ì–´ ì ìš© ì—¬ë¶€
    lines.append("")
    lines.append("#### ì ìš©ëœ ê¸°ë²•")
    if info.used_synonyms is not None:
        if info.used_synonyms:
            lines.append("- ğŸ“š **ë™ì˜ì–´ ì‚¬ì „**: âœ… ì ìš©ë¨ (ìœ ì‚¬ì–´ë¡œ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥)")
        else:
            lines.append("- ğŸ“š **ë™ì˜ì–´ ì‚¬ì „**: â– ë¯¸ì ìš©")

    # ì¸í…íŠ¸ ì ìš© ì—¬ë¶€
    if info.used_intent is not None:
        if info.used_intent:
            lines.append("- ğŸ¯ **ì˜ë„ ì¸ì‹**: âœ… ë§¤ì¹­ë¨")
            if info.matched_intents:
                intents_str = ", ".join([f"`{i}`" for i in info.matched_intents])
                lines.append(f"  - ë§¤ì¹­ëœ ì˜ë„: {intents_str}")
        else:
            lines.append("- ğŸ¯ **ì˜ë„ ì¸ì‹**: â– ë¯¸ë§¤ì¹­")

    return "\n".join(lines)


def _decide_search_mode_ui(query: str) -> str:
    """Auto-detect search mode without manual selection."""
    from .common import decide_search_mode

    return decide_search_mode(query, None)


def _process_with_handler(
    query: str,
    top_k: int,
    include_abolished: bool,
    llm_provider: str,
    llm_model: str,
    llm_base_url: str,
    target_db_path: str,
    audience_override: Optional[Audience],
    use_tools: bool,
    history: List[dict],
    state: dict,
    use_mock_llm: bool = False,
    default_db_path: str = DEFAULT_DB_PATH,
) -> QueryResult:
    """Process query using QueryHandler."""
    db_path_value = target_db_path or default_db_path
    store_for_query = ChromaVectorStore(persist_directory=db_path_value)

    # Initialize LLM client
    llm_client = None
    if not use_mock_llm:
        try:
            llm_client = LLMClientAdapter(
                provider=llm_provider,
                model=llm_model or None,
                base_url=llm_base_url or None,
            )
        except Exception:
            pass  # Will use search only if LLM fails
    else:
        llm_client = MockLLMClient()

    handler = QueryHandler(
        store=store_for_query,
        llm_client=llm_client,
        function_gemma_client=llm_client if use_tools else None,
        use_reranker=True,  # Default to True for Web UI
    )

    context = QueryContext(
        state=state,
        history=history,
        interactive=True,
        last_regulation=state.get("last_regulation"),
        last_rule_code=state.get("last_rule_code"),
    )

    options = QueryOptions(
        top_k=top_k,
        include_abolished=include_abolished,
        audience_override=audience_override,
        use_function_gemma=use_tools,
        llm_provider=llm_provider,
        llm_model=llm_model,
        llm_base_url=llm_base_url,
    )

    return handler.process_query(query, context, options)


def create_app(
    db_path: str = DEFAULT_DB_PATH,
    use_mock_llm: bool = False,
) -> "gr.Blocks":
    """
    Create Gradio app instance with ChatGPT-style interface.

    Args:
        db_path: Path to ChromaDB storage.
        use_mock_llm: Use mock LLM for testing without API key.

    Returns:
        Gradio Blocks app.
    """
    if not GRADIO_AVAILABLE:
        raise ImportError("gradio is required. Install with: uv add gradio")

    # Initialize components
    store = ChromaVectorStore(persist_directory=db_path)
    loader = JSONDocumentLoader()

    llm_status = "LLM ì‚¬ìš© ê°€ëŠ¥"
    if use_mock_llm:
        llm_status = "âš ï¸ Mock LLM (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)"

    # Initialize llm_client for evaluation tab (P2)
    llm_client = None
    if use_mock_llm:
        llm_client = MockLLMClient()
    else:
        try:
            llm_client = LLMClientAdapter()
        except Exception:
            pass  # Will be None if initialization fails

    sync_usecase = SyncUseCase(loader, store)

    data_input_dir = Path("data/input")
    data_output_dir = Path("data/output")
    data_input_dir.mkdir(parents=True, exist_ok=True)
    data_output_dir.mkdir(parents=True, exist_ok=True)

    def _find_latest_json(output_dir: Path) -> Optional[Path]:
        json_files = [
            p
            for p in output_dir.rglob("*.json")
            if not p.name.endswith("_metadata.json")
        ]
        if not json_files:
            return None
        return max(json_files, key=lambda p: p.stat().st_mtime)

    def _list_json_files(output_dir: Path) -> List[Path]:
        return sorted(
            [
                p
                for p in output_dir.rglob("*.json")
                if not p.name.endswith("_metadata.json")
            ],
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )

    auto_sync_message = ""
    if store.count() == 0:
        latest_json = _find_latest_json(data_output_dir)
        if latest_json:
            try:
                result = sync_usecase.incremental_sync(str(latest_json))
                auto_sync_message = f"ìë™ ë™ê¸°í™”: {latest_json.name} ({result})"
            except Exception as e:
                auto_sync_message = f"ìë™ ë™ê¸°í™” ì‹¤íŒ¨: {e}"

    # Get initial status
    def get_status_text() -> str:
        status = sync_usecase.get_sync_status()
        auto_sync_note = f"\n- {auto_sync_message}" if auto_sync_message else ""
        return f"""**ë™ê¸°í™” ìƒíƒœ**
- ë§ˆì§€ë§‰ ë™ê¸°í™”: {status["last_sync"] or "ì—†ìŒ"}
- ê·œì •ì§‘ íŒŒì¼: {status["json_file"] or "ì—†ìŒ"}
- ì¸ë±ì‹±ëœ ê·œì •: {status["store_regulations"]}ê°œ
- ì €ì¥ëœ ì¡°í•­ ìˆ˜: {status["store_chunks"]}ê°œ
- LLM: {llm_status}{auto_sync_note}
"""

    # Initialize use cases
    QueryAnalyzer()
    FullViewUseCase(JSONDocumentLoader())

    def _parse_audience(selection: str) -> Optional[Audience]:
        if selection == "êµìˆ˜":
            return Audience.FACULTY
        if selection == "í•™ìƒ":
            return Audience.STUDENT
        if selection == "ì§ì›":
            return Audience.STAFF
        return None

    def _format_table_matches(
        matches: List[TableMatch],
        table_no: Optional[int],
        label: Optional[str],
    ) -> str:
        label_text = label or "ë³„í‘œ"
        lines = []
        for idx, match in enumerate(matches, 1):
            path = clean_path_segments(match.path) if match.path else []
            heading = " > ".join(path) if path else match.title or label_text
            if table_no:
                table_label = f"{label_text} {table_no}"
            else:
                table_label = infer_attachment_label(match, label_text)
            lines.append(f"### [{idx}] {heading} ({table_label})")
            if match.text:
                lines.append(match.text)
            lines.append(normalize_markdown_table(match.markdown).strip())
        return "\n\n".join([line for line in lines if line])

    def _format_toc(toc: List[str]) -> str:
        if not toc:
            return "ëª©ì°¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        return "### ëª©ì°¨\n" + "\n".join([f"- {t}" for t in toc])

    def _build_sources_markdown(results, query: str, show_debug: bool) -> str:
        sources_md = ["### ğŸ“š ì°¸ê³  ê·œì •\n"]
        norm_scores = normalize_relevance_scores(results) if results else {}
        display_sources = filter_by_relevance(results, norm_scores) if results else []

        for i, r in enumerate(display_sources, 1):
            reg_name = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            path = (
                " > ".join(clean_path_segments(r.chunk.parent_path))
                if r.chunk.parent_path
                else r.chunk.title
            )
            norm_score = norm_scores.get(r.chunk.id, 0.0)
            rel_pct = int(norm_score * 100)
            rel_label = get_relevance_label_combined(rel_pct)
            score_info = f" | AI ì‹ ë¢°ë„: {r.score:.3f}" if show_debug else ""
            snippet = strip_path_prefix(r.chunk.text, r.chunk.parent_path or [])

            # Format regulation references in snippet as links (visual only for now)
            # We use a dummy link that doesn't go anywhere but looks like a link
            snippet_with_links = format_as_markdown_links(
                snippet,
                extract_and_format_references(snippet, "markdown")[0],
                link_template="#",
            )

            # ë§¤ì¹­ ì„¤ëª… ì¶”ê°€
            explanation, _ = format_search_result_with_explanation(
                r, query, show_score=show_debug
            )

            sources_md.append(f"""#### [{i}] {reg_name}
**ê²½ë¡œ:** {path}
**ë§¤ì¹­ ì •ë³´:** {explanation}

{snippet_with_links[:500]}{"..." if len(snippet_with_links) > 500 else ""}

*ê·œì •ë²ˆí˜¸: {r.chunk.rule_code} | ê´€ë ¨ë„: {rel_pct}% {rel_label}{score_info}*

---
""")

        return "\n".join(sources_md)

    def _run_ask_once(
        question: str,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
        audience_override: Optional[Audience],
        show_debug: bool,
        history_text: Optional[str] = None,
        search_query: Optional[str] = None,
    ) -> Tuple[str, str, str, str, str]:
        db_path_value = target_db_path or db_path
        store_for_ask = ChromaVectorStore(persist_directory=db_path_value)
        if store_for_ask.count() == 0:
            return (
                "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.",
                "",
                "",
                "",
                "",
            )

        if use_mock_llm:
            llm_client = MockLLMClient()
        else:
            llm_client = LLMClientAdapter(
                provider=llm_provider,
                model=llm_model or None,
                base_url=llm_base_url or None,
            )

        search_with_llm = SearchUseCase(store_for_ask, llm_client)
        filter = None
        if not include_abolished:
            filter = SearchFilter(status=RegulationStatus.ACTIVE)

        answer = search_with_llm.ask(
            question,
            filter=filter,
            top_k=top_k,
            include_abolished=include_abolished,
            audience_override=audience_override,
            history_text=history_text,
            search_query=search_query,
        )

        answer_text = normalize_markdown_emphasis(answer.text)
        sources_text = _build_sources_markdown(answer.sources, question, show_debug)
        debug_text = ""
        if show_debug:
            debug_text = _format_query_rewrite_debug(
                search_with_llm.get_last_query_rewrite()
            )
        rule_code = answer.sources[0].chunk.rule_code if answer.sources else ""
        top_regulation_title = ""
        if answer.sources:
            top_chunk = answer.sources[0].chunk
            if top_chunk.parent_path:
                top_regulation_title = top_chunk.parent_path[0]
            else:
                top_regulation_title = top_chunk.title
        return answer_text, sources_text, debug_text, rule_code, top_regulation_title

    def _run_ask_stream(
        question: str,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
        audience_override: Optional[Audience],
        show_debug: bool,
        history_text: Optional[str] = None,
        search_query: Optional[str] = None,
    ):
        """
        Streaming version of _run_ask_once.

        Yields:
            dict: Contains type ('progress', 'token', 'sources', 'debug', 'metadata')
                  and corresponding content.
        """
        db_path_value = target_db_path or db_path
        store_for_ask = ChromaVectorStore(persist_directory=db_path_value)
        if store_for_ask.count() == 0:
            yield {
                "type": "error",
                "content": "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.",
            }
            return

        # Progress: Starting search
        yield {"type": "progress", "content": "ğŸ” 1/3 ê·œì • ê²€ìƒ‰ ì¤‘..."}

        if use_mock_llm:
            llm_client = MockLLMClient()
        else:
            llm_client = LLMClientAdapter(
                provider=llm_provider,
                model=llm_model or None,
                base_url=llm_base_url or None,
            )

        search_with_llm = SearchUseCase(store_for_ask, llm_client)
        filter = None
        if not include_abolished:
            filter = SearchFilter(status=RegulationStatus.ACTIVE)

        # Progress: Reranking
        yield {
            "type": "progress",
            "content": "ğŸ” 1/3 ê·œì • ê²€ìƒ‰ ì¤‘...\nğŸ¯ 2/3 ê´€ë ¨ë„ ì¬ì •ë ¬ ì¤‘...",
        }

        sources = []
        rule_code = ""
        regulation_title = ""
        debug_text = ""

        # Use streaming if available
        try:
            for item in search_with_llm.ask_stream(
                question,
                filter=filter,
                top_k=top_k,
                include_abolished=include_abolished,
                audience_override=audience_override,
                history_text=history_text,
                search_query=search_query,
            ):
                if item["type"] == "metadata":
                    sources = item["sources"]
                    # Progress: LLM generating
                    yield {
                        "type": "progress",
                        "content": "ğŸ” 1/3 ê·œì • ê²€ìƒ‰ ì¤‘...\nğŸ¯ 2/3 ê´€ë ¨ë„ ì¬ì •ë ¬ ì¤‘...\nğŸ¤– 3/3 AI ë‹µë³€ ìƒì„± ì¤‘...",
                    }

                    if sources:
                        top_chunk = sources[0].chunk
                        rule_code = top_chunk.rule_code
                        regulation_title = (
                            top_chunk.parent_path[0]
                            if top_chunk.parent_path
                            else top_chunk.title
                        )
                elif item["type"] == "token":
                    yield {"type": "token", "content": item["content"]}
        except Exception:
            # Fallback to non-streaming
            answer = search_with_llm.ask(
                question,
                filter=filter,
                top_k=top_k,
                include_abolished=include_abolished,
                audience_override=audience_override,
                history_text=history_text,
                search_query=search_query,
            )
            sources = answer.sources
            if sources:
                top_chunk = sources[0].chunk
                rule_code = top_chunk.rule_code
                regulation_title = (
                    top_chunk.parent_path[0]
                    if top_chunk.parent_path
                    else top_chunk.title
                )
            yield {"type": "token", "content": answer.text}

        # Send sources and debug info at the end
        sources_text = _build_sources_markdown(sources, question, show_debug)
        if show_debug:
            debug_text = _format_query_rewrite_debug(
                search_with_llm.get_last_query_rewrite()
            )

        yield {"type": "sources", "content": sources_text}
        yield {"type": "debug", "content": debug_text}
        yield {
            "type": "metadata",
            "rule_code": rule_code,
            "regulation_title": regulation_title,
        }

    # Main chat function (stateful)
    def chat_respond(
        msg: str,
        history: List[dict],
        state: dict,
        top_k: int,
        abolished: bool,
        llm_p: str,
        llm_m: str,
        llm_b: str,
        db_path_val: str,
        target_sel: str,
        use_context: bool,
        use_tools: bool,
        show_debug: bool,
    ):
        """Handle chat interaction with streaming."""
        if not msg.strip():
            # Show helpful message for empty input
            history.append(
                {
                    "role": "assistant",
                    "content": "ğŸ’¡ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ: 'íœ´í•™ ì‹ ì²­ ì ˆì°¨', 'êµì› ì—°êµ¬ë…„ ìê²©ì€?'",
                }
            )
            yield history, "", "", state
            return

        # Prepare arguments
        audience_override = (
            _parse_audience(target_sel) if target_sel != "ìë™" else None
        )

        # Build history context if enabled
        history_context = []
        if use_context:
            history_context = history

        # New logic inline here:
        db_path_value = db_path_val or db_path
        store_for_query = ChromaVectorStore(persist_directory=db_path_value)

        llm_client = None
        if not use_mock_llm:
            try:
                llm_client = LLMClientAdapter(
                    provider=llm_p,
                    model=llm_m or None,
                    base_url=llm_b or None,
                )
            except Exception:
                pass
        else:
            llm_client = MockLLMClient()

        handler = QueryHandler(
            store=store_for_query,
            llm_client=llm_client,
            function_gemma_client=llm_client if use_tools else None,
            use_reranker=True,  # Default true for web
        )

        context = QueryContext(
            state=state,
            history=history_context,
            interactive=True,
            last_regulation=state.get("last_regulation"),
            last_rule_code=state.get("last_rule_code"),
        )

        options = QueryOptions(
            top_k=top_k,
            include_abolished=abolished,
            audience_override=audience_override,
            use_function_gemma=use_tools,
            show_debug=show_debug,
            llm_provider=llm_p,
            llm_model=llm_m,
            llm_base_url=llm_b,
        )

        # Start streaming
        # Add user message
        history.append({"role": "user", "content": msg})
        # Initial assistant message for progress
        history.append({"role": "assistant", "content": "ğŸ” 1/3 ê·œì • ê²€ìƒ‰ ì¤‘..."})
        yield history, "", "", state

        current_response = ""
        current_debug = ""
        sources_text = ""

        for event in handler.process_query_stream(msg, context, options):
            evt_type = event["type"]

            if evt_type == "progress":
                history[-1] = {"role": "assistant", "content": event["content"]}
                yield history, "", current_debug, state

            elif evt_type == "token":
                current_response += event["content"]
                history[-1]["content"] = current_response
                yield history, "", current_debug, state

            elif evt_type == "sources":
                sources_text = event["content"]

            elif evt_type == "debug":
                current_debug += f"\n{event['content']}"
                yield (
                    history,
                    "",
                    current_debug,
                    state,
                )  # Yield debug updates immediately

            elif evt_type == "metadata":
                if event.get("rule_code"):
                    state["last_rule_code"] = event["rule_code"]
                if event.get("regulation_title"):
                    state["last_regulation"] = event["regulation_title"]

            elif evt_type == "state":
                # explicit state update
                state.update(event["update"])

            elif evt_type == "clarification":
                clarification_type = event["clarification_type"]
                clarification_options = event["options"]

                state["pending"] = {
                    "type": clarification_type,
                    "options": clarification_options,
                    "query": msg,  # Use original message for pending query
                    "mode": event.get(
                        "mode", "search"
                    ),  # Default to search if mode not specified by handler
                    "table_no": event.get("table_no"),
                    "label": event.get("label"),
                }

                clarified_content = format_clarification(
                    clarification_type, clarification_options
                )
                history[-1] = {"role": "assistant", "content": clarified_content}

                yield history, "", current_debug, state
                return  # Stop processing, waiting for user clarification

            elif evt_type == "error":
                history[-1] = {"role": "assistant", "content": f"âš ï¸ {event['content']}"}
                yield history, "", current_debug, state
                return  # Stop processing on error

            elif evt_type == "complete":
                # Final non-streaming content (e.g. Overview, Search Table) or final LLM answer
                content = event["content"]

                # If it's an LLM answer, sources might be separate.
                # For search results, sources are usually part of the content.
                if (
                    sources_text and "---" not in content[-50:]
                ):  # Avoid duplication if sources already appended
                    content += "\n\n---\n\n" + sources_text

                history[-1] = {
                    "role": "assistant",
                    "content": normalize_markdown_emphasis(content),
                }
                state["last_query"] = msg  # Update last_query with the original message
                # State updates for last_regulation/last_rule_code are handled by 'metadata' event
                # or by the state update from QueryHandler.
                yield history, "", current_debug, state

        # Final yield to ensure everything is settled, especially if no 'complete' event was sent
        # (e.g., if only progress updates were sent and then nothing more)
        # This also ensures the last state is yielded.
        yield history, "", current_debug, state

    def record_web_feedback(query, rule_code, rating, comment):
        """Record feedback from Web UI."""
        if not query or not rule_code:
            return gr.update(value="âš ï¸ í”¼ë“œë°±ì„ ë‚¨ê¸¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", visible=True)

        from ..infrastructure.feedback import FeedbackCollector

        collector = FeedbackCollector()
        collector.record_feedback(
            query=query,
            rule_code=rule_code,
            rating=rating,
            comment=comment or None,
            source="web",
        )
        return gr.update(value="âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!", visible=True)

    def _render_status(target_db_path: str) -> str:
        db_path_value = target_db_path or db_path
        try:
            store_local = ChromaVectorStore(persist_directory=db_path_value)
        except Exception as e:
            return f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"

        sync_state_path = Path("data/sync_state.json")
        last_synced = None
        if sync_state_path.exists():
            try:
                import json

                data = json.loads(sync_state_path.read_text(encoding="utf-8"))
                last_synced = data.get("json_file")
            except Exception:
                last_synced = None

        json_files = _list_json_files(data_output_dir)

        lines = []
        lines.append("## DB ìƒíƒœ")
        lines.append(f"- DB ê²½ë¡œ: `{db_path_value}`")
        lines.append(f"- ì €ì¥ëœ ì¡°í•­ ìˆ˜: {store_local.count()}")
        lines.append(f"- ê·œì • ìˆ˜: {len(store_local.get_all_rule_codes())}")
        if last_synced:
            lines.append(f"- **ê·œì •ì§‘: `{last_synced}`**")

        lines.append("\n## JSON íŒŒì¼ ëª©ë¡ (`data/output`)")
        if json_files:
            lines.append("| íŒŒì¼ | ìˆ˜ì • ì‹œê° | í¬ê¸° | ë§ˆì§€ë§‰ ë™ê¸°í™” |")
            lines.append("|---|---|---|---|")
            for p in json_files:
                mtime = datetime.fromtimestamp(p.stat().st_mtime).strftime(
                    "%Y-%m-%d %H:%M"
                )
                size_kb = f"{p.stat().st_size / 1024:.1f} KB"
                is_synced = "âœ…" if last_synced and p.name == last_synced else ""
                lines.append(f"| `{p.name}` | {mtime} | {size_kb} | {is_synced} |")
        else:
            lines.append("- JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        return "\n".join(lines)

    # Build UI (theme/css are passed to launch() for Gradio 6.0 compatibility)
    with gr.Blocks(
        title="ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A",
    ) as app:
        # Header - Minimal & Clean
        gr.HTML("""
            <div style="text-align: center; padding: 28px 20px 20px;">
                <h1 style="font-size: 1.6rem; font-weight: 600; color: #fafafa;
                           letter-spacing: -0.025em; margin: 0;">
                    ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A
                </h1>
                <p style="color: #a3a3a3; margin-top: 6px; font-size: 0.9rem; font-weight: 400;">
                    ì§ˆë¬¸í•˜ë©´ AIê°€ ë‹µë³€í•˜ê³ , ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ ê·œì •ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤
                </p>
            </div>
        """)

        with gr.Tabs():
            # Tab 1: Chat (Main)
            with gr.TabItem("ğŸ’¬ ì±„íŒ…"):
                with gr.Row():
                    # Main chat area
                    with gr.Column(scale=3):
                        # Navigation Buttons
                        with gr.Row():
                            btn_back = gr.Button("â—€ ë’¤ë¡œ", size="sm", interactive=False)
                            btn_forward = gr.Button(
                                "ì•ìœ¼ë¡œ â–¶", size="sm", interactive=False
                            )
                            # Spacer
                            gr.HTML("<div style='flex-grow: 1;'></div>")

                        chat_bot = gr.Chatbot(
                            label="",
                            height=500,
                            show_label=False,
                            value=[
                                {
                                    "role": "assistant",
                                    "content": "ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”! ëŒ€í•™ ê·œì •ì„ ê²€ìƒ‰í•˜ê±°ë‚˜ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nğŸ’¡ ì•„ë˜ ì˜ˆì‹œ ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜ ì§ì ‘ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                                }
                            ],
                            avatar_images=("ğŸ‘¤", "ğŸ¤–"),
                            # bubble_full_width removed for Gradio 6.0 compatibility
                        )

                        # Input area
                        with gr.Row():
                            chat_input = gr.Textbox(
                                placeholder="ì§ˆë¬¸ì´ë‚˜ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: íœ´í•™ ì‹ ì²­ ì ˆì°¨ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?)",
                                lines=1,
                                show_label=False,
                                scale=6,
                                container=False,
                            )
                            chat_send = gr.Button(
                                "ì „ì†¡",
                                variant="primary",
                                scale=1,
                                min_width=80,
                            )

                        # Example queries as clickable cards
                        gr.Markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
                        with gr.Row():
                            ex1 = gr.Button(
                                "ğŸ“ íœ´í•™ ì‹ ì²­ ì ˆì°¨ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", size="sm"
                            )
                            ex2 = gr.Button("ğŸ“– êµì›ì¸ì‚¬ê·œì • ì „ë¬¸", size="sm")
                            ex3 = gr.Button("ğŸ” êµì› ì—°êµ¬ë…„", size="sm")
                        with gr.Row():
                            ex4 = gr.Button("ğŸ“‹ í•™ì¹™ ë³„í‘œ 1", size="sm")
                            ex5 = gr.Button("ğŸ˜¢ í•™êµ ê·¸ë§Œë‘ê³  ì‹¶ì–´ìš”", size="sm")

                        # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ì„ ì˜ˆì‹œ ë²„íŠ¼ê³¼ ë¶„ë¦¬
                        gr.Markdown("---")
                        chat_clear = gr.Button(
                            "ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", variant="secondary", size="sm"
                        )

                    # Settings sidebar
                    with gr.Column(scale=1):
                        gr.Markdown("### âš™ï¸ ì„¤ì •")

                        chat_top_k = gr.Slider(
                            minimum=1, maximum=20, value=5, step=1, label="ê²°ê³¼ ìˆ˜"
                        )
                        chat_abolished = gr.Checkbox(
                            label="íì§€ ê·œì • í¬í•¨", value=False
                        )
                        chat_target = gr.Radio(
                            choices=["ìë™", "êµìˆ˜", "í•™ìƒ", "ì§ì›"],
                            value="ìë™",
                            label="ëŒ€ìƒ ì„ íƒ",
                        )
                        chat_context = gr.Checkbox(label="ëŒ€í™” ë¬¸ë§¥ í™œìš©", value=True)
                        chat_use_tools = gr.Checkbox(
                            label="ğŸ› ï¸ Tool Calling ì‚¬ìš©",
                            value=True,
                            info="FunctionGemmaë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.",
                        )
                        chat_debug = gr.Checkbox(label="ë””ë²„ê·¸ ì¶œë ¥", value=False)

                        with gr.Accordion("ğŸ¤– LLM ì„¤ì •", open=False):
                            chat_llm_p = gr.Dropdown(
                                choices=LLM_PROVIDERS,
                                value=DEFAULT_LLM_PROVIDER,
                                label="Provider",
                            )
                            chat_llm_m = gr.Textbox(
                                value=DEFAULT_LLM_MODEL, label="Model"
                            )
                            chat_llm_b = gr.Textbox(
                                value=DEFAULT_LLM_BASE_URL, label="Base URL"
                            )

                        # Detail panelì€ ìˆ¨ê¹€ ì²˜ë¦¬ (ì±„íŒ…ì°½ì— ì§ì ‘ í‘œì‹œ)
                        chat_detail = gr.Markdown(visible=False)

                        with gr.Accordion("ğŸ”§ ë””ë²„ê·¸", open=False):
                            chat_debug_out = gr.Markdown()

                chat_state = gr.State(
                    {
                        "audience": None,
                        "pending": None,
                        "last_query": None,
                        "last_mode": None,
                        "last_regulation": None,
                        "last_rule_code": None,
                        "nav_history": [],  # List of (mode, query, regulation)
                        "nav_index": -1,
                    }
                )

                # Navigation Logic
                def update_nav_buttons(state):
                    history = state.get("nav_history", [])
                    index = state.get("nav_index", -1)
                    has_back = index > 0
                    has_forward = index < len(history) - 1
                    return (
                        gr.update(interactive=has_back),
                        gr.update(interactive=has_forward),
                        state,
                    )

                def confirm_navigation(state, direction):
                    history = state.get("nav_history", [])
                    index = state.get("nav_index", -1)

                    new_index = index + direction
                    if 0 <= new_index < len(history):
                        state["nav_index"] = new_index
                        mode, query, regulation = history[new_index]
                        return query, state
                    return None, state

                db_state = gr.State(db_path)

                # Event handlers
                def on_submit(
                    msg,
                    history,
                    state,
                    top_k,
                    abolished,
                    llm_p,
                    llm_m,
                    llm_b,
                    db,
                    target,
                    context,
                    use_tools,
                    debug,
                ):
                    # Update History for Navigation
                    # Logic: If query changes effectively (new search or view), apend to history
                    # We need to capture the FINAL state to update navigation

                    # Store previous state to detect change
                    prev_query = state.get("last_query")
                    prev_mode = state.get("last_mode")

                    final_state = state
                    for result in chat_respond(
                        msg,
                        history,
                        state,
                        top_k,
                        abolished,
                        llm_p,
                        llm_m,
                        llm_b,
                        db,
                        target,
                        context,
                        use_tools,
                        debug,
                    ):
                        # Unpack result and add empty string for input clear
                        hist, detail, dbg, st = result
                        final_state = st
                        yield hist, detail, dbg, st, ""

                    # After generation, update navigation history if meaningful change
                    curr_query = final_state.get("last_query")
                    curr_mode = final_state.get("last_mode")

                    if curr_query and (
                        curr_query != prev_query or curr_mode != prev_mode
                    ):
                        # Append to history
                        nav_history = final_state.get("nav_history", [])
                        nav_index = final_state.get("nav_index", -1)

                        # If we were back in history, truncate future
                        if nav_index < len(nav_history) - 1:
                            nav_history = nav_history[: nav_index + 1]

                        nav_history.append(
                            (curr_mode, curr_query, final_state.get("last_regulation"))
                        )
                        final_state["nav_history"] = nav_history
                        final_state["nav_index"] = len(nav_history) - 1

                        yield hist, detail, dbg, final_state, ""

                def on_back_click(
                    history,
                    state,
                    top_k,
                    abolished,
                    llm_p,
                    llm_m,
                    llm_b,
                    db,
                    target,
                    context,
                    use_tools,
                    debug,
                ):
                    query, new_state = confirm_navigation(state, -1)
                    if query:
                        # Re-run query
                        for res in on_submit(
                            query,
                            history,
                            new_state,
                            top_k,
                            abolished,
                            llm_p,
                            llm_m,
                            llm_b,
                            db,
                            target,
                            context,
                            use_tools,
                            debug,
                        ):
                            yield res
                    else:
                        yield history, "", "", state, ""

                def on_forward_click(
                    history,
                    state,
                    top_k,
                    abolished,
                    llm_p,
                    llm_m,
                    llm_b,
                    db,
                    target,
                    context,
                    use_tools,
                    debug,
                ):
                    query, new_state = confirm_navigation(state, 1)
                    if query:
                        for res in on_submit(
                            query,
                            history,
                            new_state,
                            top_k,
                            abolished,
                            llm_p,
                            llm_m,
                            llm_b,
                            db,
                            target,
                            context,
                            use_tools,
                            debug,
                        ):
                            yield res
                    else:
                        yield history, "", "", state, ""

                # Redefine on_submit to include button updates
                def on_submit_with_nav(
                    msg,
                    history,
                    state,
                    top_k,
                    abolished,
                    llm_p,
                    llm_m,
                    llm_b,
                    db,
                    target,
                    context,
                    use_tools,
                    debug,
                ):
                    # Wrap the generator
                    gen = on_submit(
                        msg,
                        history,
                        state,
                        top_k,
                        abolished,
                        llm_p,
                        llm_m,
                        llm_b,
                        db,
                        target,
                        context,
                        use_tools,
                        debug,
                    )
                    for res in gen:
                        hist, detail, dbg, st, inp = res
                        # Calc button state
                        nav_history = st.get("nav_history", [])
                        nav_index = st.get("nav_index", -1)
                        has_back = nav_index > 0
                        has_forward = nav_index < len(nav_history) - 1

                        yield (
                            hist,
                            detail,
                            dbg,
                            st,
                            inp,
                            gr.update(interactive=has_back),
                            gr.update(interactive=has_forward),
                        )

                chat_send.click(
                    fn=on_submit_with_nav,
                    inputs=[
                        chat_input,
                        chat_bot,
                        chat_state,
                        chat_top_k,
                        chat_abolished,
                        chat_llm_p,
                        chat_llm_m,
                        chat_llm_b,
                        db_state,
                        chat_target,
                        chat_context,
                        chat_use_tools,
                        chat_debug,
                    ],
                    outputs=[
                        chat_bot,
                        chat_detail,
                        chat_debug_out,
                        chat_state,
                        chat_input,
                        btn_back,
                        btn_forward,
                    ],
                )
                chat_input.submit(
                    fn=on_submit_with_nav,
                    inputs=[
                        chat_input,
                        chat_bot,
                        chat_state,
                        chat_top_k,
                        chat_abolished,
                        chat_llm_p,
                        chat_llm_m,
                        chat_llm_b,
                        db_state,
                        chat_target,
                        chat_context,
                        chat_use_tools,
                        chat_debug,
                    ],
                    outputs=[
                        chat_bot,
                        chat_detail,
                        chat_debug_out,
                        chat_state,
                        chat_input,
                        btn_back,
                        btn_forward,
                    ],
                )

                # Wire up Back/Forward
                btn_back.click(
                    fn=on_back_click,
                    inputs=[
                        chat_bot,
                        chat_state,
                        chat_top_k,
                        chat_abolished,
                        chat_llm_p,
                        chat_llm_m,
                        chat_llm_b,
                        db_state,
                        chat_target,
                        chat_context,
                        chat_use_tools,
                        chat_debug,
                    ],
                    outputs=[
                        chat_bot,
                        chat_detail,
                        chat_debug_out,
                        chat_state,
                        chat_input,
                        btn_back,
                        btn_forward,
                    ],
                )
                btn_forward.click(
                    fn=on_forward_click,
                    inputs=[
                        chat_bot,
                        chat_state,
                        chat_top_k,
                        chat_abolished,
                        chat_llm_p,
                        chat_llm_m,
                        chat_llm_b,
                        db_state,
                        chat_target,
                        chat_context,
                        chat_use_tools,
                        chat_debug,
                    ],
                    outputs=[
                        chat_bot,
                        chat_detail,
                        chat_debug_out,
                        chat_state,
                        chat_input,
                        btn_back,
                        btn_forward,
                    ],
                )

                chat_clear.click(
                    fn=lambda: (
                        [],
                        "",  # chat_detailì€ ì´ì œ ë¹ˆ ê°’ (ì±„íŒ…ì°½ì— ì§ì ‘ í‘œì‹œ)
                        "",
                        {
                            "audience": None,
                            "pending": None,
                            "last_query": None,
                            "last_mode": None,
                            "last_regulation": None,
                            "last_rule_code": None,
                        },
                    ),
                    inputs=[],
                    outputs=[chat_bot, chat_detail, chat_debug_out, chat_state],
                )

                # Example button handlers
                def fill_example(example_text):
                    return example_text

                ex1.click(
                    fn=lambda: "íœ´í•™ ì‹ ì²­ ì ˆì°¨ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", outputs=[chat_input]
                )
                ex2.click(fn=lambda: "êµì›ì¸ì‚¬ê·œì • ì „ë¬¸", outputs=[chat_input])
                ex3.click(fn=lambda: "êµì› ì—°êµ¬ë…„", outputs=[chat_input])
                ex4.click(fn=lambda: "í•™ì¹™ ë³„í‘œ 1", outputs=[chat_input])
                ex5.click(fn=lambda: "í•™êµ ê·¸ë§Œë‘ê³  ì‹¶ì–´ìš”", outputs=[chat_input])

            # Tab 2: Status
            with gr.TabItem("ğŸ“‚ ë°ì´í„° í˜„í™©"):
                gr.Markdown(
                    "> DB ê´€ë¦¬(ë™ê¸°í™”, ì´ˆê¸°í™”)ëŠ” CLIì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤: `regulation sync`, `regulation reset`"
                )

                status_db_path = gr.Textbox(
                    value=db_path,
                    label="DB ê²½ë¡œ",
                    interactive=False,
                )
                status_markdown = gr.Markdown(_render_status(db_path))
                refresh_btn = gr.Button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", variant="secondary")

                def _refresh_status_only(target_db_path: str):
                    return _render_status(target_db_path)

                refresh_btn.click(
                    fn=_refresh_status_only,
                    inputs=[status_db_path],
                    outputs=[status_markdown],
                )

            # Tab 3: Live Monitor (Phase 4-5)
            with gr.TabItem("ğŸ“¡ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°"):
                _create_live_monitor_tab(db_path)

            # Tab 4: Quality Evaluation (P2)
            with gr.TabItem("ğŸ“Š í’ˆì§ˆ í‰ê°€"):
                _create_evaluation_tab(db_path, llm_client if not use_mock_llm else None)

    return app


def _create_evaluation_tab(db_path: str, llm_client):
    """Create quality evaluation tab with P2 components."""
    gr.Markdown("### ğŸ¯ RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€")
    gr.Markdown("BatchEvaluationExecutor, ProgressReporter, FailureClassifierë¥¼ í™œìš©í•œ ì¢…í•© í‰ê°€")

    with gr.Row():
        # Left column: Settings and controls
        with gr.Column(scale=1):
            gr.Markdown("#### âš™ï¸ í‰ê°€ ì„¤ì •")

            eval_personas = gr.Dropdown(
                choices=[
                    "all",
                    "student-undergraduate",
                    "student-graduate",
                    "professor",
                    "staff-admin",
                    "parent",
                    "student-international",
                ],
                value="all",
                label="í˜ë¥´ì†Œë‚˜ ì„ íƒ",
                multiselect=True,
            )
            eval_queries = gr.Slider(
                minimum=5, maximum=50, value=25, step=5,
                label="í˜ë¥´ì†Œë‚˜ë‹¹ ì¿¼ë¦¬ ìˆ˜"
            )
            eval_batch_size = gr.Slider(
                minimum=1, maximum=10, value=5, step=1,
                label="ë°°ì¹˜ í¬ê¸°"
            )
            eval_threshold = gr.Slider(
                minimum=0.4, maximum=0.8, value=0.6, step=0.05,
                label="ì‹¤íŒ¨ ì„ê³„ê°’"
            )

            gr.Markdown("#### ğŸ® ì‹¤í–‰ ì œì–´")

            with gr.Row():
                eval_run_btn = gr.Button("â–¶ í‰ê°€ ì‹œì‘", variant="primary")
                eval_resume_btn = gr.Button("âµ ì¬ê°œ", variant="secondary")
                eval_stop_btn = gr.Button("â¹ ì¤‘ì§€", variant="stop")

            eval_session_id = gr.Textbox(
                label="ì„¸ì…˜ ID",
                placeholder="ì¬ê°œí•  ì„¸ì…˜ ID ì…ë ¥",
            )

        # Right column: Progress and results
        with gr.Column(scale=2):
            gr.Markdown("#### ğŸ“ˆ ì§„í–‰ ìƒí™©")

            eval_progress_bar = gr.Textbox(
                label="ì§„í–‰ë¥ ",
                value="í‰ê°€ ëŒ€ê¸° ì¤‘...",
                interactive=False,
            )
            eval_eta = gr.Textbox(
                label="ì˜ˆìƒ ì™„ë£Œ ì‹œê°„",
                value="-",
                interactive=False,
            )
            eval_status = gr.Textbox(
                label="ìƒíƒœ",
                value="ëŒ€ê¸° ì¤‘",
                interactive=False,
            )

    # Results section
    gr.Markdown("---")
    gr.Markdown("#### ğŸ“‹ í‰ê°€ ê²°ê³¼")

    with gr.Row():
        with gr.Column(scale=1):
            eval_metrics = gr.Dataframe(
                headers=["ë©”íŠ¸ë¦­", "ê°’", "ëª©í‘œ", "ìƒíƒœ"],
                datatype=["str", "str", "str", "str"],
                value=[
                    ["Faithfulness", "-", "0.90", "-"],
                    ["Answer Relevancy", "-", "0.85", "-"],
                    ["Contextual Precision", "-", "0.80", "-"],
                    ["Contextual Recall", "-", "0.80", "-"],
                    ["Overall Score", "-", "0.85", "-"],
                ],
                label="ë©”íŠ¸ë¦­ë³„ ì ìˆ˜",
                interactive=False,
            )

        with gr.Column(scale=1):
            eval_summary = gr.Markdown(
                value="í‰ê°€ë¥¼ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.",
                label="í‰ê°€ ìš”ì•½",
            )

    # Failure analysis and recommendations
    gr.Markdown("---")
    gr.Markdown("#### ğŸ” ì‹¤íŒ¨ ë¶„ì„ ë° ê°œì„  ê¶Œì¥ì‚¬í•­")

    with gr.Row():
        eval_failures = gr.Dataframe(
            headers=["ì‹¤íŒ¨ ìœ í˜•", "ê±´ìˆ˜", "ë¹„ìœ¨"],
            datatype=["str", "str", "str"],
            value=[],
            label="ì‹¤íŒ¨ ìœ í˜• ë¶„ì„",
            interactive=False,
        )
        eval_recommendations = gr.Markdown(
            value="í‰ê°€ ì™„ë£Œ í›„ ê°œì„  ê¶Œì¥ì‚¬í•­ì´ í‘œì‹œë©ë‹ˆë‹¤.",
            label="ê°œì„  ê¶Œì¥ì‚¬í•­",
        )

    # SPEC Generation
    gr.Markdown("---")
    gr.Markdown("#### ğŸ“ SPEC ë¬¸ì„œ ìƒì„±")

    with gr.Row():
        eval_gen_spec_btn = gr.Button("ğŸ“„ SPEC ë¬¸ì„œ ìƒì„±", variant="secondary")
        eval_spec_output = gr.Code(
            language="markdown",
            label="ìƒì„±ëœ SPEC",
            value="# SPEC ë¬¸ì„œ\n\ní‰ê°€ ì™„ë£Œ í›„ ìƒì„± ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
            lines=10,
        )

    # Event handlers
    def run_evaluation(personas, queries_per_persona, batch_size, threshold, progress=gr.Progress()):
        """Run full evaluation with progress tracking."""
        try:
            from ..application.evaluation import CheckpointManager, ProgressReporter
            from ..domain.evaluation import (
                FailureClassifier,
                PersonaManager,
                RecommendationEngine,
                RAGQualityEvaluator,
            )

            # Initialize components
            persona_mgr = PersonaManager()
            checkpoint_mgr = CheckpointManager(checkpoint_dir="data/checkpoints")
            evaluator = RAGQualityEvaluator(judge_model="gpt-4o", use_ragas=True)

            # Determine personas
            if "all" in personas or not personas:
                target_personas = persona_mgr.list_personas()
            else:
                target_personas = list(personas)

            total_queries = len(target_personas) * queries_per_persona
            persona_counts = {p: queries_per_persona for p in target_personas}
            reporter = ProgressReporter(persona_counts=persona_counts)

            # Create session
            import uuid
            session_id = f"eval-{uuid.uuid4().hex[:8]}"
            checkpoint_mgr.create_session(
                session_id=session_id,
                total_queries=total_queries,
                personas=target_personas,
            )

            # Initialize RAG
            from ..application.search_usecase import SearchUseCase
            from ..infrastructure.chroma_store import ChromaVectorStore

            vector_store = ChromaVectorStore(persist_directory=db_path)
            search_usecase = SearchUseCase(
                store=vector_store,
                llm_client=llm_client,
                use_reranker=True,
            )

            results = []
            completed = 0

            for persona_id in target_personas:
                queries = persona_mgr.generate_queries(persona_id, count=queries_per_persona)

                for query in queries:
                    try:
                        # Search
                        search_results = search_usecase.search(query_text=query, top_k=5)
                        contexts = [r.chunk.text for r in search_results] if search_results else []

                        if not contexts:
                            continue

                        # Generate answer
                        from ..infrastructure.tool_executor import ToolExecutor
                        tool_executor = ToolExecutor(
                            search_usecase=search_usecase,
                            llm_client=llm_client,
                        )
                        answer = tool_executor._handle_generate_answer(
                            {"question": query, "context": "\n\n".join(contexts)}
                        )

                        if not answer:
                            continue

                        # Evaluate
                        result = evaluator.evaluate_single_turn(query, contexts, answer)
                        result.persona = persona_id
                        results.append(result)

                        # Update progress
                        reporter.update(persona=persona_id, query_id=f"q_{completed}", score=result.overall_score)
                        completed += 1
                        progress(completed / total_queries, desc=f"í‰ê°€ ì¤‘: {query[:30]}...")

                    except Exception:
                        pass

            # Calculate metrics
            if results:
                avg_faithfulness = sum(r.faithfulness for r in results if hasattr(r, 'faithfulness')) / len(results)
                avg_relevancy = sum(r.answer_relevancy for r in results if hasattr(r, 'answer_relevancy')) / len(results)
                avg_precision = sum(r.contextual_precision for r in results if hasattr(r, 'contextual_precision')) / len(results)
                avg_recall = sum(r.contextual_recall for r in results if hasattr(r, 'contextual_recall')) / len(results)
                avg_overall = sum(r.overall_score for r in results) / len(results)

                metrics_data = [
                    ["Faithfulness", f"{avg_faithfulness:.2f}", "0.90", "âœ…" if avg_faithfulness >= 0.90 else "âŒ"],
                    ["Answer Relevancy", f"{avg_relevancy:.2f}", "0.85", "âœ…" if avg_relevancy >= 0.85 else "âŒ"],
                    ["Contextual Precision", f"{avg_precision:.2f}", "0.80", "âœ…" if avg_precision >= 0.80 else "âŒ"],
                    ["Contextual Recall", f"{avg_recall:.2f}", "0.80", "âœ…" if avg_recall >= 0.80 else "âŒ"],
                    ["Overall Score", f"{avg_overall:.2f}", "0.85", "âœ…" if avg_overall >= 0.85 else "âŒ"],
                ]

                # Classify failures
                classifier = FailureClassifier()
                failures = classifier.classify_batch(results)

                failures_data = [
                    [f.failure_type.value, str(f.count), f"{f.count/len(results)*100:.1f}%"]
                    for f in failures
                ]

                # Generate recommendations
                engine = RecommendationEngine()
                failure_counts = {f.failure_type: f.count for f in failures}
                recommendations = engine.generate_recommendations(failure_counts, threshold=1)

                rec_text = "### ê°œì„  ê¶Œì¥ì‚¬í•­\n\n"
                for rec in recommendations[:5]:
                    rec_text += f"**{rec.title}** ({rec.priority.value})\n"
                    rec_text += f"- {rec.description}\n"
                    rec_text += f"- ì˜ˆìƒ íš¨ê³¼: {rec.impact_estimate}\n\n"

                summary_text = f"""
### í‰ê°€ ìš”ì•½

- **ì„¸ì…˜ ID**: {session_id}
- **í‰ê°€ëœ ì¿¼ë¦¬**: {len(results)}ê°œ
- **í‰ê·  ì ìˆ˜**: {avg_overall:.2f}
- **í•©ê²©ë¥ **: {sum(1 for r in results if r.overall_score >= threshold)/len(results)*100:.1f}%
"""

                return (
                    f"ì™„ë£Œ: {completed}/{total_queries} (100%)",
                    "ì™„ë£Œ",
                    f"ì„¸ì…˜ {session_id} ì™„ë£Œ",
                    metrics_data,
                    summary_text,
                    failures_data,
                    rec_text,
                )

            return (
                f"ì™„ë£Œ: {completed}/{total_queries}",
                "-",
                "í‰ê°€ ì™„ë£Œ (ê²°ê³¼ ì—†ìŒ)",
                [],
                "í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
                [],
                "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            )

        except Exception as e:
            return (
                "ì˜¤ë¥˜ ë°œìƒ",
                "-",
                f"ì˜¤ë¥˜: {str(e)}",
                [],
                f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                [],
                "",
            )

    def generate_spec_from_results():
        """Generate SPEC document from latest failures."""
        try:
            from ..domain.evaluation import (
                FailureClassifier,
                RecommendationEngine,
                SPECGenerator,
            )
            from ..infrastructure.storage.evaluation_store import EvaluationStore

            # Get recent evaluations
            store = EvaluationStore(storage_dir="data/evaluations")
            evaluations = store.get_evaluations(max_score=0.6, limit=50)

            if not evaluations:
                return "# SPEC ë¬¸ì„œ\n\në¶„ì„í•  ì‹¤íŒ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # Classify and generate SPEC
            classifier = FailureClassifier()
            failures = classifier.classify_batch(evaluations)

            engine = RecommendationEngine()
            failure_counts = {f.failure_type: f.count for f in failures}
            recommendations = engine.generate_recommendations(failure_counts, threshold=1)

            generator = SPECGenerator()
            spec = generator.generate_spec(failures=failures, recommendations=recommendations)

            return spec.to_markdown()

        except Exception as e:
            return f"# ì˜¤ë¥˜\n\nSPEC ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def resume_evaluation(session_id):
        """Resume interrupted evaluation."""
        from ..application.evaluation import ResumeController, CheckpointManager

        if not session_id:
            return "ì„¸ì…˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "-", "ëŒ€ê¸° ì¤‘", [], "", [], ""

        checkpoint_mgr = CheckpointManager(checkpoint_dir="data/checkpoints")
        resume_ctrl = ResumeController(checkpoint_manager=checkpoint_mgr)

        can_resume, reason = resume_ctrl.can_resume(session_id)
        if not can_resume:
            return f"ì¬ê°œ ë¶ˆê°€: {reason}", "-", "ì¬ê°œ ì‹¤íŒ¨", [], "", [], ""

        context = resume_ctrl.get_resume_context(session_id)
        if not context:
            return "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "-", "ì¬ê°œ ì‹¤íŒ¨", [], "", [], ""

        return (
            f"ì¬ê°œ: {context.completed_count}/{context.total_count}",
            f"ë‚¨ì€ ì¿¼ë¦¬: {context.total_count - context.completed_count}",
            f"ì„¸ì…˜ {session_id} ì¬ê°œ ì¤€ë¹„ë¨",
            [],
            f"ì„¸ì…˜ ì¬ê°œ ì •ë³´:\n- ì™„ë£Œìœ¨: {context.completion_rate:.1f}%\n- ë‚¨ì€ í˜ë¥´ì†Œë‚˜: {', '.join(context.remaining_personas)}",
            [],
            "ì¬ê°œ í›„ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.",
        )

    # Connect event handlers
    eval_run_btn.click(
        fn=run_evaluation,
        inputs=[eval_personas, eval_queries, eval_batch_size, eval_threshold],
        outputs=[
            eval_progress_bar,
            eval_eta,
            eval_status,
            eval_metrics,
            eval_summary,
            eval_failures,
            eval_recommendations,
        ],
    )

    eval_resume_btn.click(
        fn=resume_evaluation,
        inputs=[eval_session_id],
        outputs=[
            eval_progress_bar,
            eval_eta,
            eval_status,
            eval_metrics,
            eval_summary,
            eval_failures,
            eval_recommendations,
        ],
    )

    eval_gen_spec_btn.click(
        fn=generate_spec_from_results,
        inputs=[],
        outputs=[eval_spec_output],
    )

    eval_stop_btn.click(
        fn=lambda: ("ì¤‘ì§€ë¨", "-", "ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë¨", [], "", [], ""),
        inputs=[],
        outputs=[
            eval_progress_bar,
            eval_eta,
            eval_status,
            eval_metrics,
            eval_summary,
            eval_failures,
            eval_recommendations,
        ],
    )


def _create_live_monitor_tab(db_path: str):
    """Create Live Monitor tab for real-time RAG pipeline monitoring.

    This implements Phase 4-5 of SPEC-RAG-MONITOR-001.
    """
    from .web.live_monitor import LiveMonitor

    gr.Markdown("### ğŸ“¡ ì‹¤ì‹œê°„ RAG íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§")
    gr.Markdown("RAG íŒŒì´í”„ë¼ì¸ì˜ ì‹¤í–‰ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.")

    # Initialize monitor
    monitor = LiveMonitor()

    # Import EventType for event filtering
    from ..infrastructure.logging.events import EventType

    with gr.Row():
        # Left column: Event timeline
        with gr.Column(scale=2):
            gr.Markdown("#### ğŸ“Š ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸")

            with gr.Row():
                # Event type filter
                event_filter = gr.Dropdown(
                    choices=["ì „ì²´"] + [et.value for et in EventType],
                    value="ì „ì²´",
                    label="ì´ë²¤íŠ¸ ìœ í˜• í•„í„°",
                    scale=2,
                )

                # Refresh button
                refresh_btn = gr.Button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", variant="secondary", scale=1)

            # Event display (Dataframe)
            event_display = gr.Dataframe(
                headers=["ì‹œê°„", "ìœ í˜•", "ìš”ì•½"],
                datatype=["str", "str", "str"],
                value=[],
                label="ì´ë²¤íŠ¸",
                interactive=False,
                wrap=True,
                # max_rows removed for Gradio 6.0 compatibility
            )

            # Clear events button
            clear_btn = gr.Button("ğŸ—‘ï¸ ì´ë²¤íŠ¸ ì§€ìš°ê¸°", variant="secondary", size="sm")

        # Right column: Query testing
        with gr.Column(scale=1):
            gr.Markdown("#### ğŸ§ª ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")

            query_input = gr.Textbox(
                label="í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬",
                placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: íœ´í•™ ì‹ ì²­ ì ˆì°¨)",
                lines=2,
            )

            query_top_k = gr.Slider(
                minimum=1,
                maximum=10,
                value=5,
                step=1,
                label="ê²°ê³¼ ìˆ˜ (top_k)",
            )

            submit_btn = gr.Button("â–¶ ì‹¤í–‰", variant="primary")

            # Result display
            result_display = gr.Markdown(
                value="ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.",
                label="ê²°ê³¼",
            )

            # Event count
            event_count = gr.Textbox(
                label="ìº¡ì²˜ëœ ì´ë²¤íŠ¸ ìˆ˜",
                value="0",
                interactive=False,
            )

    # Event handlers
    def refresh_events(filter_type: str):
        """Refresh event display."""
        events = monitor.get_events_for_gradio()

        if filter_type != "ì „ì²´":
            # Filter by event type
            events = [e for e in events if e[1] == filter_type]

        return events, str(len(events))

    def run_query(query: str, top_k: int):
        """Run test query and return results."""
        if not query.strip():
            return "ì¿¼ë¦¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "0"

        result = monitor.submit_query(query, top_k=top_k)

        if not result.get("success"):
            return f"âŒ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}", str(result.get("event_count", 0))

        # Format result
        output = f"### ê²°ê³¼\n\n"
        output += f"**ì¿¼ë¦¬**: {result['query']}\n\n"
        output += f"**ì‘ë‹µ ìœ í˜•**: {result.get('result_type', 'unknown')}\n\n"
        output += f"**ìº¡ì²˜ëœ ì´ë²¤íŠ¸**: {result.get('event_count', 0)}ê°œ\n\n"

        if result.get("result"):
            # Truncate long results
            result_text = result['result']
            if len(result_text) > 500:
                result_text = result_text[:500] + "..."
            output += f"---\n\n{result_text}"

        return output, str(result.get("event_count", 0))

    def clear_events():
        """Clear all events from buffer."""
        monitor.clear_events()
        return [], "0"

    # Wire up event handlers
    refresh_btn.click(
        fn=refresh_events,
        inputs=[event_filter],
        outputs=[event_display, event_count],
    )

    submit_btn.click(
        fn=run_query,
        inputs=[query_input, query_top_k],
        outputs=[result_display, event_count],
    )

    clear_btn.click(
        fn=clear_events,
        inputs=[],
        outputs=[event_display, event_count],
    )


# Alias for backward compatibility with tests
create_demo = create_app


def main():
    """Launch Gradio app."""

    parser = argparse.ArgumentParser(description="ê·œì •ì§‘ RAG ì›¹ UI")
    parser.add_argument("--port", type=int, default=7860, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--share", action="store_true", help="ê³µê°œ ë§í¬ ìƒì„±")
    parser.add_argument("--mock-llm", action="store_true", help="Mock LLM ì‚¬ìš©")
    parser.add_argument("--db-path", default=DEFAULT_DB_PATH, help="DB ê²½ë¡œ")

    args = parser.parse_args()

    app = create_app(db_path=args.db_path, use_mock_llm=args.mock_llm)
    app.launch(
        server_port=args.port,
        share=args.share,
        show_error=True,
        css=CUSTOM_CSS,
        theme=gr.themes.Soft(
            primary_hue=gr.themes.colors.emerald,
            neutral_hue=gr.themes.colors.neutral,
        ).set(
            body_background_fill="#0f0f0f",
            body_background_fill_dark="#0f0f0f",
            block_background_fill="#1a1a1a",
            block_background_fill_dark="#1a1a1a",
            border_color_primary="rgba(255,255,255,0.06)",
            border_color_primary_dark="rgba(255,255,255,0.06)",
        ),
    )


if __name__ == "__main__":
    main()
