"""
Gradio Web UI for Regulation RAG System.

Provides a user-friendly web interface for:
- Searching regulations
- Asking questions with LLM-generated answers
- Viewing sync status

Usage:
    uv run python -m src.rag.interface.gradio_app
"""

import argparse
import os
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

from ..infrastructure.chroma_store import ChromaVectorStore
from ..infrastructure.json_loader import JSONDocumentLoader
from ...main import run_pipeline
from ..infrastructure.llm_adapter import LLMClientAdapter
from ..infrastructure.llm_client import MockLLMClient
from ..infrastructure.hybrid_search import QueryAnalyzer, Audience
from ..application.sync_usecase import SyncUseCase
from ..application.search_usecase import QueryRewriteInfo, SearchUseCase
from ..application.full_view_usecase import FullViewUseCase, TableMatch
from ..domain.value_objects import SearchFilter
from ..domain.entities import RegulationStatus
from .chat_logic import (
    attachment_label_variants,
    expand_followup_query,
    format_clarification,
    parse_attachment_request,
    resolve_audience_choice,
    resolve_regulation_choice,
)
from .formatters import (
    normalize_relevance_scores,
    filter_by_relevance,
    get_relevance_label_combined,
    get_confidence_info,
    clean_path_segments,
    render_full_view_nodes,
    normalize_markdown_table,
    normalize_markdown_emphasis,
    strip_path_prefix,
)


# Default paths
DEFAULT_DB_PATH = "data/chroma_db"
DEFAULT_JSON_PATH = "data/output/ê·œì •ì§‘-test01.json"
LLM_PROVIDERS = ["ollama", "lmstudio", "mlx", "local", "openai", "gemini", "openrouter"]
DEFAULT_LLM_PROVIDER = os.getenv("LLM_PROVIDER") or "ollama"
if DEFAULT_LLM_PROVIDER not in LLM_PROVIDERS:
    DEFAULT_LLM_PROVIDER = "ollama"
DEFAULT_LLM_MODEL = os.getenv("LLM_MODEL") or ""
DEFAULT_LLM_BASE_URL = os.getenv("LLM_BASE_URL") or ""


def _format_query_rewrite_debug(info: Optional[QueryRewriteInfo]) -> str:
    if not info:
        return ""

    lines = ["### ğŸ”„ ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼"]

    if not info.used:
        lines.append(f"- **ìƒíƒœ**: ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ë¯¸ì ìš©")
        lines.append(f"- **ì›ë³¸ ì¿¼ë¦¬**: `{info.original}`")
        return "\n".join(lines)

    # ë°©ë²• í‘œì‹œ
    if info.method == "llm":
        method_label = "ğŸ¤– LLM ê¸°ë°˜ ë¦¬ë¼ì´íŒ…"
    elif info.method == "rules":
        method_label = "ğŸ“‹ ê·œì¹™ ê¸°ë°˜ í™•ì¥ (ë™ì˜ì–´/ì¸í…íŠ¸)"
    else:
        method_label = "â“ ì•Œìˆ˜ì—†ìŒ"

    # ì¶”ê°€ ìƒíƒœ í‘œì‹œ
    status_tags = []
    if info.from_cache:
        status_tags.append("ğŸ“¦ ìºì‹œ íˆíŠ¸")
    if info.fallback:
        status_tags.append("âš ï¸ LLM ì‹¤íŒ¨â†’í´ë°±")
    status_text = " | ".join(status_tags) if status_tags else ""

    lines.append(f"**ë°©ë²•**: {method_label}")
    if status_text:
        lines.append(f"**ìƒíƒœ**: {status_text}")

    # ì¿¼ë¦¬ ë³€í™˜ ê²°ê³¼
    lines.append("")
    lines.append("#### ì¿¼ë¦¬ ë³€í™˜")
    lines.append(f"- **ì›ë³¸**: `{info.original}`")
    if info.original == info.rewritten:
        lines.append("- **ê²°ê³¼**: (ë³€ê²½ ì—†ìŒ)")
    else:
        lines.append(f"- **ë³€í™˜**: `{info.rewritten}`")

    # ë™ì˜ì–´ ì ìš© ì—¬ë¶€
    lines.append("")
    lines.append("#### ì ìš©ëœ ê¸°ë²•")
    if info.used_synonyms is not None:
        if info.used_synonyms:
            lines.append("- ğŸ“š **ë™ì˜ì–´ ì‚¬ì „**: âœ… ì ìš©ë¨ (ìœ ì‚¬ì–´ë¡œ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥)")
        else:
            lines.append("- ğŸ“š **ë™ì˜ì–´ ì‚¬ì „**: â– ë¯¸ì ìš©")

    # ì¸í…íŠ¸ ì ìš© ì—¬ë¶€
    if info.used_intent is not None:
        if info.used_intent:
            lines.append("- ğŸ¯ **ì˜ë„ ì¸ì‹**: âœ… ë§¤ì¹­ë¨")
            if info.matched_intents:
                intents_str = ", ".join([f"`{i}`" for i in info.matched_intents])
                lines.append(f"  - ë§¤ì¹­ëœ ì˜ë„: {intents_str}")
        else:
            lines.append("- ğŸ¯ **ì˜ë„ ì¸ì‹**: â– ë¯¸ë§¤ì¹­")

    return "\n".join(lines)





def _decide_search_mode_ui(query: str, mode_selection: str) -> str:
    """Wrapper for shared decide_search_mode in Gradio."""
    from .common import decide_search_mode
    
    force_mode = None
    if mode_selection == "ê²€ìƒ‰ (Search)":
        force_mode = "search"
    elif mode_selection == "ì§ˆë¬¸ (Ask)":
        force_mode = "ask"
    elif mode_selection == "ì „ë¬¸ (Full View)":
        force_mode = "full_view"
        
    return decide_search_mode(query, force_mode)


def create_app(
    db_path: str = DEFAULT_DB_PATH,
    use_mock_llm: bool = False,
) -> "gr.Blocks":
    """
    Create Gradio app instance.

    Args:
        db_path: Path to ChromaDB storage.
        use_mock_llm: Use mock LLM for testing without API key.

    Returns:
        Gradio Blocks app.
    """
    if not GRADIO_AVAILABLE:
        raise ImportError("gradio is required. Install with: uv add gradio")

    # Initialize components
    store = ChromaVectorStore(persist_directory=db_path)
    loader = JSONDocumentLoader()

    llm_status = "â„¹ï¸ ì§ˆë¬¸ íƒ­ì—ì„œ LLM ì„¤ì •ì„ ì„ íƒí•˜ì„¸ìš”."
    if use_mock_llm:
        llm_status = "âš ï¸ Mock LLM (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)"

    search_usecase = SearchUseCase(store)  # use_rerankerëŠ” config ê¸°ë³¸ê°’ ì‚¬ìš©
    sync_usecase = SyncUseCase(loader, store)

    data_input_dir = Path("data/input")
    data_output_dir = Path("data/output")
    data_input_dir.mkdir(parents=True, exist_ok=True)
    data_output_dir.mkdir(parents=True, exist_ok=True)

    def _find_latest_json(output_dir: Path) -> Optional[Path]:
        json_files = [
            p for p in output_dir.rglob("*.json")
            if not p.name.endswith("_metadata.json")
        ]
        if not json_files:
            return None
        return max(json_files, key=lambda p: p.stat().st_mtime)

    def _list_json_files(output_dir: Path) -> List[Path]:
        return sorted(
            [
                p for p in output_dir.rglob("*.json")
                if not p.name.endswith("_metadata.json")
            ],
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )

    def _list_hwp_files(input_dir: Path) -> List[Path]:
        return sorted(input_dir.rglob("*.hwp"))

    def _render_status(target_db_path: str) -> str:
        db_path_value = target_db_path or db_path
        try:
            store_local = ChromaVectorStore(persist_directory=db_path_value)
        except Exception as e:
            return f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"

        sync_state_path = Path("data/sync_state.json")
        last_synced = None
        if sync_state_path.exists():
            try:
                import json
                data = json.loads(sync_state_path.read_text(encoding="utf-8"))
                last_synced = data.get("json_file")
            except Exception:
                last_synced = None

        json_files = _list_json_files(data_output_dir)
        hwp_files = _list_hwp_files(data_input_dir)
        json_by_stem = {p.stem: p for p in json_files}

        lines = []
        lines.append("## DB ìƒíƒœ")
        lines.append(f"- DB ê²½ë¡œ: `{db_path_value}`")
        lines.append(f"- ì²­í¬ ìˆ˜: {store_local.count()}")
        lines.append(f"- ê·œì • ìˆ˜: {len(store_local.get_all_rule_codes())}")
        if last_synced:
            lines.append(f"- **ê·œì •ì§‘: `{last_synced}`**")

        lines.append("\n## JSON íŒŒì¼ ëª©ë¡ (`data/output`)")
        if json_files:
            lines.append("| íŒŒì¼ | ìˆ˜ì • ì‹œê° | í¬ê¸° | ë§ˆì§€ë§‰ ë™ê¸°í™” |")
            lines.append("|---|---|---|---|")
            for p in json_files:
                mtime = datetime.fromtimestamp(p.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
                size_kb = f"{p.stat().st_size / 1024:.1f} KB"
                is_synced = "âœ…" if last_synced and p.name == last_synced else ""
                lines.append(f"| `{p.name}` | {mtime} | {size_kb} | {is_synced} |")
        else:
            lines.append("- JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        lines.append("\n## HWP íŒŒì¼ ëª©ë¡ (`data/input`)")
        if hwp_files:
            lines.append("| íŒŒì¼ | ë³€í™˜ ì—¬ë¶€ | ëŒ€ì‘ JSON |")
            lines.append("|---|---|---|")
            for p in hwp_files:
                json_path = json_by_stem.get(p.stem)
                converted = "âœ…" if json_path else "âŒ"
                json_name = f"`{json_path.name}`" if json_path else "-"
                lines.append(f"| `{p.name}` | {converted} | {json_name} |")
        else:
            lines.append("- HWP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        return "\n".join(lines)

    def _json_choices() -> List[str]:
        return [str(p) for p in _list_json_files(data_output_dir)]

    auto_sync_message = ""
    if store.count() == 0:
        latest_json = _find_latest_json(data_output_dir)
        if latest_json:
            try:
                result = sync_usecase.incremental_sync(str(latest_json))
                auto_sync_message = f"ìë™ ë™ê¸°í™”: {latest_json.name} ({result})"
            except Exception as e:
                auto_sync_message = f"ìë™ ë™ê¸°í™” ì‹¤íŒ¨: {e}"

    # Get initial status
    def get_status_text() -> str:
        status = sync_usecase.get_sync_status()
        auto_sync_note = f"\n- {auto_sync_message}" if auto_sync_message else ""
        return f"""**ë™ê¸°í™” ìƒíƒœ**
- ë§ˆì§€ë§‰ ë™ê¸°í™”: {status['last_sync'] or 'ì—†ìŒ'}
- JSON íŒŒì¼: {status['json_file'] or 'ì—†ìŒ'}
- ì¸ë±ì‹±ëœ ê·œì •: {status['store_regulations']}ê°œ
- ì²­í¬ ìˆ˜: {status['store_chunks']}ê°œ
- LLM: {llm_status}{auto_sync_note}
"""

    def _persist_upload(file_path: str) -> Path:
        input_path = Path(file_path)
        data_input_dir = Path("data/input")
        data_input_dir.mkdir(parents=True, exist_ok=True)
        target_path = data_input_dir / input_path.name
        if input_path.resolve() != target_path.resolve():
            shutil.copy2(input_path, target_path)
    # Unified Search Function
    query_analyzer = QueryAnalyzer()
    full_view_usecase = FullViewUseCase(JSONDocumentLoader())

    def _parse_audience(selection: str) -> Optional[Audience]:
        if selection == "êµìˆ˜":
            return Audience.FACULTY
        if selection == "í•™ìƒ":
            return Audience.STUDENT
        if selection == "ì§ì›":
            return Audience.STAFF
        return None

    def _format_table_matches(
        matches: List[TableMatch],
        table_no: Optional[int],
        label: Optional[str],
    ) -> str:
        label_text = label or "ë³„í‘œ"
        lines = []
        for idx, match in enumerate(matches, 1):
            path = clean_path_segments(match.path) if match.path else []
            heading = " > ".join(path) if path else match.title or label_text
            table_label = f"{label_text} {table_no}" if table_no else label_text
            lines.append(f"### [{idx}] {heading} ({table_label})")
            if match.text:
                lines.append(match.text)
            lines.append(normalize_markdown_table(match.markdown).strip())
        return "\n\n".join([line for line in lines if line])

    def _format_toc(toc: List[str]) -> str:
        if not toc:
            return "ëª©ì°¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        return "### ëª©ì°¨\n" + "\n".join([f"- {t}" for t in toc])

    def _build_search_table(results) -> str:
        table_rows = ["| # | ê·œì •ëª… | ì½”ë“œ | ì¡°í•­ | ì ìˆ˜ |", "|---|------|------|------|------|"]
        for i, r in enumerate(results, 1):
            reg_title = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            path_segments = clean_path_segments(r.chunk.parent_path) if r.chunk.parent_path else []
            path = " > ".join(path_segments[-2:]) if path_segments else r.chunk.title
            table_rows.append(f"| {i} | {reg_title} | {r.chunk.rule_code} | {path[:40]} | {r.score:.2f} |")
        return "\n".join(table_rows)

    def _build_sources_markdown(results, show_debug: bool) -> str:
        sources_md = ["### ğŸ“š ì°¸ê³  ê·œì •\n"]
        norm_scores = normalize_relevance_scores(results) if results else {}
        display_sources = filter_by_relevance(results, norm_scores) if results else []

        for i, r in enumerate(display_sources, 1):
            reg_name = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            path = " > ".join(clean_path_segments(r.chunk.parent_path)) if r.chunk.parent_path else r.chunk.title
            norm_score = norm_scores.get(r.chunk.id, 0.0)
            rel_pct = int(norm_score * 100)
            rel_label = get_relevance_label_combined(rel_pct)
            score_info = f" | AI ì‹ ë¢°ë„: {r.score:.3f}" if show_debug else ""
            snippet = strip_path_prefix(r.chunk.text, r.chunk.parent_path or [])

            sources_md.append(f"""#### [{i}] {reg_name}
**ê²½ë¡œ:** {path}

{snippet[:300]}{'...' if len(snippet) > 300 else ''}

*ê·œì •ë²ˆí˜¸: {r.chunk.rule_code} | ê´€ë ¨ë„: {rel_pct}% {rel_label}{score_info}*

---
""")

        return "\n".join(sources_md)

    def _run_ask_once(
        question: str,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
        audience_override: Optional[Audience],
        show_debug: bool,
    ) -> Tuple[str, str, str, str, str]:
        db_path_value = target_db_path or db_path
        store_for_ask = ChromaVectorStore(persist_directory=db_path_value)
        if store_for_ask.count() == 0:
            return "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation-rag sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.", "", "", ""

        if use_mock_llm:
            llm_client = MockLLMClient()
        else:
            llm_client = LLMClientAdapter(
                provider=llm_provider,
                model=llm_model or None,
                base_url=llm_base_url or None,
            )

        search_with_llm = SearchUseCase(store_for_ask, llm_client)
        filter = None
        if not include_abolished:
            filter = SearchFilter(status=RegulationStatus.ACTIVE)

        answer = search_with_llm.ask(
            question,
            filter=filter,
            top_k=top_k,
            include_abolished=include_abolished,
            audience_override=audience_override,
        )

        answer_text = normalize_markdown_emphasis(answer.text)
        sources_text = _build_sources_markdown(answer.sources, show_debug)
        debug_text = ""
        if show_debug:
            debug_text = _format_query_rewrite_debug(
                search_with_llm.get_last_query_rewrite()
            )
        rule_code = answer.sources[0].chunk.rule_code if answer.sources else ""
        top_regulation_title = ""
        if answer.sources:
            top_chunk = answer.sources[0].chunk
            if top_chunk.parent_path:
                top_regulation_title = top_chunk.parent_path[0]
            else:
                top_regulation_title = top_chunk.title
        return answer_text, sources_text, debug_text, rule_code, top_regulation_title

    def unified_search(
        query: str,
        mode_selection: str,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
        target_audience: str,
        show_debug: bool,
    ):
        """Execute unified search/ask based on mode."""
        if not query.strip():
            yield "ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", "", "", ""
            return

        attachment_request = parse_attachment_request(query, None)
        if attachment_request:
            reg_query, table_no, label = attachment_request
            matches = full_view_usecase.find_matches(reg_query)
            if not matches:
                yield "í•´ë‹¹ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "", "", query, ""
                return
            if len(matches) > 1:
                options = "\n".join([f"- {m.title}" for m in matches])
                detail = f"ë‹¤ìŒ ê·œì • ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:\n{options}"
                yield "ê·œì • í›„ë³´ê°€ ì—¬ëŸ¬ ê°œì…ë‹ˆë‹¤.", detail, "", query, ""
                return
            match = matches[0]
            label_variants = attachment_label_variants(label)
            tables = full_view_usecase.find_tables(match.rule_code, table_no, label_variants)
            if not tables:
                label_text = label or "ë³„í‘œ"
                yield f"{label_text}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "", "", query, match.rule_code
                return
            label_text = label or "ë³„í‘œ"
            title_label = f"{match.title} {label_text}"
            if table_no:
                title_label = f"{match.title} {label_text} {table_no}"
            detail = _format_table_matches(tables, table_no, label_text)
            yield title_label, detail, "", query, match.rule_code
            return

        mode = _decide_search_mode_ui(query, mode_selection)
        audience_override = _parse_audience(target_audience)
        if mode in ("search", "ask") and audience_override is None:
            if query_analyzer.is_audience_ambiguous(query):
                msg = "ëŒ€ìƒì´ ëª¨í˜¸í•©ë‹ˆë‹¤. êµìˆ˜/í•™ìƒ/ì§ì› ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
                yield msg, "", "", "", ""
                return

        if mode == "full_view":
            table, detail, debug, q, code = full_view_regulations(query, show_debug)
            yield table, detail, debug, q, code
            return

        if mode == "search":
             # Search (Retrieval)
             # Reuse search_regulations logic but yield it as a generator to match interface
             table, detail, debug, q, code = search_regulations(
                 query, top_k, include_abolished, audience_override, show_debug
             )
             yield table, detail, debug, q, code
        else:
            # Ask (LLM)
            # Delegate to ask_question generator
            for result in ask_question(
                query, top_k, include_abolished, 
                llm_provider, llm_model, llm_base_url, 
                target_db_path, audience_override, show_debug
            ):
                yield result

    # Chat Function (stateful)
    def chat_respond(
        message: str,
        history: List[dict],
        state: dict,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
        target_audience: str,
        use_context: bool,
        show_debug: bool,
    ):
        history = history or []
        state = state or {}
        state.setdefault("audience", None)
        state.setdefault("pending", None)
        state.setdefault("last_query", None)
        state.setdefault("last_mode", None)
        state.setdefault("last_regulation", None)
        state.setdefault("last_rule_code", None)
        details = ""
        debug_text = ""

        history = history or []
        if not message or not message.strip():
            return history, details, debug_text, state
        if history and isinstance(history[0], (list, tuple)):
            normalized = []
            for user_text, assistant_text in history:
                normalized.append({"role": "user", "content": user_text})
                normalized.append({"role": "assistant", "content": assistant_text})
            history = normalized

        history.append({"role": "user", "content": message})

        audience_override = _parse_audience(target_audience)
        explicit_audience = resolve_audience_choice(message)
        if audience_override:
            state["audience"] = target_audience
        elif explicit_audience:
            state["audience"] = explicit_audience

        pending = state.get("pending")
        attachment_query = None
        attachment_no = None
        attachment_label = None
        attachment_requested = False
        if pending:
            if pending["type"] == "audience":
                choice = resolve_audience_choice(message) or state.get("audience")
                if not choice:
                    response = format_clarification("audience", pending["options"])
                    history.append({"role": "assistant", "content": response})
                    return history, details, debug_text, state
                state["audience"] = choice
                state["pending"] = None
                query = pending["query"]
                mode = pending["mode"]
            elif pending["type"] == "regulation":
                choice = resolve_regulation_choice(message, pending["options"])
                if not choice:
                    response = format_clarification("regulation", pending["options"])
                    history.append({"role": "assistant", "content": response})
                    return history, details, debug_text, state
                state["pending"] = None
                query = choice
                mode = "full_view"
            elif pending["type"] == "regulation_table":
                choice = resolve_regulation_choice(message, pending["options"])
                if not choice:
                    response = format_clarification("regulation", pending["options"])
                    history.append({"role": "assistant", "content": response})
                    return history, details, debug_text, state
                state["pending"] = None
                attachment_query = choice
                attachment_no = pending.get("table_no")
                attachment_label = pending.get("label")
                attachment_requested = True
                query = choice
                mode = "attachment"
            else:
                state["pending"] = None
                query = message
                mode = _decide_search_mode_ui(message, "ìë™ (Auto)")
        else:
            context_hint = None
            if use_context:
                context_hint = state.get("last_regulation") or state.get("last_query")
            query = expand_followup_query(message, context_hint)
            mode = _decide_search_mode_ui(query, "ìë™ (Auto)")
            attachment_request = parse_attachment_request(
                query,
                state.get("last_regulation") if use_context else None,
            )
            if attachment_request:
                attachment_query, attachment_no, attachment_label = attachment_request
                attachment_requested = True
                query = attachment_query
                mode = "attachment"

        analyzer = query_analyzer

        if attachment_requested:
            matches = full_view_usecase.find_matches(attachment_query or query)
            if not matches:
                history.append({"role": "assistant", "content": "í•´ë‹¹ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})
                return history, details, debug_text, state
            if len(matches) > 1:
                options = [m.title for m in matches]
                state["pending"] = {
                    "type": "regulation_table",
                    "options": options,
                    "query": query,
                    "table_no": attachment_no,
                    "label": attachment_label,
                }
                history.append({"role": "assistant", "content": format_clarification("regulation", options)})
                return history, details, debug_text, state

            match = matches[0]
            label_variants = attachment_label_variants(attachment_label)
            tables = full_view_usecase.find_tables(match.rule_code, attachment_no, label_variants)
            if not tables:
                label_text = attachment_label or "ë³„í‘œ"
                history.append({"role": "assistant", "content": f"{label_text}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})
                return history, details, debug_text, state
            label_text = attachment_label or "ë³„í‘œ"
            details = _format_table_matches(tables, attachment_no, label_text)
            title_label = f"{match.title} {label_text}"
            if attachment_no:
                title_label = f"{match.title} {label_text} {attachment_no}"
            history.append({"role": "assistant", "content": f"**{title_label}** ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤."})
            state["last_query"] = query
            state["last_mode"] = "attachment"
            state["last_regulation"] = match.title
            state["last_rule_code"] = match.rule_code
            return history, details, debug_text, state

        if mode == "full_view":
            matches = full_view_usecase.find_matches(query)
            if not matches:
                history.append({"role": "assistant", "content": "í•´ë‹¹ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})
                return history, details, debug_text, state
            if len(matches) > 1:
                options = [m.title for m in matches]
                state["pending"] = {"type": "regulation", "options": options, "query": query, "mode": mode}
                history.append({"role": "assistant", "content": format_clarification("regulation", options)})
                return history, details, debug_text, state
            view = full_view_usecase.get_full_view(matches[0].rule_code) or full_view_usecase.get_full_view(matches[0].title)
            if not view:
                history.append({"role": "assistant", "content": "ê·œì • ì „ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."})
                return history, details, debug_text, state

            toc_text = _format_toc(view.toc)
            content_text = render_full_view_nodes(view.content)
            addenda_text = render_full_view_nodes(view.addenda)
            details = toc_text + "\n\n### ë³¸ë¬¸\n\n" + (content_text or "ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            if addenda_text:
                details += "\n\n### ë¶€ì¹™\n\n" + addenda_text
            history.append({"role": "assistant", "content": f"**{view.title}** ì „ë¬¸ì„ í‘œì‹œí•©ë‹ˆë‹¤."})
            state["last_query"] = query
            state["last_mode"] = "full_view"
            state["last_regulation"] = view.title
            state["last_rule_code"] = view.rule_code
            return history, details, debug_text, state

        if state.get("audience") is None and analyzer.is_audience_ambiguous(query):
            options = ["êµìˆ˜", "í•™ìƒ", "ì§ì›"]
            state["pending"] = {"type": "audience", "options": options, "query": query, "mode": mode}
            history.append({"role": "assistant", "content": format_clarification("audience", options)})
            return history, details, debug_text, state

        audience_override = _parse_audience(state.get("audience") or "")

        if mode == "search":
            if store.count() == 0:
                history.append({"role": "assistant", "content": "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation-rag sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."})
                return history, details, debug_text, state
            search_with_hybrid = SearchUseCase(store)
            results = search_with_hybrid.search_unique(
                query,
                top_k=top_k,
                include_abolished=include_abolished,
                audience_override=audience_override,
            )
            if not results:
                history.append({"role": "assistant", "content": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."})
            else:
                history.append({"role": "assistant", "content": _build_search_table(results)})
                top = results[0]
                full_path = " > ".join(clean_path_segments(top.chunk.parent_path)) if top.chunk.parent_path else top.chunk.title
                top_text = strip_path_prefix(top.chunk.text, top.chunk.parent_path or [])
                details = f"""### ğŸ† 1ìœ„ ê²°ê³¼: {top.chunk.rule_code}

**ê·œì •ëª…:** {top.chunk.parent_path[0] if top.chunk.parent_path else top.chunk.title}

**ê²½ë¡œ:** {full_path}

---

{top_text}
"""
                state["last_query"] = query
                state["last_mode"] = "search"
                state["last_regulation"] = top.chunk.parent_path[0] if top.chunk.parent_path else top.chunk.title
                state["last_rule_code"] = top.chunk.rule_code
            if show_debug:
                debug_text = _format_query_rewrite_debug(search_with_hybrid.get_last_query_rewrite())
            return history, details, debug_text, state

        answer_text, sources_text, debug_text, rule_code, regulation_title = _run_ask_once(
            query,
            top_k,
            include_abolished,
            llm_provider,
            llm_model,
            llm_base_url,
            target_db_path,
            audience_override,
            show_debug,
        )
        history.append({"role": "assistant", "content": answer_text})
        details = sources_text
        state["last_query"] = query
        state["last_mode"] = "ask"
        if regulation_title:
            state["last_regulation"] = regulation_title
        if rule_code:
            state["last_rule_code"] = rule_code
        return history, details, debug_text, state


    # Search function
    def search_regulations(
        query: str,
        top_k: int,
        include_abolished: bool,
        audience_override: Optional[Audience],
        show_debug: bool,
    ) -> Tuple[str, str, str]:
        """Execute search and return formatted results."""
        if not query.strip():
            return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", ""

        if store.count() == 0:
            return "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation-rag sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.", "", ""

        # SearchUseCaseê°€ HybridSearcherë¥¼ ìë™ ì´ˆê¸°í™”
        search_with_hybrid = SearchUseCase(store)
        results = search_with_hybrid.search_unique(
            query,
            top_k=top_k,
            include_abolished=include_abolished,
            audience_override=audience_override,
        )

        if not results:
            debug_text = ""
            if show_debug:
                debug_text = _format_query_rewrite_debug(
                    search_with_hybrid.get_last_query_rewrite()
                )
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "", debug_text

        # Format results as markdown table (CLI ìˆ˜ì¤€)
        table_rows = ["| # | ê·œì •ëª… | ì½”ë“œ | ì¡°í•­ | ì ìˆ˜ |", "|---|------|------|------|------|"]
        for i, r in enumerate(results, 1):
            reg_title = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            path_segments = clean_path_segments(r.chunk.parent_path) if r.chunk.parent_path else []
            path = " > ".join(path_segments[-2:]) if path_segments else r.chunk.title
            table_rows.append(f"| {i} | {reg_title} | {r.chunk.rule_code} | {path[:40]} | {r.score:.2f} |")

        table = "\n".join(table_rows)

        # Top result detail (CLI ìˆ˜ì¤€)
        top = results[0]
        full_path = " > ".join(clean_path_segments(top.chunk.parent_path)) if top.chunk.parent_path else top.chunk.title
        detail = f"""### ğŸ† 1ìœ„ ê²°ê³¼: {top.chunk.rule_code}

**ê·œì •ëª…:** {top.chunk.parent_path[0] if top.chunk.parent_path else top.chunk.title}

**ê²½ë¡œ:** {full_path}

---

{top.chunk.text}
"""

        debug_text = ""
        if show_debug:
            debug_text = _format_query_rewrite_debug(
                search_with_hybrid.get_last_query_rewrite()
            )

        # Return (table, detail, debug, query, rule_code)
        top_rule_code = results[0].chunk.rule_code if results else ""
        return table, detail, debug_text, query, top_rule_code

    def full_view_regulations(
        query: str,
        show_debug: bool,
    ) -> Tuple[str, str, str, str, str]:
        """Render regulation full view for 'ì „ë¬¸' requests."""
        matches = full_view_usecase.find_matches(query)
        if not matches:
            return "í•´ë‹¹ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "", "", query, ""

        if len(matches) > 1:
            options = "\n".join([f"- {m.title}" for m in matches])
            detail = f"ë‹¤ìŒ ê·œì • ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:\n{options}"
            return "ê·œì • í›„ë³´ê°€ ì—¬ëŸ¬ ê°œì…ë‹ˆë‹¤.", detail, "", query, ""

        match = matches[0]
        view = full_view_usecase.get_full_view(match.rule_code) or full_view_usecase.get_full_view(match.title)
        if not view:
            return "ê·œì • ì „ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "", "", query, ""

        toc_text = _format_toc(view.toc)
        content_text = render_full_view_nodes(view.content)
        addenda_text = render_full_view_nodes(view.addenda)
        detail = "### ë³¸ë¬¸\n\n" + (content_text or "ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        if addenda_text:
            detail += "\n\n### ë¶€ì¹™\n\n" + addenda_text
        return toc_text, detail, "", query, view.rule_code

    # Ask function (with LLM) - Generator for streaming progress
    def ask_question(
        question: str,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
        audience_override: Optional[Audience],
        show_debug: bool,
    ):
        """Ask question and get LLM answer with progress updates."""
        if not question.strip():
            yield "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", "", "", ""
            return

        # Step 1: Initialize
        yield "â³ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...", "", "", "", ""
        
        db_path_value = target_db_path or db_path
        store_for_ask = ChromaVectorStore(persist_directory=db_path_value)

        if store_for_ask.count() == 0:
            yield "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation-rag sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.", "", "", "", ""
            return

        # Step 2: Initialize LLM
        yield "â³ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...", "", "", "", ""
        
        if use_mock_llm:
            llm_client = MockLLMClient()
        else:
            try:
                llm_client = LLMClientAdapter(
                    provider=llm_provider,
                    model=llm_model or None,
                    base_url=llm_base_url or None,
                )
            except Exception as e:
                yield f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "", "", "", ""
                return

        # Step 3: Search
        yield "ğŸ” ê´€ë ¨ ê·œì • ê²€ìƒ‰ ì¤‘...", "", "", "", ""
        
        search_with_llm = SearchUseCase(store_for_ask, llm_client)

        filter = None
        if not include_abolished:
            filter = SearchFilter(status=RegulationStatus.ACTIVE)

        # Step 4: Generate answer
        yield "ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘... (10-30ì´ˆ ì†Œìš”)", "", "", "", ""
        
        answer = search_with_llm.ask(
            question,
            filter=filter,
            top_k=top_k,
            include_abolished=include_abolished,
            audience_override=audience_override,
        )

        answer_text = normalize_markdown_emphasis(answer.text)

        # Format sources using shared formatters
        sources_list = answer.sources
        norm_scores = normalize_relevance_scores(sources_list) if sources_list else {}
        display_sources = filter_by_relevance(sources_list, norm_scores) if sources_list else []

        sources_md = ["### ğŸ“š ì°¸ê³  ê·œì •\n"]
        
        for i, r in enumerate(display_sources, 1):
            reg_name = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            path = " > ".join(clean_path_segments(r.chunk.parent_path)) if r.chunk.parent_path else r.chunk.title
            norm_score = norm_scores.get(r.chunk.id, 0.0)
            rel_pct = int(norm_score * 100)
            rel_label = get_relevance_label_combined(rel_pct)
            
            # AI ì‹ ë¢°ë„ëŠ” show_debugì¼ ë•Œë§Œ í‘œì‹œ
            score_info = f" | AI ì‹ ë¢°ë„: {r.score:.3f}" if show_debug else ""
            snippet = strip_path_prefix(r.chunk.text, r.chunk.parent_path or [])
            
            sources_md.append(f"""#### [{i}] {reg_name}
**ê²½ë¡œ:** {path}

{snippet[:300]}{'...' if len(snippet) > 300 else ''}

*ê·œì •ë²ˆí˜¸: {r.chunk.rule_code} | ê´€ë ¨ë„: {rel_pct}% {rel_label}{score_info}*

---
""")

        # Confidence description using shared formatter
        conf_icon, conf_label, _ = get_confidence_info(answer.confidence)
        if answer.confidence >= 0.7:
            conf_desc = f"{conf_icon} ë‹µë³€ ì‹ ë¢°ë„ {conf_label}"
        elif answer.confidence >= 0.4:
            conf_desc = f"{conf_icon} ë‹µë³€ ì‹ ë¢°ë„ {conf_label} - ì›ë¬¸ í™•ì¸ ê¶Œì¥"
        else:
            conf_desc = f"{conf_icon} ë‹µë³€ ì‹ ë¢°ë„ {conf_label} - í•™êµ í–‰ì •ì‹¤ ë¬¸ì˜ ê¶Œì¥"

        sources_text = "\n".join(sources_md) + f"\n**{conf_desc}** (ì‹ ë¢°ë„ {answer.confidence:.0%})"

        debug_text = ""
        if show_debug:
            debug_text = _format_query_rewrite_debug(
                search_with_llm.get_last_query_rewrite()
            )

        # Return (answer, sources, debug, query, rule_code)
        rule_code = answer.sources[0].chunk.rule_code if answer.sources else ""
        yield answer_text, sources_text, debug_text, question, rule_code

    def record_web_feedback(query, rule_code, rating, comment):
        """Record feedback from Web UI."""
        if not query or not rule_code:
            return gr.update(value="âš ï¸ í”¼ë“œë°±ì„ ë‚¨ê¸¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", visible=True)
            
        from ..infrastructure.feedback import FeedbackCollector
        collector = FeedbackCollector()
        collector.record_feedback(
            query=query,
            rule_code=rule_code,
            rating=rating,
            comment=comment or None,
            source="web"
        )
        return gr.update(value="âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!", visible=True)

    # Sync function
    def run_sync(json_path: str, full_sync: bool) -> str:
        """Run synchronization."""
        if not json_path.strip():
            return "JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        try:
            if full_sync:
                result = sync_usecase.full_sync(json_path)
            else:
                result = sync_usecase.incremental_sync(json_path)

            if result.has_errors:
                return f"âŒ ì˜¤ë¥˜ ë°œìƒ:\n" + "\n".join(result.errors)

            return f"""âœ… ë™ê¸°í™” ì™„ë£Œ!
- ì¶”ê°€: {result.added}ê°œ
- ìˆ˜ì •: {result.modified}ê°œ
- ì‚­ì œ: {result.removed}ê°œ
- ë³€ê²½ì—†ìŒ: {result.unchanged}ê°œ
- ì´ ì²­í¬: {store.count()}ê°œ
"""
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜: {str(e)}"

    def run_conversion_and_sync(
        hwp_file: str,
        use_llm: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        output_dir: str,
        target_db_path: str,
        full_sync: bool,
    ) -> Tuple[str, str, str]:
        if not hwp_file:
            return "HWP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", "", ""

        output_dir_value = output_dir or "data/output"
        db_path_value = target_db_path or db_path

        try:
            from dotenv import load_dotenv
            load_dotenv()
        except Exception:
            pass

        input_path = _persist_upload(hwp_file)

        args = argparse.Namespace(
            input_path=str(input_path),
            output_dir=output_dir_value,
            use_llm=use_llm,
            provider=llm_provider,
            model=llm_model or None,
            base_url=llm_base_url or None,
            allow_llm_fallback=True,
            force=False,
            cache_dir=".cache",
            verbose=True,
            enhance_rag=True,
        )

        from rich.console import Console
        console = Console(record=True)
        status = run_pipeline(args, console=console)
        log_text = console.export_text() or ""

        if status != 0:
            return log_text or "ë³€í™˜ ì‹¤íŒ¨", "", ""

        json_path = Path(output_dir_value) / f"{input_path.stem}.json"
        if not json_path.exists():
            return f"{log_text}\nJSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}", "", ""

        store_local = ChromaVectorStore(persist_directory=db_path_value)
        loader_local = JSONDocumentLoader()
        sync_local = SyncUseCase(loader_local, store_local)
        if full_sync:
            sync_result = sync_local.full_sync(str(json_path))
        else:
            sync_result = sync_local.incremental_sync(str(json_path))

        sync_lines = [str(sync_result), f"ì´ ì²­í¬ ìˆ˜: {store_local.count()}"]
        if sync_result.has_errors:
            sync_lines.extend(sync_result.errors)

        status_text = "\n".join([log_text, "[SYNC]", *sync_lines]).strip()
        return status_text, str(json_path), db_path_value

    # Build UI
    with gr.Blocks(
        title="ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A",
    ) as app:
        gr.Markdown("# ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A ì‹œìŠ¤í…œ")

        with gr.Tabs():
            # Tab 0: Chat
            with gr.TabItem("ğŸ’¬ ëŒ€í™”í˜•"):
                with gr.Row():
                    with gr.Column(scale=3):
                        chat_bot = gr.Chatbot(label="ëŒ€í™”", height=420)
                        chat_input = gr.Textbox(
                            label="ë©”ì‹œì§€ ì…ë ¥",
                            placeholder="ì§ˆë¬¸ ë˜ëŠ” ê·œì •ëª…ì„ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: êµì›ì¸ì‚¬ê·œì • ì „ë¬¸",
                            lines=2,
                        )
                        with gr.Row():
                            chat_send = gr.Button("ì „ì†¡", variant="primary")
                            chat_clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")
                    with gr.Column(scale=2):
                        chat_top_k = gr.Slider(minimum=1, maximum=20, value=5, step=1, label="ê²°ê³¼ ìˆ˜")
                        chat_abolished = gr.Checkbox(label="íì§€ ê·œì • í¬í•¨", value=False)
                        chat_target = gr.Radio(
                            choices=["ìë™", "êµìˆ˜", "í•™ìƒ", "ì§ì›"],
                            value="ìë™",
                            label="ëŒ€ìƒ ì„ íƒ",
                        )
                        chat_context = gr.Checkbox(label="ë¬¸ë§¥ í™œìš©", value=True)
                        chat_debug = gr.Checkbox(label="ë””ë²„ê·¸ ì¶œë ¥", value=False)
                        with gr.Accordion("âš™ï¸ LLM ì„¤ì • (ì§ˆë¬¸ ëª¨ë“œìš©)", open=False):
                            chat_llm_p = gr.Dropdown(choices=LLM_PROVIDERS, value=DEFAULT_LLM_PROVIDER, label="Provider")
                            chat_llm_m = gr.Textbox(value=DEFAULT_LLM_MODEL, label="Model")
                            chat_llm_b = gr.Textbox(value=DEFAULT_LLM_BASE_URL, label="Base URL")
                        chat_detail = gr.Markdown(label="ìƒì„¸ / ê·¼ê±°")
                        chat_debug_out = gr.Markdown(label="ë””ë²„ê·¸")

                chat_state = gr.State(
                    {
                        "audience": None,
                        "pending": None,
                        "last_query": None,
                        "last_mode": None,
                        "last_regulation": None,
                        "last_rule_code": None,
                    }
                )

                chat_send.click(
                    fn=chat_respond,
                    inputs=[
                        chat_input,
                        chat_bot,
                        chat_state,
                        chat_top_k,
                        chat_abolished,
                        chat_llm_p,
                        chat_llm_m,
                        chat_llm_b,
                        gr.State(db_path),
                        chat_target,
                        chat_context,
                        chat_debug,
                    ],
                    outputs=[chat_bot, chat_detail, chat_debug_out, chat_state],
                )
                chat_input.submit(
                    fn=chat_respond,
                    inputs=[
                        chat_input,
                        chat_bot,
                        chat_state,
                        chat_top_k,
                        chat_abolished,
                        chat_llm_p,
                        chat_llm_m,
                        chat_llm_b,
                        gr.State(db_path),
                        chat_target,
                        chat_context,
                        chat_debug,
                    ],
                    outputs=[chat_bot, chat_detail, chat_debug_out, chat_state],
                )
                chat_clear.click(
                    fn=lambda: (
                        [],
                        "",
                        "",
                        {
                            "audience": None,
                            "pending": None,
                            "last_query": None,
                            "last_mode": None,
                            "last_regulation": None,
                            "last_rule_code": None,
                        },
                    ),
                    inputs=[],
                    outputs=[chat_bot, chat_detail, chat_debug_out, chat_state],
                )

            # Tab 1: Unified Search
            with gr.TabItem("ğŸ” í†µí•© ê²€ìƒ‰"):
                with gr.Row():
                    with gr.Column(scale=4):
                        uni_query = gr.Textbox(
                            label="ê²€ìƒ‰ì–´ ë˜ëŠ” ì§ˆë¬¸",
                            placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ìê²©ì€? (ì§ˆë¬¸) / ì—°êµ¬ë…„ ê·œì • (ê²€ìƒ‰)",
                            lines=2,
                        )
                        with gr.Row():
                            uni_mode = gr.Radio(
                                choices=["ìë™ (Auto)", "ê²€ìƒ‰ (Search)", "ì§ˆë¬¸ (Ask)", "ì „ë¬¸ (Full View)"],
                                value="ìë™ (Auto)",
                                label="ê²€ìƒ‰ ëª¨ë“œ",
                                scale=2,
                            )
                            uni_btn = gr.Button("ğŸ” ì‹¤í–‰", variant="primary", scale=1)

                    with gr.Column(scale=1):
                        uni_top_k = gr.Slider(minimum=1, maximum=20, value=5, step=1, label="ê²°ê³¼ ìˆ˜")
                        uni_abolished = gr.Checkbox(label="íì§€ ê·œì • í¬í•¨", value=False)
                        uni_debug = gr.Checkbox(label="ë””ë²„ê·¸ ì¶œë ¥", value=False)
                        uni_target = gr.Radio(
                            choices=["ìë™", "êµìˆ˜", "í•™ìƒ", "ì§ì›"],
                            value="ìë™",
                            label="ëŒ€ìƒ ì„ íƒ",
                        )

                with gr.Accordion("âš™ï¸ LLM ì„¤ì • (ì§ˆë¬¸ ëª¨ë“œìš©)", open=False):
                    with gr.Row():
                        llm_p = gr.Dropdown(choices=LLM_PROVIDERS, value=DEFAULT_LLM_PROVIDER, label="Provider")
                        llm_m = gr.Textbox(value=DEFAULT_LLM_MODEL, label="Model")
                        llm_b = gr.Textbox(value=DEFAULT_LLM_BASE_URL, label="Base URL")

                uni_main = gr.Markdown(label="ê²°ê³¼ / ë‹µë³€")
                uni_detail = gr.Markdown(label="ìƒì„¸ / ê·¼ê±°")

                with gr.Accordion("ğŸ”§ ë””ë²„ê·¸ ì •ë³´", open=False):
                    uni_debug_out = gr.Markdown()

                # Feedback State
                uni_fb_query = gr.State("")
                uni_fb_rule = gr.State("")

                # Feedback Row (Shared)
                with gr.Row(visible=False) as uni_fb_row:
                    with gr.Column(scale=4):
                        uni_fb_comment = gr.Textbox(label="í”¼ë“œë°± ì˜ê²¬ (ì„ íƒ)", placeholder="ê²°ê³¼ì— ëŒ€í•œ ì˜ê²¬ì„ ë‚¨ê²¨ì£¼ì„¸ìš”.")
                    with gr.Column(scale=1):
                        with gr.Row():
                            uni_fb_up = gr.Button("ğŸ‘", size="sm")
                            uni_fb_neu = gr.Button("ğŸ˜", size="sm")
                            uni_fb_down = gr.Button("ğŸ‘", size="sm")
                        uni_fb_msg = gr.Markdown(visible=False)

                # Events
                uni_btn.click(
                    fn=unified_search,
                    inputs=[
                        uni_query, uni_mode, uni_top_k, uni_abolished,
                        llm_p, llm_m, llm_b,
                        gr.State(db_path), uni_target, uni_debug
                    ],
                    outputs=[uni_main, uni_detail, uni_debug_out, uni_fb_query, uni_fb_rule],
                )

                # Feedback Events
                uni_query.change(lambda: gr.update(visible=False), None, uni_fb_row)
                uni_btn.click(lambda: gr.update(visible=True), None, uni_fb_row)

                for btn, rating in [(uni_fb_up, 1), (uni_fb_neu, 0), (uni_fb_down, -1)]:
                    btn.click(
                        fn=lambda q, r, c, rt=rating: record_web_feedback(q, r, rt, c),
                        inputs=[uni_fb_query, uni_fb_rule, uni_fb_comment],
                        outputs=[uni_fb_msg]
                    )

            # Tab 3: Status (Read-only)
            with gr.TabItem("ğŸ“‚ ë°ì´í„° í˜„í™©"):
                gr.Markdown("> DB ê´€ë¦¬(ë™ê¸°í™”, ì´ˆê¸°í™”)ëŠ” CLIì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤: `regulation-rag sync`, `regulation-rag reset`")

                status_db_path = gr.Textbox(
                    value=db_path,
                    label="DB ê²½ë¡œ",
                    interactive=False,
                )
                status_markdown = gr.Markdown(_render_status(db_path))
                refresh_btn = gr.Button("ìƒˆë¡œê³ ì¹¨", variant="secondary")

                def _refresh_status_only(target_db_path: str):
                    return _render_status(target_db_path)

                refresh_btn.click(
                    fn=_refresh_status_only,
                    inputs=[status_db_path],
                    outputs=[status_markdown],
                )

    return app


def main():
    """Launch Gradio app."""
    import argparse

    parser = argparse.ArgumentParser(description="ê·œì •ì§‘ RAG ì›¹ UI")
    parser.add_argument("--port", type=int, default=7860, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--share", action="store_true", help="ê³µê°œ ë§í¬ ìƒì„±")
    parser.add_argument("--mock-llm", action="store_true", help="Mock LLM ì‚¬ìš©")
    parser.add_argument("--db-path", default=DEFAULT_DB_PATH, help="DB ê²½ë¡œ")

    args = parser.parse_args()

    app = create_app(db_path=args.db_path, use_mock_llm=args.mock_llm)
    app.launch(
        theme=gr.themes.Soft(),
        server_port=args.port,
        share=args.share,
        show_error=True,
    )


if __name__ == "__main__":
    main()
