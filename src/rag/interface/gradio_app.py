"""
Gradio Web UI for Regulation RAG System.

Provides a user-friendly web interface for:
- Searching regulations
- Asking questions with LLM-generated answers
- Viewing sync status

Usage:
    uv run python -m src.rag.interface.gradio_app
"""

import argparse
import os
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

from ..infrastructure.chroma_store import ChromaVectorStore
from ..infrastructure.json_loader import JSONDocumentLoader
from ...main import run_pipeline
from ..infrastructure.llm_adapter import LLMClientAdapter
from ..infrastructure.llm_client import MockLLMClient
from ..application.sync_usecase import SyncUseCase
from ..application.search_usecase import QueryRewriteInfo, SearchUseCase
from ..domain.value_objects import SearchFilter
from ..domain.entities import RegulationStatus


# Default paths
DEFAULT_DB_PATH = "data/chroma_db"
DEFAULT_JSON_PATH = "data/output/ê·œì •ì§‘-test01.json"
LLM_PROVIDERS = ["ollama", "lmstudio", "mlx", "local", "openai", "gemini", "openrouter"]
DEFAULT_LLM_PROVIDER = os.getenv("LLM_PROVIDER") or "ollama"
if DEFAULT_LLM_PROVIDER not in LLM_PROVIDERS:
    DEFAULT_LLM_PROVIDER = "ollama"
DEFAULT_LLM_MODEL = os.getenv("LLM_MODEL") or ""
DEFAULT_LLM_BASE_URL = os.getenv("LLM_BASE_URL") or ""


def _format_query_rewrite_debug(info: Optional[QueryRewriteInfo]) -> str:
    if not info:
        return ""

    lines = ["### ğŸ ë””ë²„ê·¸"]

    if not info.used:
        lines.append(f"- ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…: (ì ìš© ì•ˆë¨) '{info.original}'")
        return "\n".join(lines)

    if info.method == "llm":
        method_label = "LLM"
    elif info.method == "rules":
        method_label = "ê·œì¹™"
    else:
        method_label = "ì•Œìˆ˜ì—†ìŒ"

    extras = []
    if info.from_cache:
        extras.append("ìºì‹œ")
    if info.fallback:
        extras.append("LLM ì‹¤íŒ¨ í´ë°±")
    extra_text = f" ({', '.join(extras)})" if extras else ""

    if info.original == info.rewritten:
        lines.append(
            f"- ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…[{method_label}]{extra_text}: (ë³€ê²½ ì—†ìŒ) '{info.original}'"
        )
    else:
        lines.append(
            f"- ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…[{method_label}]{extra_text}: '{info.original}' -> '{info.rewritten}'"
        )

    if info.used_synonyms is not None:
        lines.append(f"- ë™ì˜ì–´ ì‚¬ì „: {'ì‚¬ìš©' if info.used_synonyms else 'ë¯¸ì‚¬ìš©'}")
    if info.used_intent is not None:
        lines.append(f"- ì˜ë„ í‚¤ì›Œë“œ: {'ì‚¬ìš©' if info.used_intent else 'ë¯¸ì‚¬ìš©'}")

    return "\n".join(lines)


def create_app(
    db_path: str = DEFAULT_DB_PATH,
    use_mock_llm: bool = False,
) -> "gr.Blocks":
    """
    Create Gradio app instance.

    Args:
        db_path: Path to ChromaDB storage.
        use_mock_llm: Use mock LLM for testing without API key.

    Returns:
        Gradio Blocks app.
    """
    if not GRADIO_AVAILABLE:
        raise ImportError("gradio is required. Install with: uv add gradio")

    # Initialize components
    store = ChromaVectorStore(persist_directory=db_path)
    loader = JSONDocumentLoader()

    llm_status = "â„¹ï¸ ì§ˆë¬¸ íƒ­ì—ì„œ LLM ì„¤ì •ì„ ì„ íƒí•˜ì„¸ìš”."
    if use_mock_llm:
        llm_status = "âš ï¸ Mock LLM (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)"

    search_usecase = SearchUseCase(store)  # use_rerankerëŠ” config ê¸°ë³¸ê°’ ì‚¬ìš©
    sync_usecase = SyncUseCase(loader, store)

    data_input_dir = Path("data/input")
    data_output_dir = Path("data/output")
    data_input_dir.mkdir(parents=True, exist_ok=True)
    data_output_dir.mkdir(parents=True, exist_ok=True)

    def _find_latest_json(output_dir: Path) -> Optional[Path]:
        json_files = [
            p for p in output_dir.rglob("*.json")
            if not p.name.endswith("_metadata.json")
        ]
        if not json_files:
            return None
        return max(json_files, key=lambda p: p.stat().st_mtime)

    def _list_json_files(output_dir: Path) -> List[Path]:
        return sorted(
            [
                p for p in output_dir.rglob("*.json")
                if not p.name.endswith("_metadata.json")
            ],
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )

    def _list_hwp_files(input_dir: Path) -> List[Path]:
        return sorted(input_dir.rglob("*.hwp"))

    def _render_status(target_db_path: str) -> str:
        db_path_value = target_db_path or db_path
        try:
            store_local = ChromaVectorStore(persist_directory=db_path_value)
        except Exception as e:
            return f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"

        sync_state_path = Path("data/sync_state.json")
        last_synced = None
        if sync_state_path.exists():
            try:
                import json
                data = json.loads(sync_state_path.read_text(encoding="utf-8"))
                last_synced = data.get("json_file")
            except Exception:
                last_synced = None

        json_files = _list_json_files(data_output_dir)
        hwp_files = _list_hwp_files(data_input_dir)
        json_by_stem = {p.stem: p for p in json_files}

        lines = []
        lines.append("## DB ìƒíƒœ")
        lines.append(f"- DB ê²½ë¡œ: `{db_path_value}`")
        lines.append(f"- ì²­í¬ ìˆ˜: {store_local.count()}")
        lines.append(f"- ê·œì • ìˆ˜: {len(store_local.get_all_rule_codes())}")
        if last_synced:
            lines.append(f"- **ê·œì •ì§‘: `{last_synced}`**")

        lines.append("\n## JSON íŒŒì¼ ëª©ë¡ (`data/output`)")
        if json_files:
            lines.append("| íŒŒì¼ | ìˆ˜ì • ì‹œê° | í¬ê¸° | ë§ˆì§€ë§‰ ë™ê¸°í™” |")
            lines.append("|---|---|---|---|")
            for p in json_files:
                mtime = datetime.fromtimestamp(p.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
                size_kb = f"{p.stat().st_size / 1024:.1f} KB"
                is_synced = "âœ…" if last_synced and p.name == last_synced else ""
                lines.append(f"| `{p.name}` | {mtime} | {size_kb} | {is_synced} |")
        else:
            lines.append("- JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        lines.append("\n## HWP íŒŒì¼ ëª©ë¡ (`data/input`)")
        if hwp_files:
            lines.append("| íŒŒì¼ | ë³€í™˜ ì—¬ë¶€ | ëŒ€ì‘ JSON |")
            lines.append("|---|---|---|")
            for p in hwp_files:
                json_path = json_by_stem.get(p.stem)
                converted = "âœ…" if json_path else "âŒ"
                json_name = f"`{json_path.name}`" if json_path else "-"
                lines.append(f"| `{p.name}` | {converted} | {json_name} |")
        else:
            lines.append("- HWP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        return "\n".join(lines)

    def _json_choices() -> List[str]:
        return [str(p) for p in _list_json_files(data_output_dir)]

    auto_sync_message = ""
    if store.count() == 0:
        latest_json = _find_latest_json(data_output_dir)
        if latest_json:
            try:
                result = sync_usecase.incremental_sync(str(latest_json))
                auto_sync_message = f"ìë™ ë™ê¸°í™”: {latest_json.name} ({result})"
            except Exception as e:
                auto_sync_message = f"ìë™ ë™ê¸°í™” ì‹¤íŒ¨: {e}"

    # Get initial status
    def get_status_text() -> str:
        status = sync_usecase.get_sync_status()
        auto_sync_note = f"\n- {auto_sync_message}" if auto_sync_message else ""
        return f"""**ë™ê¸°í™” ìƒíƒœ**
- ë§ˆì§€ë§‰ ë™ê¸°í™”: {status['last_sync'] or 'ì—†ìŒ'}
- JSON íŒŒì¼: {status['json_file'] or 'ì—†ìŒ'}
- ì¸ë±ì‹±ëœ ê·œì •: {status['store_regulations']}ê°œ
- ì²­í¬ ìˆ˜: {status['store_chunks']}ê°œ
- LLM: {llm_status}{auto_sync_note}
"""

    def _persist_upload(file_path: str) -> Path:
        input_path = Path(file_path)
        data_input_dir = Path("data/input")
        data_input_dir.mkdir(parents=True, exist_ok=True)
        target_path = data_input_dir / input_path.name
        if input_path.resolve() != target_path.resolve():
            shutil.copy2(input_path, target_path)
        return target_path

    # Search function
    def search_regulations(
        query: str,
        top_k: int,
        include_abolished: bool,
        show_debug: bool,
    ) -> Tuple[str, str, str]:
        """Execute search and return formatted results."""
        if not query.strip():
            return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", ""

        if store.count() == 0:
            return "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation-rag sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.", "", ""

        # SearchUseCaseê°€ HybridSearcherë¥¼ ìë™ ì´ˆê¸°í™”
        search_with_hybrid = SearchUseCase(store)
        results = search_with_hybrid.search_unique(
            query,
            top_k=top_k,
            include_abolished=include_abolished,
        )

        if not results:
            debug_text = ""
            if show_debug:
                debug_text = _format_query_rewrite_debug(
                    search_with_hybrid.get_last_query_rewrite()
                )
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "", debug_text

        # Format results as markdown table (CLI ìˆ˜ì¤€)
        table_rows = ["| # | ê·œì •ëª… | ì½”ë“œ | ì¡°í•­ | ì ìˆ˜ |", "|---|------|------|------|------|"]
        for i, r in enumerate(results, 1):
            reg_title = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            path = " > ".join(r.chunk.parent_path[-2:]) if r.chunk.parent_path else r.chunk.title
            table_rows.append(f"| {i} | {reg_title} | {r.chunk.rule_code} | {path[:40]} | {r.score:.2f} |")

        table = "\n".join(table_rows)

        # Top result detail (CLI ìˆ˜ì¤€)
        top = results[0]
        full_path = ' > '.join(top.chunk.parent_path) if top.chunk.parent_path else top.chunk.title
        detail = f"""### ğŸ† 1ìœ„ ê²°ê³¼: {top.chunk.rule_code}

**ê·œì •ëª…:** {top.chunk.parent_path[0] if top.chunk.parent_path else top.chunk.title}

**ê²½ë¡œ:** {full_path}

---

{top.chunk.text}
"""

        debug_text = ""
        if show_debug:
            debug_text = _format_query_rewrite_debug(
                search_with_hybrid.get_last_query_rewrite()
            )

        return table, detail, debug_text

    # Ask function (with LLM) - Generator for streaming progress
    def ask_question(
        question: str,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
        show_debug: bool,
    ):
        """Ask question and get LLM answer with progress updates."""
        if not question.strip():
            yield "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", ""
            return

        # Step 1: Initialize
        yield "â³ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...", "", ""
        
        db_path_value = target_db_path or db_path
        store_for_ask = ChromaVectorStore(persist_directory=db_path_value)

        if store_for_ask.count() == 0:
            yield "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CLIì—ì„œ 'regulation-rag sync'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.", "", ""
            return

        # Step 2: Initialize LLM
        yield "â³ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...", "", ""
        
        if use_mock_llm:
            llm_client = MockLLMClient()
        else:
            try:
                llm_client = LLMClientAdapter(
                    provider=llm_provider,
                    model=llm_model or None,
                    base_url=llm_base_url or None,
                )
            except Exception as e:
                yield f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "", ""
                return

        # Step 3: Search
        yield "ğŸ” ê´€ë ¨ ê·œì • ê²€ìƒ‰ ì¤‘...", "", ""
        
        search_with_llm = SearchUseCase(store_for_ask, llm_client)

        filter = None
        if not include_abolished:
            filter = SearchFilter(status=RegulationStatus.ACTIVE)

        # Step 4: Generate answer
        yield "ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘... (10-30ì´ˆ ì†Œìš”)", "", ""
        
        answer = search_with_llm.ask(
            question,
            filter=filter,
            top_k=top_k,
            include_abolished=include_abolished,
        )

        # Format sources (CLI ìˆ˜ì¤€)
        # Relative normalization for display
        sources_list = answer.sources
        if sources_list:
            scores = [r.score for r in sources_list]
            max_s, min_s = max(scores), min(scores)
            if max_s == min_s:
                norm_scores = {r.chunk.id: 1.0 for r in sources_list}
            else:
                norm_scores = {r.chunk.id: (r.score - min_s) / (max_s - min_s) for r in sources_list}
        else:
            norm_scores = {}

        sources_md = ["### ğŸ“š ì°¸ê³  ê·œì •\n"]
        for i, r in enumerate(answer.sources, 1):
            reg_name = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            path = " > ".join(r.chunk.parent_path) if r.chunk.parent_path else r.chunk.title
            norm_score = norm_scores.get(r.chunk.id, 0.0)
            rel_pct = int(norm_score * 100)
            
            if rel_pct >= 80:
                rel_label = "ğŸŸ¢ ë§¤ìš° ë†’ìŒ"
            elif rel_pct >= 50:
                rel_label = "ğŸŸ¡ ë†’ìŒ"
            elif rel_pct >= 30:
                rel_label = "ğŸŸ  ë³´í†µ"
            else:
                rel_label = "ğŸ”´ ë‚®ìŒ"
            
            sources_md.append(f"""#### [{i}] {reg_name}
**ê²½ë¡œ:** {path}

{r.chunk.text[:300]}{'...' if len(r.chunk.text) > 300 else ''}

*ê·œì •ë²ˆí˜¸: {r.chunk.rule_code} | ê´€ë ¨ë„: {rel_pct}% {rel_label}*

---
""")

        # Confidence description
        if answer.confidence >= 0.7:
            conf_desc = "ğŸŸ¢ ë‹µë³€ ì‹ ë¢°ë„ ë†’ìŒ"
        elif answer.confidence >= 0.4:
            conf_desc = "ğŸŸ¡ ë‹µë³€ ì‹ ë¢°ë„ ë³´í†µ - ì›ë¬¸ í™•ì¸ ê¶Œì¥"
        else:
            conf_desc = "ğŸ”´ ë‹µë³€ ì‹ ë¢°ë„ ë‚®ìŒ - í•™êµ í–‰ì •ì‹¤ ë¬¸ì˜ ê¶Œì¥"

        sources_text = "\n".join(sources_md) + f"\n**{conf_desc}** (ì‹ ë¢°ë„ {answer.confidence:.0%})"

        debug_text = ""
        if show_debug:
            debug_text = _format_query_rewrite_debug(
                search_with_llm.get_last_query_rewrite()
            )

        yield answer.text, sources_text, debug_text

    # Sync function
    def run_sync(json_path: str, full_sync: bool) -> str:
        """Run synchronization."""
        if not json_path.strip():
            return "JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        try:
            if full_sync:
                result = sync_usecase.full_sync(json_path)
            else:
                result = sync_usecase.incremental_sync(json_path)

            if result.has_errors:
                return f"âŒ ì˜¤ë¥˜ ë°œìƒ:\n" + "\n".join(result.errors)

            return f"""âœ… ë™ê¸°í™” ì™„ë£Œ!
- ì¶”ê°€: {result.added}ê°œ
- ìˆ˜ì •: {result.modified}ê°œ
- ì‚­ì œ: {result.removed}ê°œ
- ë³€ê²½ì—†ìŒ: {result.unchanged}ê°œ
- ì´ ì²­í¬: {store.count()}ê°œ
"""
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜: {str(e)}"

    def run_conversion_and_sync(
        hwp_file: str,
        use_llm: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        output_dir: str,
        target_db_path: str,
        full_sync: bool,
    ) -> Tuple[str, str, str]:
        if not hwp_file:
            return "HWP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", "", ""

        output_dir_value = output_dir or "data/output"
        db_path_value = target_db_path or db_path

        try:
            from dotenv import load_dotenv
            load_dotenv()
        except Exception:
            pass

        input_path = _persist_upload(hwp_file)

        args = argparse.Namespace(
            input_path=str(input_path),
            output_dir=output_dir_value,
            use_llm=use_llm,
            provider=llm_provider,
            model=llm_model or None,
            base_url=llm_base_url or None,
            allow_llm_fallback=True,
            force=False,
            cache_dir=".cache",
            verbose=True,
            enhance_rag=True,
        )

        from rich.console import Console
        console = Console(record=True)
        status = run_pipeline(args, console=console)
        log_text = console.export_text() or ""

        if status != 0:
            return log_text or "ë³€í™˜ ì‹¤íŒ¨", "", ""

        json_path = Path(output_dir_value) / f"{input_path.stem}.json"
        if not json_path.exists():
            return f"{log_text}\nJSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}", "", ""

        store_local = ChromaVectorStore(persist_directory=db_path_value)
        loader_local = JSONDocumentLoader()
        sync_local = SyncUseCase(loader_local, store_local)
        if full_sync:
            sync_result = sync_local.full_sync(str(json_path))
        else:
            sync_result = sync_local.incremental_sync(str(json_path))

        sync_lines = [str(sync_result), f"ì´ ì²­í¬ ìˆ˜: {store_local.count()}"]
        if sync_result.has_errors:
            sync_lines.extend(sync_result.errors)

        status_text = "\n".join([log_text, "[SYNC]", *sync_lines]).strip()
        return status_text, str(json_path), db_path_value

    # Build UI
    with gr.Blocks(
        title="ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A",
        theme=gr.themes.Soft(),
    ) as app:
        gr.Markdown("# ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A ì‹œìŠ¤í…œ")

        with gr.Tabs():
            # Tab 1: Search
            with gr.TabItem("ğŸ” ê²€ìƒ‰"):
                with gr.Row():
                    with gr.Column(scale=3):
                        search_query = gr.Textbox(
                            label="ê²€ìƒ‰ì–´",
                            placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ìê²©",
                            lines=1,
                        )
                    with gr.Column(scale=1):
                        search_top_k = gr.Slider(
                            minimum=1, maximum=20, value=5, step=1,
                            label="ê²°ê³¼ ìˆ˜",
                        )
                        search_abolished = gr.Checkbox(
                            label="íì§€ ê·œì • í¬í•¨",
                            value=False,
                        )
                        search_debug_toggle = gr.Checkbox(
                            label="ë””ë²„ê·¸ ì¶œë ¥",
                            value=False,
                        )

                search_btn = gr.Button("ê²€ìƒ‰", variant="primary")

                search_results = gr.Markdown(label="ê²€ìƒ‰ ê²°ê³¼")
                search_detail = gr.Markdown(label="ìƒì„¸ ë‚´ìš©")
                with gr.Accordion("ë””ë²„ê·¸", open=False):
                    search_debug = gr.Markdown()

                search_btn.click(
                    fn=search_regulations,
                    inputs=[search_query, search_top_k, search_abolished, search_debug_toggle],
                    outputs=[search_results, search_detail, search_debug],
                )

            # Tab 2: Ask (Q&A)
            with gr.TabItem("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°"):
                with gr.Row():
                    with gr.Column(scale=3):
                        ask_question_input = gr.Textbox(
                            label="ì§ˆë¬¸",
                            placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ì‹ ì²­ ìê²©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                            lines=2,
                        )
                    with gr.Column(scale=1):
                        ask_top_k = gr.Slider(
                            minimum=1, maximum=10, value=5, step=1,
                            label="ì°¸ê³  ê·œì • ìˆ˜",
                        )
                        ask_abolished = gr.Checkbox(
                            label="íì§€ ê·œì • í¬í•¨",
                            value=False,
                        )
                        ask_debug_toggle = gr.Checkbox(
                            label="ë””ë²„ê·¸ ì¶œë ¥",
                            value=False,
                        )

                with gr.Accordion("LLM ì„¤ì •", open=False):
                    with gr.Row():
                        llm_provider = gr.Dropdown(
                            choices=LLM_PROVIDERS,
                            value=DEFAULT_LLM_PROVIDER,
                            label="í”„ë¡œë°”ì´ë”",
                        )
                        llm_model = gr.Textbox(
                            value=DEFAULT_LLM_MODEL,
                            label="ëª¨ë¸ (ì„ íƒ)",
                        )
                        llm_base_url = gr.Textbox(
                            value=DEFAULT_LLM_BASE_URL,
                            label="Base URL (ë¡œì»¬ìš©)",
                            placeholder="ì˜ˆ: http://127.0.0.1:11434",
                        )

                ask_btn = gr.Button("ì§ˆë¬¸í•˜ê¸°", variant="primary")

                ask_answer = gr.Markdown(label="ë‹µë³€")
                ask_sources = gr.Markdown(label="ì°¸ê³  ê·œì •")
                with gr.Accordion("ë””ë²„ê·¸", open=False):
                    ask_debug = gr.Markdown()

                ask_btn.click(
                    fn=ask_question,
                    inputs=[
                        ask_question_input,
                        ask_top_k,
                        ask_abolished,
                        llm_provider,
                        llm_model,
                        llm_base_url,
                        gr.State(db_path),
                        ask_debug_toggle,
                    ],
                    outputs=[ask_answer, ask_sources, ask_debug],
                )

            # Tab 3: Status (Read-only)
            with gr.TabItem("ğŸ“‚ ë°ì´í„° í˜„í™©"):
                gr.Markdown("> DB ê´€ë¦¬(ë™ê¸°í™”, ì´ˆê¸°í™”)ëŠ” CLIì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤: `regulation-rag sync`, `regulation-rag reset`")
                
                status_db_path = gr.Textbox(
                    value=db_path,
                    label="DB ê²½ë¡œ",
                    interactive=False,
                )
                status_markdown = gr.Markdown(_render_status(db_path))
                refresh_btn = gr.Button("ìƒˆë¡œê³ ì¹¨", variant="secondary")

                def _refresh_status_only(target_db_path: str):
                    return _render_status(target_db_path)

                refresh_btn.click(
                    fn=_refresh_status_only,
                    inputs=[status_db_path],
                    outputs=[status_markdown],
                )

    return app


def main():
    """Launch Gradio app."""
    import argparse

    parser = argparse.ArgumentParser(description="ê·œì •ì§‘ RAG ì›¹ UI")
    parser.add_argument("--port", type=int, default=7860, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--share", action="store_true", help="ê³µê°œ ë§í¬ ìƒì„±")
    parser.add_argument("--mock-llm", action="store_true", help="Mock LLM ì‚¬ìš©")
    parser.add_argument("--db-path", default=DEFAULT_DB_PATH, help="DB ê²½ë¡œ")

    args = parser.parse_args()

    app = create_app(db_path=args.db_path, use_mock_llm=args.mock_llm)
    app.launch(
        server_port=args.port,
        share=args.share,
        show_error=True,
    )


if __name__ == "__main__":
    main()
