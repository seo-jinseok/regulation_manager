"""
Gradio Web UI for Regulation RAG System.

Provides a user-friendly web interface for:
- Searching regulations
- Asking questions with LLM-generated answers
- Viewing sync status

Usage:
    uv run python -m src.rag.interface.gradio_app
"""

import argparse
import os
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

from ..infrastructure.chroma_store import ChromaVectorStore
from ..infrastructure.json_loader import JSONDocumentLoader
from ...main import run_pipeline
from ..infrastructure.llm_adapter import LLMClientAdapter
from ..infrastructure.llm_client import MockLLMClient
from ..application.sync_usecase import SyncUseCase
from ..application.search_usecase import SearchUseCase
from ..domain.value_objects import SearchFilter
from ..domain.entities import RegulationStatus


# Default paths
DEFAULT_DB_PATH = "data/chroma_db"
DEFAULT_JSON_PATH = "data/output/ê·œì •ì§‘-test01.json"
LLM_PROVIDERS = ["ollama", "lmstudio", "mlx", "local", "openai", "gemini", "openrouter"]
DEFAULT_LLM_PROVIDER = os.getenv("LLM_PROVIDER") or "ollama"
if DEFAULT_LLM_PROVIDER not in LLM_PROVIDERS:
    DEFAULT_LLM_PROVIDER = "ollama"
DEFAULT_LLM_MODEL = os.getenv("LLM_MODEL") or ""
DEFAULT_LLM_BASE_URL = os.getenv("LLM_BASE_URL") or ""


def create_app(
    db_path: str = DEFAULT_DB_PATH,
    use_mock_llm: bool = False,
) -> "gr.Blocks":
    """
    Create Gradio app instance.

    Args:
        db_path: Path to ChromaDB storage.
        use_mock_llm: Use mock LLM for testing without API key.

    Returns:
        Gradio Blocks app.
    """
    if not GRADIO_AVAILABLE:
        raise ImportError("gradio is required. Install with: uv add gradio")

    # Initialize components
    store = ChromaVectorStore(persist_directory=db_path)
    loader = JSONDocumentLoader()

    llm_status = "â„¹ï¸ ì§ˆë¬¸ íƒ­ì—ì„œ LLM ì„¤ì •ì„ ì„ íƒí•˜ì„¸ìš”."
    if use_mock_llm:
        llm_status = "âš ï¸ Mock LLM (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)"

    search_usecase = SearchUseCase(store)
    sync_usecase = SyncUseCase(loader, store)

    data_input_dir = Path("data/input")
    data_output_dir = Path("data/output")
    data_input_dir.mkdir(parents=True, exist_ok=True)
    data_output_dir.mkdir(parents=True, exist_ok=True)

    def _find_latest_json(output_dir: Path) -> Optional[Path]:
        json_files = [
            p for p in output_dir.rglob("*.json")
            if not p.name.endswith("_metadata.json")
        ]
        if not json_files:
            return None
        return max(json_files, key=lambda p: p.stat().st_mtime)

    def _list_json_files(output_dir: Path) -> List[Path]:
        return sorted(
            [
                p for p in output_dir.rglob("*.json")
                if not p.name.endswith("_metadata.json")
            ],
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )

    def _list_hwp_files(input_dir: Path) -> List[Path]:
        return sorted(input_dir.rglob("*.hwp"))

    def _render_status(target_db_path: str) -> str:
        db_path_value = target_db_path or db_path
        try:
            store_local = ChromaVectorStore(persist_directory=db_path_value)
        except Exception as e:
            return f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"

        sync_state_path = Path("data/sync_state.json")
        last_synced = None
        if sync_state_path.exists():
            try:
                import json
                data = json.loads(sync_state_path.read_text(encoding="utf-8"))
                last_synced = data.get("json_file")
            except Exception:
                last_synced = None

        json_files = _list_json_files(data_output_dir)
        hwp_files = _list_hwp_files(data_input_dir)
        json_by_stem = {p.stem: p for p in json_files}

        lines = []
        lines.append("## DB ìƒíƒœ")
        lines.append(f"- DB ê²½ë¡œ: `{db_path_value}`")
        lines.append(f"- ì²­í¬ ìˆ˜: {store_local.count()}")
        lines.append(f"- ê·œì • ìˆ˜: {len(store_local.get_all_rule_codes())}")
        if last_synced:
            lines.append(f"- ë§ˆì§€ë§‰ ë™ê¸°í™” JSON: `{last_synced}`")

        lines.append("\n## JSON íŒŒì¼ ëª©ë¡ (`data/output`)")
        if json_files:
            lines.append("| íŒŒì¼ | ìˆ˜ì • ì‹œê° | í¬ê¸° | ë§ˆì§€ë§‰ ë™ê¸°í™” |")
            lines.append("|---|---|---|---|")
            for p in json_files:
                mtime = datetime.fromtimestamp(p.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
                size_kb = f"{p.stat().st_size / 1024:.1f} KB"
                is_synced = "âœ…" if last_synced and p.name == last_synced else ""
                lines.append(f"| `{p.name}` | {mtime} | {size_kb} | {is_synced} |")
        else:
            lines.append("- JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        lines.append("\n## HWP íŒŒì¼ ëª©ë¡ (`data/input`)")
        if hwp_files:
            lines.append("| íŒŒì¼ | ë³€í™˜ ì—¬ë¶€ | ëŒ€ì‘ JSON |")
            lines.append("|---|---|---|")
            for p in hwp_files:
                json_path = json_by_stem.get(p.stem)
                converted = "âœ…" if json_path else "âŒ"
                json_name = f"`{json_path.name}`" if json_path else "-"
                lines.append(f"| `{p.name}` | {converted} | {json_name} |")
        else:
            lines.append("- HWP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        return "\n".join(lines)

    def _json_choices() -> List[str]:
        return [str(p) for p in _list_json_files(data_output_dir)]

    auto_sync_message = ""
    if store.count() == 0:
        latest_json = _find_latest_json(data_output_dir)
        if latest_json:
            try:
                result = sync_usecase.incremental_sync(str(latest_json))
                auto_sync_message = f"ìë™ ë™ê¸°í™”: {latest_json.name} ({result})"
            except Exception as e:
                auto_sync_message = f"ìë™ ë™ê¸°í™” ì‹¤íŒ¨: {e}"

    # Get initial status
    def get_status_text() -> str:
        status = sync_usecase.get_sync_status()
        auto_sync_note = f"\n- {auto_sync_message}" if auto_sync_message else ""
        return f"""**ë™ê¸°í™” ìƒíƒœ**
- ë§ˆì§€ë§‰ ë™ê¸°í™”: {status['last_sync'] or 'ì—†ìŒ'}
- JSON íŒŒì¼: {status['json_file'] or 'ì—†ìŒ'}
- ì¸ë±ì‹±ëœ ê·œì •: {status['store_regulations']}ê°œ
- ì²­í¬ ìˆ˜: {status['store_chunks']}ê°œ
- LLM: {llm_status}{auto_sync_note}
"""

    def _persist_upload(file_path: str) -> Path:
        input_path = Path(file_path)
        data_input_dir = Path("data/input")
        data_input_dir.mkdir(parents=True, exist_ok=True)
        target_path = data_input_dir / input_path.name
        if input_path.resolve() != target_path.resolve():
            shutil.copy2(input_path, target_path)
        return target_path

    # Search function
    def search_regulations(
        query: str,
        top_k: int,
        include_abolished: bool,
    ) -> Tuple[str, str]:
        """Execute search and return formatted results."""
        if not query.strip():
            return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""

        if store.count() == 0:
            return "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ë™ê¸°í™”ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.", ""

        results = search_usecase.search(
            query,
            top_k=top_k,
            include_abolished=include_abolished,
        )

        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", ""

        # Format results as markdown table
        table_rows = ["| # | ê·œì •ëª… | ì¡°í•­ | ì ìˆ˜ |", "|---|------|------|------|"]
        for i, r in enumerate(results, 1):
            path = " > ".join(r.chunk.parent_path[-2:]) if r.chunk.parent_path else r.chunk.title
            reg_title = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.rule_code
            table_rows.append(f"| {i} | {reg_title} | {path[:30]} | {r.score:.2f} |")

        table = "\n".join(table_rows)

        # Top result detail
        top = results[0]
        detail = f"""### 1ìœ„ ê²°ê³¼: {top.chunk.rule_code}
**ê²½ë¡œ:** {' > '.join(top.chunk.parent_path)}

{top.chunk.text[:500]}{'...' if len(top.chunk.text) > 500 else ''}
"""

        return table, detail

    # Ask function (with LLM)
    def ask_question(
        question: str,
        top_k: int,
        include_abolished: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        target_db_path: str,
    ) -> Tuple[str, str]:
        """Ask question and get LLM answer."""
        if not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""

        db_path_value = target_db_path or db_path
        store_for_ask = ChromaVectorStore(persist_directory=db_path_value)

        if use_mock_llm:
            llm_client = MockLLMClient()
        else:
            try:
                llm_client = LLMClientAdapter(
                    provider=llm_provider,
                    model=llm_model or None,
                    base_url=llm_base_url or None,
                )
            except Exception as e:
                return f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ""

        if store_for_ask.count() == 0:
            return "ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", ""

        search_with_llm = SearchUseCase(store_for_ask, llm_client)

        filter = None
        if not include_abolished:
            filter = SearchFilter(status=RegulationStatus.ACTIVE)

        answer = search_with_llm.ask(
            question,
            filter=filter,
            top_k=top_k,
            include_abolished=include_abolished,
        )

        # Format sources
        sources = []
        for i, r in enumerate(answer.sources, 1):
            path = " > ".join(r.chunk.parent_path[-2:]) if r.chunk.parent_path else ""
            sources.append(f"{i}. [{r.chunk.rule_code}] {path}")

        sources_text = "\n".join(sources) if sources else "ì¶œì²˜ ì—†ìŒ"

        return answer.text, f"### ì°¸ê³  ê·œì •\n{sources_text}\n\n*ì‹ ë¢°ë„: {answer.confidence:.0%}*"

    # Sync function
    def run_sync(json_path: str, full_sync: bool) -> str:
        """Run synchronization."""
        if not json_path.strip():
            return "JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        try:
            if full_sync:
                result = sync_usecase.full_sync(json_path)
            else:
                result = sync_usecase.incremental_sync(json_path)

            if result.has_errors:
                return f"âŒ ì˜¤ë¥˜ ë°œìƒ:\n" + "\n".join(result.errors)

            return f"""âœ… ë™ê¸°í™” ì™„ë£Œ!
- ì¶”ê°€: {result.added}ê°œ
- ìˆ˜ì •: {result.modified}ê°œ
- ì‚­ì œ: {result.removed}ê°œ
- ë³€ê²½ì—†ìŒ: {result.unchanged}ê°œ
- ì´ ì²­í¬: {store.count()}ê°œ
"""
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜: {str(e)}"

    def run_conversion_and_sync(
        hwp_file: str,
        use_llm: bool,
        llm_provider: str,
        llm_model: str,
        llm_base_url: str,
        output_dir: str,
        target_db_path: str,
        full_sync: bool,
    ) -> Tuple[str, str, str]:
        if not hwp_file:
            return "HWP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", "", ""

        output_dir_value = output_dir or "data/output"
        db_path_value = target_db_path or db_path

        try:
            from dotenv import load_dotenv
            load_dotenv()
        except Exception:
            pass

        input_path = _persist_upload(hwp_file)

        args = argparse.Namespace(
            input_path=str(input_path),
            output_dir=output_dir_value,
            use_llm=use_llm,
            provider=llm_provider,
            model=llm_model or None,
            base_url=llm_base_url or None,
            allow_llm_fallback=True,
            force=False,
            cache_dir=".cache",
            verbose=True,
            enhance_rag=True,
        )

        from rich.console import Console
        console = Console(record=True)
        status = run_pipeline(args, console=console)
        log_text = console.export_text() or ""

        if status != 0:
            return log_text or "ë³€í™˜ ì‹¤íŒ¨", "", ""

        json_path = Path(output_dir_value) / f"{input_path.stem}.json"
        if not json_path.exists():
            return f"{log_text}\nJSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}", "", ""

        store_local = ChromaVectorStore(persist_directory=db_path_value)
        loader_local = JSONDocumentLoader()
        sync_local = SyncUseCase(loader_local, store_local)
        if full_sync:
            sync_result = sync_local.full_sync(str(json_path))
        else:
            sync_result = sync_local.incremental_sync(str(json_path))

        sync_lines = [str(sync_result), f"ì´ ì²­í¬ ìˆ˜: {store_local.count()}"]
        if sync_result.has_errors:
            sync_lines.extend(sync_result.errors)

        status_text = "\n".join([log_text, "[SYNC]", *sync_lines]).strip()
        return status_text, str(json_path), db_path_value

    # Build UI
    with gr.Blocks(
        title="ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A",
        theme=gr.themes.Soft(),
    ) as app:
        gr.Markdown("# ğŸ“š ëŒ€í•™ ê·œì •ì§‘ Q&A ì‹œìŠ¤í…œ")

        with gr.Tabs():
            # Tab 0: All-in-one
            with gr.TabItem("ğŸ§© ì˜¬ì¸ì›"):
                gr.Markdown("HWP ì—…ë¡œë“œ â†’ JSON ë³€í™˜ â†’ DB ë™ê¸°í™” â†’ ì§ˆë¬¸ê¹Œì§€ í•œ ë²ˆì— ì§„í–‰í•©ë‹ˆë‹¤.")

                hwp_file = gr.File(
                    label="HWP íŒŒì¼ ì—…ë¡œë“œ",
                    file_types=[".hwp"],
                    type="filepath",
                )
                use_llm_preprocess = gr.Checkbox(
                    label="LLM ì „ì²˜ë¦¬ ì‚¬ìš© (ë¬¸ì„œ í’ˆì§ˆ ë‚®ì€ ê²½ìš° ì¶”ì²œ)",
                    value=False,
                )

                with gr.Accordion("LLM ì„¤ì •", open=False):
                    llm_provider_easy = gr.Dropdown(
                        choices=LLM_PROVIDERS,
                        value=DEFAULT_LLM_PROVIDER,
                        label="í”„ë¡œë°”ì´ë”",
                    )
                    llm_model_easy = gr.Textbox(
                        value=DEFAULT_LLM_MODEL,
                        label="ëª¨ë¸ (ì„ íƒ)",
                    )
                    llm_base_url_easy = gr.Textbox(
                        value=DEFAULT_LLM_BASE_URL,
                        label="Base URL (ë¡œì»¬ìš©)",
                        placeholder="ì˜ˆ: http://127.0.0.1:11434",
                    )

                with gr.Accordion("ê³ ê¸‰ ì„¤ì •", open=False):
                    output_dir = gr.Textbox(
                        value="data/output",
                        label="ì¶œë ¥ í´ë”",
                    )
                    db_path_input = gr.Textbox(
                        value=db_path,
                        label="DB ê²½ë¡œ",
                    )
                    full_sync_input = gr.Checkbox(
                        label="ì „ì²´ ë™ê¸°í™”",
                        value=False,
                    )

                convert_btn = gr.Button("ë³€í™˜ + DB ë™ê¸°í™”", variant="primary")
                pipeline_status = gr.Textbox(label="ì§„í–‰ ë¡œê·¸", lines=12)
                output_json_path = gr.Textbox(label="ìƒì„±ëœ JSON ê²½ë¡œ")
                output_db_path = gr.Textbox(label="DB ê²½ë¡œ")

                convert_btn.click(
                    fn=run_conversion_and_sync,
                    inputs=[
                        hwp_file,
                        use_llm_preprocess,
                        llm_provider_easy,
                        llm_model_easy,
                        llm_base_url_easy,
                        output_dir,
                        db_path_input,
                        full_sync_input,
                    ],
                    outputs=[pipeline_status, output_json_path, output_db_path],
                )

                gr.Markdown("---")
                ask_question_input_easy = gr.Textbox(
                    label="ì§ˆë¬¸",
                    placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ì‹ ì²­ ìê²©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    lines=2,
                )
                ask_top_k_easy = gr.Slider(
                    minimum=1, maximum=10, value=5, step=1,
                    label="ì°¸ê³  ê·œì • ìˆ˜",
                )
                ask_abolished_easy = gr.Checkbox(
                    label="íì§€ ê·œì • í¬í•¨",
                    value=False,
                )
                ask_btn_easy = gr.Button("ì§ˆë¬¸í•˜ê¸°", variant="secondary")
                ask_answer_easy = gr.Markdown(label="ë‹µë³€")
                ask_sources_easy = gr.Markdown(label="ì°¸ê³  ê·œì •")

                ask_btn_easy.click(
                    fn=ask_question,
                    inputs=[
                        ask_question_input_easy,
                        ask_top_k_easy,
                        ask_abolished_easy,
                        llm_provider_easy,
                        llm_model_easy,
                        llm_base_url_easy,
                        db_path_input,
                    ],
                    outputs=[ask_answer_easy, ask_sources_easy],
                )

            # Tab 0.5: Status
            with gr.TabItem("ğŸ“‚ ë°ì´í„° í˜„í™©"):
                status_db_path = gr.Textbox(
                    value=db_path,
                    label="DB ê²½ë¡œ",
                )
                status_markdown = gr.Markdown(_render_status(db_path))
                refresh_btn = gr.Button("ìƒˆë¡œê³ ì¹¨", variant="secondary")

                with gr.Row():
                    json_choices = _json_choices()
                    json_select = gr.Dropdown(
                        choices=json_choices,
                        value=json_choices[0] if json_choices else "",
                        label="ë™ê¸°í™”í•  JSON ì„ íƒ",
                    )
                    full_sync_select = gr.Checkbox(
                        label="ì „ì²´ ë™ê¸°í™”",
                        value=False,
                    )
                    sync_btn = gr.Button("ë™ê¸°í™” ì‹¤í–‰", variant="primary")
                sync_result = gr.Markdown()

                def _refresh_status(target_db_path: str):
                    updated_status = _render_status(target_db_path)
                    choices = _json_choices()
                    value = choices[0] if choices else ""
                    return updated_status, gr.update(choices=choices, value=value)

                refresh_btn.click(
                    fn=_refresh_status,
                    inputs=[status_db_path],
                    outputs=[status_markdown, json_select],
                )

                sync_btn.click(
                    fn=run_sync,
                    inputs=[json_select, full_sync_select],
                    outputs=[sync_result],
                )

            # Tab 1: Search
            with gr.TabItem("ğŸ” ê²€ìƒ‰"):
                with gr.Row():
                    with gr.Column(scale=3):
                        search_query = gr.Textbox(
                            label="ê²€ìƒ‰ì–´",
                            placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ìê²©",
                            lines=1,
                        )
                    with gr.Column(scale=1):
                        search_top_k = gr.Slider(
                            minimum=1, maximum=20, value=5, step=1,
                            label="ê²°ê³¼ ìˆ˜",
                        )
                        search_abolished = gr.Checkbox(
                            label="íì§€ ê·œì • í¬í•¨",
                            value=False,
                        )

                search_btn = gr.Button("ê²€ìƒ‰", variant="primary")

                search_results = gr.Markdown(label="ê²€ìƒ‰ ê²°ê³¼")
                search_detail = gr.Markdown(label="ìƒì„¸ ë‚´ìš©")

                search_btn.click(
                    fn=search_regulations,
                    inputs=[search_query, search_top_k, search_abolished],
                    outputs=[search_results, search_detail],
                )

            # Tab 2: Ask (Q&A)
            with gr.TabItem("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°"):
                with gr.Row():
                    with gr.Column(scale=3):
                        ask_question_input = gr.Textbox(
                            label="ì§ˆë¬¸",
                            placeholder="ì˜ˆ: êµì› ì—°êµ¬ë…„ ì‹ ì²­ ìê²©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                            lines=2,
                        )
                    with gr.Column(scale=1):
                        ask_top_k = gr.Slider(
                            minimum=1, maximum=10, value=5, step=1,
                            label="ì°¸ê³  ê·œì • ìˆ˜",
                        )
                        ask_abolished = gr.Checkbox(
                            label="íì§€ ê·œì • í¬í•¨",
                            value=False,
                        )

                with gr.Accordion("LLM ì„¤ì •", open=False):
                    with gr.Row():
                        llm_provider = gr.Dropdown(
                            choices=LLM_PROVIDERS,
                            value=DEFAULT_LLM_PROVIDER,
                            label="í”„ë¡œë°”ì´ë”",
                        )
                        llm_model = gr.Textbox(
                            value=DEFAULT_LLM_MODEL,
                            label="ëª¨ë¸ (ì„ íƒ)",
                        )
                        llm_base_url = gr.Textbox(
                            value=DEFAULT_LLM_BASE_URL,
                            label="Base URL (ë¡œì»¬ìš©)",
                            placeholder="ì˜ˆ: http://127.0.0.1:11434",
                        )

                ask_btn = gr.Button("ì§ˆë¬¸í•˜ê¸°", variant="primary")

                ask_answer = gr.Markdown(label="ë‹µë³€")
                ask_sources = gr.Markdown(label="ì°¸ê³  ê·œì •")

                ask_btn.click(
                    fn=ask_question,
                    inputs=[ask_question_input, ask_top_k, ask_abolished, llm_provider, llm_model, llm_base_url, gr.State(db_path)],
                    outputs=[ask_answer, ask_sources],
                )

            # Tab 3: Sync
            with gr.TabItem("âš™ï¸ ì„¤ì •"):
                gr.Markdown(get_status_text())

                with gr.Row():
                    sync_json_path = gr.Textbox(
                        label="JSON íŒŒì¼ ê²½ë¡œ",
                        value=DEFAULT_JSON_PATH,
                    )
                    sync_full = gr.Checkbox(
                        label="ì „ì²´ ë™ê¸°í™”",
                        value=False,
                    )

                sync_btn = gr.Button("ë™ê¸°í™” ì‹¤í–‰", variant="secondary")
                sync_result = gr.Markdown(label="ê²°ê³¼")

                sync_btn.click(
                    fn=run_sync,
                    inputs=[sync_json_path, sync_full],
                    outputs=[sync_result],
                )

    return app


def main():
    """Launch Gradio app."""
    import argparse

    parser = argparse.ArgumentParser(description="ê·œì •ì§‘ RAG ì›¹ UI")
    parser.add_argument("--port", type=int, default=7860, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--share", action="store_true", help="ê³µê°œ ë§í¬ ìƒì„±")
    parser.add_argument("--mock-llm", action="store_true", help="Mock LLM ì‚¬ìš©")
    parser.add_argument("--db-path", default=DEFAULT_DB_PATH, help="DB ê²½ë¡œ")

    args = parser.parse_args()

    app = create_app(db_path=args.db_path, use_mock_llm=args.mock_llm)
    app.launch(
        server_port=args.port,
        share=args.share,
        show_error=True,
    )


if __name__ == "__main__":
    main()
