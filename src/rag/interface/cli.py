"""
CLI Interface for Regulation RAG System.

Provides command-line tools for:
- Syncing regulations
- Searching regulations
- Asking questions

Usage:
    uv run python -m src.rag.interface.cli sync data/output/ê·œì •ì§‘.json
    uv run python -m src.rag.interface.cli search "êµì› ì—°êµ¬ë…„"
    uv run python -m src.rag.interface.cli ask "êµì› ì—°êµ¬ë…„ ì‹ ì²­ ìê²©ì€?"
"""

import argparse
import os
import sys
from pathlib import Path
from typing import Optional

# Load .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Rich for pretty output (optional)
try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.markdown import Markdown
    RICH_AVAILABLE = True
    console = Console()
except ImportError:
    RICH_AVAILABLE = False
    console = None


def print_info(msg: str) -> None:
    """Print info message."""
    if RICH_AVAILABLE:
        console.print(f"[blue]â„¹[/blue] {msg}")
    else:
        print(f"[INFO] {msg}")


def print_success(msg: str) -> None:
    """Print success message."""
    if RICH_AVAILABLE:
        console.print(f"[green]âœ“[/green] {msg}")
    else:
        print(f"[OK] {msg}")


def print_error(msg: str) -> None:
    """Print error message."""
    if RICH_AVAILABLE:
        console.print(f"[red]âœ—[/red] {msg}")
    else:
        print(f"[ERROR] {msg}")


def print_query_rewrite(search, original_query: str) -> None:
    """Print query rewrite info when available."""
    info = search.get_last_query_rewrite()
    if not info:
        return

    if not info.used:
        print_info(f"ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…: (ì ìš© ì•ˆë¨) '{original_query}'")
        return

    method_label = None
    if info.method == "llm":
        method_label = "LLM"
    elif info.method == "rules":
        method_label = "ê·œì¹™"

    extras = []
    if info.from_cache:
        extras.append("ìºì‹œ")
    if info.fallback:
        extras.append("LLM ì‹¤íŒ¨ í´ë°±")
    extra_text = f" ({', '.join(extras)})" if extras else ""
    prefix = f"ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…[{method_label}]{extra_text}" if method_label else "ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…"

    if info.original == info.rewritten:
        print_info(f"{prefix}: (ë³€ê²½ ì—†ìŒ) '{info.original}'")
    else:
        print_info(f"{prefix}: '{info.original}' -> '{info.rewritten}'")

    if info.used_synonyms is not None:
        print_info(f"ë™ì˜ì–´ ì‚¬ì „: {'ì‚¬ìš©' if info.used_synonyms else 'ë¯¸ì‚¬ìš©'}")
    if info.used_intent is not None:
        print_info(f"ì˜ë„ í‚¤ì›Œë“œ: {'ì‚¬ìš©' if info.used_intent else 'ë¯¸ì‚¬ìš©'}")
    if info.matched_intents:
        print_info(f"ë§¤ì¹­ ì˜ë„: {', '.join(info.matched_intents)}")


def create_parser() -> argparse.ArgumentParser:
    """Create argument parser."""
    providers = ["ollama", "lmstudio", "mlx", "local", "openai", "gemini", "openrouter"]
    default_provider = os.getenv("LLM_PROVIDER") or "ollama"
    if default_provider not in providers:
        default_provider = "ollama"
    default_model = os.getenv("LLM_MODEL") or None
    default_base_url = os.getenv("LLM_BASE_URL") or None
    parser = argparse.ArgumentParser(
        prog="rag",
        description="ê·œì •ì§‘ RAG ì‹œìŠ¤í…œ CLI",
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # sync command
    sync_parser = subparsers.add_parser(
        "sync",
        help="ê·œì • ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™”",
    )
    sync_parser.add_argument(
        "json_path",
        type=str,
        help="ê·œì •ì§‘ JSON íŒŒì¼ ê²½ë¡œ",
    )
    sync_parser.add_argument(
        "--full",
        action="store_true",
        help="ì „ì²´ ì¬ë™ê¸°í™” (ê¸°ë³¸: ì¦ë¶„ ë™ê¸°í™”)",
    )
    sync_parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )

    # search command
    search_parser = subparsers.add_parser(
        "search",
        help="ê·œì • ê²€ìƒ‰",
    )
    search_parser.add_argument(
        "query",
        type=str,
        help="ê²€ìƒ‰ ì¿¼ë¦¬",
    )
    search_parser.add_argument(
        "-n", "--top-k",
        type=int,
        default=5,
        help="ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸: 5)",
    )
    search_parser.add_argument(
        "--include-abolished",
        action="store_true",
        help="íì§€ ê·œì • í¬í•¨",
    )
    search_parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )
    search_parser.add_argument(
        "--no-rerank",
        action="store_true",
        help="BGE Reranker ë¹„í™œì„±í™” (ê¸°ë³¸: reranking ì‚¬ìš©)",
    )
    search_parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="ìƒì„¸ ì •ë³´ ì¶œë ¥ (ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ë“±)",
    )
    search_parser.add_argument(
        "--debug",
        action="store_true",
        help="ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥ (ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ë“±)",
    )

    # ask command
    ask_parser = subparsers.add_parser(
        "ask",
        help="ê·œì • ì§ˆë¬¸ (LLM ë‹µë³€)",
    )
    ask_parser.add_argument(
        "question",
        type=str,
        help="ì§ˆë¬¸",
    )
    ask_parser.add_argument(
        "-n", "--top-k",
        type=int,
        default=5,
        help="ì°¸ê³  ê·œì • ìˆ˜ (ê¸°ë³¸: 5)",
    )
    ask_parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )
    ask_parser.add_argument(
        "--provider",
        type=str,
        default=default_provider,
        choices=providers,
        help="LLM í”„ë¡œë°”ì´ë” (ollama, lmstudio, mlx, local, openai, gemini, openrouter)",
    )
    ask_parser.add_argument(
        "--model",
        type=str,
        default=default_model,
        help="ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸: í”„ë¡œë°”ì´ë”ë³„ ê¸°ë³¸ê°’)",
    )
    ask_parser.add_argument(
        "--base-url",
        type=str,
        default=default_base_url,
        help="ë¡œì»¬ ì„œë²„ URL (ollama, lmstudio, mlx, localìš©)",
    )
    ask_parser.add_argument(
        "--show-sources",
        action="store_true",
        help="ê´€ë ¨ ê·œì • ì „ë¬¸ ì¶œë ¥",
    )
    ask_parser.add_argument(
        "--no-rerank",
        action="store_true",
        help="BGE Reranker ë¹„í™œì„±í™” (ê¸°ë³¸: reranking ì‚¬ìš©)",
    )
    ask_parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="ìƒì„¸ ì •ë³´ ì¶œë ¥ (LLM ì„¤ì •, ì¸ë±ìŠ¤ êµ¬ì¶• í˜„í™© ë“±)",
    )
    ask_parser.add_argument(
        "--debug",
        action="store_true",
        help="ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥ (ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ë“±)",
    )

    # status command
    status_parser = subparsers.add_parser(
        "status",
        help="ë™ê¸°í™” ìƒíƒœ í™•ì¸",
    )
    status_parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )

    # reset command
    reset_parser = subparsers.add_parser(
        "reset",
        help="ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ëª¨ë“  ë°ì´í„° ì‚­ì œ)",
    )
    reset_parser.add_argument(
        "--confirm",
        action="store_true",
        required=True,
        help="ì´ˆê¸°í™” í™•ì¸ (í•„ìˆ˜)",
    )
    reset_parser.add_argument(
        "--db-path",
        type=str,
        default="data/chroma_db",
        help="ChromaDB ì €ì¥ ê²½ë¡œ",
    )

    return parser


def cmd_sync(args) -> int:
    """Execute sync command."""
    from ..infrastructure.json_loader import JSONDocumentLoader
    from ..infrastructure.chroma_store import ChromaVectorStore
    from ..application.sync_usecase import SyncUseCase

    json_path = Path(args.json_path)
    if not json_path.exists():
        print_error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return 1

    print_info(f"ë°ì´í„°ë² ì´ìŠ¤: {args.db_path}")
    print_info(f"JSON íŒŒì¼: {json_path.name}")

    # Initialize components
    loader = JSONDocumentLoader()
    store = ChromaVectorStore(persist_directory=args.db_path)
    sync = SyncUseCase(loader, store)

    # Execute sync
    if args.full:
        print_info("ì „ì²´ ë™ê¸°í™” ì‹¤í–‰ ì¤‘...")
        result = sync.full_sync(str(json_path))
    else:
        print_info("ì¦ë¶„ ë™ê¸°í™” ì‹¤í–‰ ì¤‘...")
        result = sync.incremental_sync(str(json_path))

    # Print results
    if result.has_errors:
        for error in result.errors:
            print_error(error)
        return 1

    print_success(str(result))
    print_info(f"ì´ ì²­í¬ ìˆ˜: {store.count()}")
    return 0


def cmd_search(args) -> int:
    """Execute search command."""
    from ..infrastructure.chroma_store import ChromaVectorStore
    from ..application.search_usecase import SearchUseCase

    store = ChromaVectorStore(persist_directory=args.db_path)

    if store.count() == 0:
        print_error("ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € syncë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return 1

    use_reranker = not args.no_rerank
    if use_reranker:
        print_info("ğŸ¯ BGE Reranker í™œì„±í™” (ë¹„í™œì„±í™”: --no-rerank)")

    # SearchUseCaseê°€ HybridSearcherë¥¼ ìë™ ì´ˆê¸°í™”
    print_info("ğŸ”„ Hybrid Search ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    search = SearchUseCase(store, use_reranker=use_reranker)
    print_info(f"âœ“ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")

    results = search.search_unique(
        args.query,
        top_k=args.top_k,
        include_abolished=args.include_abolished,
    )

    if args.verbose or args.debug:
        print_query_rewrite(search, args.query)

    if not results:
        print_info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    # Print results
    if RICH_AVAILABLE:
        table = Table(title=f"ê²€ìƒ‰ ê²°ê³¼: '{args.query}'")
        table.add_column("#", style="dim", width=3)
        table.add_column("ê·œì •ëª…", style="cyan")
        table.add_column("ì½”ë“œ", style="magenta")
        table.add_column("ì¡°í•­", style="green")
        table.add_column("ì ìˆ˜", justify="right", style="magenta")

        for i, r in enumerate(results, 1):
            path = " > ".join(r.chunk.parent_path[-2:]) if r.chunk.parent_path else ""
            reg_title = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            table.add_row(
                str(i),
                reg_title or r.chunk.rule_code,
                r.chunk.rule_code,
                path or r.chunk.title,
                f"{r.score:.2f}",
            )
        console.print(table)

        # Print first result details
        if results:
            top = results[0]
            console.print(Panel(
                top.chunk.text[:500] + "..." if len(top.chunk.text) > 500 else top.chunk.text,
                title=f"[1ìœ„] {top.chunk.rule_code}",
                border_style="green",
            ))
    else:
        print(f"\nê²€ìƒ‰ ê²°ê³¼: '{args.query}'")
        print("-" * 60)
        for i, r in enumerate(results, 1):
            reg_title = r.chunk.parent_path[0] if r.chunk.parent_path else r.chunk.title
            print(f"{i}. {reg_title} [{r.chunk.rule_code}] (ì ìˆ˜: {r.score:.2f})")
            print(f"   {r.chunk.text[:100]}...")

    return 0


def cmd_ask(args) -> int:
    """Execute ask command with LLM."""
    from ..infrastructure.chroma_store import ChromaVectorStore
    from ..infrastructure.llm_adapter import LLMClientAdapter
    from ..application.search_usecase import SearchUseCase

    # Step 1: Check database
    if RICH_AVAILABLE:
        with console.status("[bold blue]â³ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì¤‘...[/bold blue]"):
            store = ChromaVectorStore(persist_directory=args.db_path)
            chunk_count = store.count()
    else:
        print("[1/4] ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì¤‘...")
        store = ChromaVectorStore(persist_directory=args.db_path)
        chunk_count = store.count()

    if chunk_count == 0:
        print_error("ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € syncë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return 1

    # Step 2: Initialize LLM
    if args.verbose:
        print_info(f"LLM í”„ë¡œë°”ì´ë”: {args.provider}")
        if args.model:
            print_info(f"ëª¨ë¸: {args.model}")
        if args.base_url:
            print_info(f"Base URL: {args.base_url}")
    
    if RICH_AVAILABLE:
        with console.status("[bold blue]â³ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...[/bold blue]"):
            try:
                llm = LLMClientAdapter(
                    provider=args.provider,
                    model=args.model,
                    base_url=args.base_url,
                )
            except Exception as e:
                print_error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                if args.provider in ("ollama", "lmstudio", "local", "mlx"):
                    print_info("ë¡œì»¬ LLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
                else:
                    print_info("API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
                return 1
    else:
        print("[2/4] LLM ì´ˆê¸°í™” ì¤‘...")
        try:
            llm = LLMClientAdapter(
                provider=args.provider,
                model=args.model,
                base_url=args.base_url,
            )
        except Exception as e:
            print_error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return 1

    use_reranker = not args.no_rerank
    if args.verbose:
        if use_reranker:
            print_info("ğŸ¯ BGE Reranker í™œì„±í™”")
        else:
            print_info("BGE Reranker ë¹„í™œì„±í™”")

    # Step 3: Build search index
    if RICH_AVAILABLE:
        with console.status("[bold blue]ğŸ” ê´€ë ¨ ê·œì • ê²€ìƒ‰ ì¤‘...[/bold blue]"):
            search = SearchUseCase(store, llm_client=llm, use_reranker=use_reranker)
    else:
        print("[3/4] ê´€ë ¨ ê·œì • ê²€ìƒ‰ ì¤‘...")
        search = SearchUseCase(store, llm_client=llm, use_reranker=use_reranker)

    if args.verbose:
        print_info(f"ChromaDB ê²½ë¡œ: {args.db_path}")
        print_info(f"Top-K: {args.top_k}")
        print_info(f"ì§ˆë¬¸: {args.question}")

    # Step 4: Generate answer
    if RICH_AVAILABLE:
        with console.status("[bold green]ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘... (10-30ì´ˆ ì†Œìš”)[/bold green]"):
            try:
                answer = search.ask(
                    question=args.question,
                    top_k=args.top_k,
                )
            except Exception as e:
                print_error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
                return 1
    else:
        print("[4/4] AI ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            answer = search.ask(
                question=args.question,
                top_k=args.top_k,
            )
        except Exception as e:
            print_error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return 1

    if args.verbose or args.debug:
        print_query_rewrite(search, args.question)

    # Print answer
    if RICH_AVAILABLE:
        console.print()
        console.print(Panel(
            Markdown(answer.text),
            title="ğŸ¤– LLM ë‹µë³€",
            border_style="green",
        ))

        # Print sources with enhanced visual format
        if answer.sources:
            console.print()
            console.print("[bold cyan]ğŸ“š ì°¸ê³  ê·œì •:[/bold cyan]")
            
            # Relative normalization: min-max scaling within the batch
            # This correctly represents the Reranker's relative ranking
            def normalize_scores_relative(sources):
                if not sources:
                    return {}
                scores = [r.score for r in sources]
                max_s, min_s = max(scores), min(scores)
                if max_s == min_s:
                    return {r.chunk.id: 1.0 for r in sources}
                return {r.chunk.id: (r.score - min_s) / (max_s - min_s) for r in sources}
            
            norm_scores = normalize_scores_relative(answer.sources)
            
            for i, result in enumerate(answer.sources, 1):
                chunk = result.chunk
                # Show regulation name from parent_path[0] if available
                reg_name = chunk.parent_path[0] if chunk.parent_path else chunk.title
                
                # Clean up duplicate path segments (e.g., "ë¶€ì¹™ > ë¶€ ì¹™" -> "ë¶€ì¹™")
                def clean_path_segments(segments: list) -> list:
                    if not segments:
                        return segments
                    cleaned = [segments[0]]
                    for seg in segments[1:]:
                        # Normalize by removing spaces for comparison
                        prev_normalized = cleaned[-1].replace(" ", "").replace("ã€€", "")
                        curr_normalized = seg.replace(" ", "").replace("ã€€", "")
                        # Skip if same as previous (only whitespace differs)
                        if prev_normalized != curr_normalized:
                            cleaned.append(seg)
                    return cleaned
                
                # Show full path with duplicates removed
                cleaned_segments = clean_path_segments(chunk.parent_path) if chunk.parent_path else []
                
                # Extract more precise path from text if available
                # Text format: "ê·œì •ëª… > ì¡°í•­ > í˜¸ > ëª©: ë‚´ìš©"
                import re
                text_path_match = re.match(r'^([^:]+):\s*', chunk.text)
                if text_path_match:
                    text_path = text_path_match.group(1).strip()
                    text_segments = [s.strip() for s in text_path.split('>')]
                    # Use text path if it's more detailed than parent_path
                    if len(text_segments) > len(cleaned_segments):
                        cleaned_segments = clean_path_segments(text_segments)
                
                # Ensure regulation name is at the beginning of the path
                if cleaned_segments and reg_name and cleaned_segments[0] != reg_name:
                    # Check if reg_name is not already in the path (normalized comparison)
                    first_normalized = cleaned_segments[0].replace(" ", "")
                    reg_normalized = reg_name.replace(" ", "")
                    if first_normalized != reg_normalized:
                        cleaned_segments = [reg_name] + cleaned_segments
                
                path = " > ".join(cleaned_segments) if cleaned_segments else chunk.title
                
                # Use relative normalization for display
                norm_score = norm_scores.get(chunk.id, 0.0)
                rel_score = int(norm_score * 100)
                score_bar = "â–ˆ" * (rel_score // 10) + "â–‘" * (10 - rel_score // 10)
                
                # Relevance label for better understanding
                if rel_score >= 80:
                    rel_label = "ğŸŸ¢ ë§¤ìš° ë†’ìŒ"
                elif rel_score >= 50:
                    rel_label = "ğŸŸ¡ ë†’ìŒ"
                elif rel_score >= 30:
                    rel_label = "ğŸŸ  ë³´í†µ"
                else:
                    rel_label = "ğŸ”´ ë‚®ìŒ"
                
                # Remove path prefix from text to avoid duplication
                # Text format: "path: content" -> extract only content
                import re
                display_text = chunk.text
                # Remove leading path pattern (e.g., "ê·œì •ëª… > ì¡°í•­ > í•­ëª©: ")
                display_text = re.sub(r'^[^:]+:\s*', '', display_text)
                # Clean up remaining format (e.g., "1.:" -> "1.")
                display_text = re.sub(r'(\d+)\.\s*:', r'\1.', display_text)
                
                # Format content with visual hierarchy
                content_parts = [
                    f"[bold blue]ğŸ“– {reg_name}[/bold blue]",
                    f"[dim]ğŸ“ {path}[/dim]",
                    "",
                    display_text,
                    "",
                    f"[dim]ğŸ“‹ ê·œì •ë²ˆí˜¸: {chunk.rule_code} | ê´€ë ¨ë„: {rel_score}% {rel_label} | AI ì‹ ë¢°ë„: {result.score:.3f}[/dim]",
                ]
                
                console.print(Panel(
                    "\n".join(content_parts),
                    title=f"[{i}]",
                    border_style="blue",
                ))

        # Print confidence with user-friendly description and explanation
        console.print()
        if answer.confidence >= 0.7:
            conf_desc = "ğŸŸ¢ ë†’ìŒ"
            conf_detail = "ê²€ìƒ‰ëœ ê·œì •ì´ ì§ˆë¬¸ê³¼ ë†’ì€ ê´€ë ¨ì„±ì„ ë³´ì…ë‹ˆë‹¤. ë‹µë³€ì„ ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif answer.confidence >= 0.4:
            conf_desc = "ğŸŸ¡ ë³´í†µ"
            conf_detail = "ê´€ë ¨ ê·œì •ì„ ì°¾ì•˜ì§€ë§Œ, ì¤‘ìš”í•œ ê²°ì •ì€ ìœ„ ê·œì • ì›ë¬¸ì„ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”."
        else:
            conf_desc = "ğŸ”´ ë‚®ìŒ"
            conf_detail = "ê´€ë ¨ ê·œì •ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. í•™êµ í–‰ì •ì‹¤ì´ë‚˜ ê·œì •ì§‘ì„ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”."
        
        console.print(Panel(
            f"[bold]{conf_desc}[/bold] (ì‹ ë¢°ë„ {answer.confidence:.0%})\n\n{conf_detail}",
            title="ğŸ“Š ë‹µë³€ ì‹ ë¢°ë„",
            border_style="dim",
        ))
    else:
        print(f"\n=== LLM ë‹µë³€ ===")
        print(answer.text)
        print(f"\n=== ì°¸ê³  ê·œì • ===")
        for i, result in enumerate(answer.sources, 1):
            print(f"[{i}] {result.chunk.rule_code}: {result.chunk.text[:100]}...")
        if args.show_sources:
            print(f"\n=== ê·œì • ì „ë¬¸ ===")
            for result in answer.sources:
                print(f"\n--- {result.chunk.rule_code} ---")
                print(result.chunk.text)

    return 0


def cmd_status(args) -> int:
    """Execute status command."""
    from ..infrastructure.chroma_store import ChromaVectorStore
    from ..infrastructure.json_loader import JSONDocumentLoader
    from ..application.sync_usecase import SyncUseCase

    store = ChromaVectorStore(persist_directory=args.db_path)
    loader = JSONDocumentLoader()
    sync = SyncUseCase(loader, store)

    status = sync.get_sync_status()

    if RICH_AVAILABLE:
        table = Table(title="ë™ê¸°í™” ìƒíƒœ")
        table.add_column("í•­ëª©", style="cyan")
        table.add_column("ê°’", style="green")

        table.add_row("ë§ˆì§€ë§‰ ë™ê¸°í™”", status["last_sync"] or "ì—†ìŒ")
        table.add_row("ğŸ“š ê·œì •ì§‘", status["json_file"] or "ì—†ìŒ")
        table.add_row("ìƒíƒœ íŒŒì¼ ê·œì • ìˆ˜", str(status["state_regulations"]))
        table.add_row("DB ì²­í¬ ìˆ˜", str(status["store_chunks"]))
        table.add_row("DB ê·œì • ìˆ˜", str(status["store_regulations"]))

        console.print(table)
    else:
        print("ë™ê¸°í™” ìƒíƒœ")
        print("-" * 40)
        for k, v in status.items():
            print(f"  {k}: {v}")

    return 0


def cmd_reset(args) -> int:
    """Execute reset command - delete all data."""
    from ..infrastructure.chroma_store import ChromaVectorStore
    from ..application.sync_usecase import SyncUseCase
    from ..infrastructure.json_loader import JSONDocumentLoader

    if not args.confirm:
        print_error("ì´ˆê¸°í™”ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ --confirm í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return 1

    store = ChromaVectorStore(persist_directory=args.db_path)
    loader = JSONDocumentLoader()
    sync = SyncUseCase(loader, store)

    # Get current count
    chunk_count = store.count()
    
    if chunk_count == 0:
        print_info("ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ë¯¸ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return 0

    print_info(f"ë°ì´í„°ë² ì´ìŠ¤: {args.db_path}")
    print_info(f"ì‚­ì œ ì˜ˆì • ì²­í¬ ìˆ˜: {chunk_count}")

    # Clear vector store
    deleted = store.clear_all()
    
    # Clear sync state
    sync.reset_state()

    print_success(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ! {deleted}ê°œ ì²­í¬ ì‚­ì œë¨")
    return 0


def main(argv: Optional[list] = None) -> int:
    """Main entry point."""
    parser = create_parser()
    args = parser.parse_args(argv)

    commands = {
        "sync": cmd_sync,
        "search": cmd_search,
        "ask": cmd_ask,
        "status": cmd_status,
        "reset": cmd_reset,
    }

    if args.command in commands:
        return commands[args.command](args)

    parser.print_help()
    return 1


if __name__ == "__main__":
    sys.exit(main())
