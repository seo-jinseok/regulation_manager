# RAG 시스템 품질 테스트 및 개선

AI 에이전트가 다양한 사용자 페르소나를 시뮬레이션하여 RAG 시스템의 품질을 테스트하고, 답변 품질을 면밀히 검토하며, 멀티턴 대화까지 평가하는 워크플로우입니다.

## 성공 기준

| 메트릭 | 목표 |
|--------|------|
| 정적 테스트 통과율 | ≥ 85% |
| 동적 쿼리 성공률 | ≥ 80% |
| 멀티턴 대화 성공률 | ≥ 75% |
| 답변 품질 점수 | ≥ 4.0/5.0 |
| 회귀 발생 | 0건 |

---

## Phase 0: 사전 검증

### 0.1 시스템 상태 확인
```bash
uv run regulation status
```

### 0.2 데이터가 없으면 동기화
```bash
uv run regulation sync data/output/규정집.json
```

### 0.3 LLM 연결 확인
```bash
curl http://localhost:11434/api/tags 2>/dev/null || echo "Ollama 미실행"
```

---

## Phase 1: 정적 평가 실행

### 1.1 자동 평가 실행
```bash
uv run python scripts/auto_evaluate.py --run
```

### 1.2 결과 확인
- 통과율 ≥ 85% → Phase 2로 이동
- 통과율 < 85% → Phase 5 (개선 적용)로 이동

---

## Phase 2: 동적 쿼리 테스트 (핵심)

매 실행마다 **다양한 페르소나**와 **새로운 쿼리**를 생성하여 시스템을 테스트합니다.

### 2.1 페르소나 정의

테스트할 때마다 아래 페르소나 중 **3~5개를 무작위 선택**합니다:

| 페르소나 | 특성 | 예상 관심사 |
|----------|------|-------------|
| 🎓 신입생 | 학교 시스템에 익숙하지 않음, 비공식적 표현 | 수강신청, 장학금, 기숙사, 휴학 |
| 📚 재학생 (3학년) | 구체적 정보 필요, 졸업 준비 | 졸업요건, 전과, 복수전공, 교환학생 |
| 🎓 대학원생 | 연구/논문 중심, 전문적 질문 | 논문심사, 연구비, 학위취득, 지도교수 |
| 👨‍🏫 신임 교수 | 제도 파악 필요, 공식적 표현 | 연구년, 승진, 강의부담, 업적평가 |
| 👩‍🏫 정교수 | 세부 규정 확인, 권리 주장 | 안식년, 정년, 명예퇴직, 보직 |
| 👔 신입 직원 | 복무규정 파악, 혜택 문의 | 휴가, 복리후생, 승진, 겸직 |
| 👨‍💼 과장급 직원 | 부서 운영, 예산 관련 | 예산집행, 시설사용, 인사규정 |
| 👪 학부모 | 자녀 관련 정보, 외부 시선 | 등록금, 장학금, 휴학, 학사일정 |
| 🤕 어려운 상황의 학생 | 감정적, 급한 상황 | 제적위기, 학사경고, 성적이의, 고충처리 |
| 😡 불만있는 구성원 | 권리 주장, 신고 의향 | 성희롱, 갑질, 부당대우, 인권침해 |

### 2.2 쿼리 유형 매트릭스

각 페르소나에 대해 아래 유형 중 **2~3개 쿼리**를 생성합니다:

| 유형 | 설명 | 예시 |
|------|------|------|
| **사실 확인** | 구체적 정보 요청 | "졸업학점이 몇 학점이야?" |
| **절차 질문** | 방법/단계 문의 | "휴학 신청은 어떻게 해?" |
| **자격 확인** | 조건/요건 질문 | "장학금 받으려면 학점이 몇 점이어야 해?" |
| **비교 질문** | 옵션 간 차이 | "일반휴학이랑 군휴학 차이가 뭐야?" |
| **모호한 질문** | 구체적이지 않은 표현 | "학교 그만두고 싶어" |
| **감정 표현** | 상황 설명 + 감정 | "교수님이 너무 불공평해" |
| **복합 질문** | 여러 정보 동시 요청 | "휴학하면 장학금 어떻게 되고 복학은 언제 해야 해?" |
| **은어/축약어** | 비공식적 표현 | "수강철", "복전", "조졸" |

### 2.3 쿼리 생성 및 실행

**각 선택된 페르소나에 대해:**

1. **쿼리 생성**: 페르소나 특성과 쿼리 유형을 조합하여 자연스러운 질문 생성
2. **쿼리 실행**:
```bash
uv run regulation search "<생성된_쿼리>" -a -n 5
```

3. **결과 기록**: 쿼리, 검색 결과, LLM 답변을 캡처

### 2.4 예시 시나리오

**페르소나**: 🎓 신입생 김민수 (1학년, IT 익숙하지 않음)

```
[쿼리 1 - 절차 질문]
"수강신청 어떻게 하는 거예요? 처음이라 아무것도 몰라요"

[쿼리 2 - 모호한 질문]  
"돈이 없는데 학교 다닐 수 있나요?"

[쿼리 3 - 감정 표현]
"룸메가 너무 시끄러워서 공부가 안 돼요. 어떻게 해야 하나요?"
```

---

## Phase 3: 답변 품질 심층 검토

각 쿼리-응답 쌍에 대해 아래 기준으로 **철저히 평가**합니다.

### 3.1 답변 품질 평가 매트릭스

| 항목 | 배점 | 평가 기준 |
|------|------|----------|
| **정확성** | 1.0 | 규정 내용과 일치하는가? 오류/왜곡이 없는가? |
| **완전성** | 1.0 | 질문의 모든 측면에 답했는가? 빠진 정보가 있는가? |
| **관련성** | 1.0 | 질문 의도에 맞는 답변인가? 동떨어진 내용이 있는가? |
| **출처 명시** | 1.0 | 규정명/조항을 적절히 인용했는가? |
| **이해 용이성** | 1.0 | 페르소나가 이해하기 쉬운 수준인가? |
| **합계** | 5.0 | |

### 3.2 검토 체크리스트

각 답변에 대해 반드시 확인:

- [ ] **팩트체크**: 답변의 핵심 주장이 실제 규정과 일치하는가?
  ```bash
  # 규정 내용 직접 확인
  uv run regulation search "<규정명> 제X조" -n 3
  ```

- [ ] **누락 검토**: 사용자가 알아야 할 중요 정보가 빠졌는가?
  - 예외 조항, 기한, 필요 서류 등

- [ ] **오해 가능성**: 모호하거나 오해 소지가 있는 표현이 있는가?

- [ ] **톤 적절성**: 페르소나에게 적합한 톤인가? (학생에게 지나치게 형식적이지 않은가?)

- [ ] **할루시네이션**: 규정에 없는 내용을 지어내지 않았는가?

### 3.3 실패 판정 기준

다음 중 하나라도 해당하면 **실패**:

| 실패 유형 | 설명 | 심각도 |
|----------|------|--------|
| `WRONG_FACT` | 규정과 다른 정보 제공 | Critical |
| `HALLUCINATION` | 존재하지 않는 규정/조항 언급 | Critical |
| `IRRELEVANT` | 질문과 무관한 답변 | High |
| `INCOMPLETE` | 핵심 정보 누락 | Medium |
| `NO_SOURCE` | 출처 없이 단정적 답변 | Medium |
| `CONFUSING` | 이해하기 어려운 답변 | Low |

---

## Phase 4: 멀티턴 대화 테스트

단일 쿼리를 넘어 **후속 질문 시나리오**를 테스트합니다.

### 4.1 후속 질문 유형

| 유형 | 설명 | 예시 |
|------|------|------|
| **구체화** | 더 자세한 정보 요청 | "그러면 정확히 몇 학점이야?" |
| **관련 확장** | 연관 주제로 확장 | "휴학하면 장학금은 어떻게 돼?" |
| **예외 확인** | 특수 상황 문의 | "군대 가는 경우도 똑같아?" |
| **절차 심화** | 구체적 절차 문의 | "신청서는 어디서 받아?" |
| **조건 변경** | 다른 조건에서의 적용 | "대학원생도 마찬가지야?" |
| **확인 질문** | 이해 확인 | "그러니까 3월 전에 신청해야 한다는 거지?" |

### 4.2 멀티턴 시나리오 템플릿

각 페르소나에 대해 **1개 이상의 3턴 대화**를 실행:

```
[Turn 1 - 초기 질문]
User: "휴학하고 싶어요"
→ 시스템 응답 확인

[Turn 2 - 후속 질문 (구체화)]
User: "신청 기간은 언제야? 서류는 뭐가 필요해?"
→ 이전 맥락을 유지하며 답변하는지 확인

[Turn 3 - 후속 질문 (예외 확인)]
User: "군대 때문에 휴학하는 것도 같은 방법이야?"
→ 맥락 유지 + 새로운 정보 제공 확인
```

### 4.3 멀티턴 평가 기준

| 항목 | 평가 내용 |
|------|----------|
| **맥락 유지** | 이전 대화 내용을 기억하고 반영하는가? |
| **정보 일관성** | Turn 간에 모순되는 정보가 없는가? |
| **점진적 심화** | 후속 질문에 더 구체적인 정보를 제공하는가? |
| **중복 회피** | 이미 언급한 내용을 불필요하게 반복하지 않는가? |

### 4.4 CLI에서 멀티턴 테스트

```bash
# 인터랙티브 모드로 멀티턴 테스트
uv run regulation

# 첫 질문 입력 후 후속 질문 연속 입력
```

---

## Phase 5: 개선 적용 (실패 발생 시)

### 5.1 실패 분석

```bash
cat data/output/improvement_plan.json | python -m json.tool
```

### 5.2 제안 유형별 처리

| 유형 | 처리 방법 |
|------|----------|
| `intent` | `data/config/intents.json` 패치 |
| `synonym` | `data/config/synonyms.json` 패치 |
| `code_pattern` | `src/rag/infrastructure/query_analyzer.py` 수정 |
| `code_weight` | 가중치 조정 |
| `code_audience` | 대상 감지 로직 개선 |
| `architecture` | Phase 7에서 수동 검토 보고 |

### 5.3 데이터 패치 (intent/synonym)

**인텐트 추가** - `data/config/intents.json`:
```json
{
  "intent_id": "overseas_conference",
  "triggers": ["해외학회", "해외 학회", "국제학회"]
}
```

**동의어 추가** - `data/config/synonyms.json`에 새 항목 추가

### 5.4 코드 개선 (code_*)

| 제안 유형 | 수정 파일 | 수정 대상 |
|-----------|----------|----------|
| `code_pattern` | `src/rag/infrastructure/query_analyzer.py` | `INTENT_PATTERNS` |
| `code_weight` | `src/rag/infrastructure/query_analyzer.py` | `WEIGHT_PRESETS` |
| `code_audience` | `src/rag/infrastructure/query_analyzer.py` | `*_KEYWORDS` 상수 |

**수정 원칙**: 기존 항목 삭제 금지, 새 항목만 추가

### 5.5 패치 검증
```bash
uv run pytest tests/rag/unit/infrastructure/test_query_analyzer.py -v --tb=short
```

---

## Phase 6: 재평가 및 반복 판단

### 6.1 단위 테스트 확인
```bash
uv run pytest tests/rag/ -v --tb=short
```

### 6.2 정적 평가 재실행
```bash
uv run python scripts/auto_evaluate.py --run
```

### 6.3 실패한 동적 쿼리 재테스트

Phase 3에서 실패한 쿼리들을 다시 테스트

### 6.4 반복 판단

**종료 조건** (하나라도 해당 시 Phase 7으로):
1. 모든 목표 달성
2. 개선 한계: 2회 연속 동일 결과
3. 최대 반복: 3회 사이클 완료
4. 구조적 문제: `architecture` 유형만 남음

종료 조건 미해당 시 **Phase 5로 반복**

---

## Phase 7: 완료 보고

### 7.1 세션 요약 생성

1. **시작 상태**
   - 정적 테스트 통과율
   - 시스템 상태 (규정 수, 조항 수)

2. **동적 테스트 결과**
   - 테스트한 페르소나 목록
   - 생성된 쿼리 수
   - 성공/실패 비율

3. **멀티턴 테스트 결과**
   - 테스트한 시나리오 수
   - 맥락 유지 성공률

4. **답변 품질 점수**
   - 항목별 평균 점수
   - 주요 실패 유형 분포

5. **적용된 개선**
   - 인텐트/동의어 추가 내역
   - 코드 수정 내역

6. **최종 상태**
   - 최종 통과율
   - 남은 문제

7. **다음 단계 권장사항**

### 7.2 결과 저장 위치
- 세션 요약: `data/output/test_session_<날짜>.md`
- 실패 쿼리 목록: `data/output/failed_queries_<날짜>.json`

---

## 트러블슈팅

```bash
# LLM 연결 확인
curl http://localhost:11434/api/tags 2>/dev/null || echo "Ollama 미실행"

# 특정 테스트 디버깅
uv run pytest tests/rag/unit/infrastructure/test_query_analyzer.py::test_specific -v -s

# 변경사항 확인/되돌리기
git diff data/config/
git checkout -- data/config/intents.json

# 규정 내용 직접 확인 (팩트체크용)
uv run regulation search "<규정명>" -n 10

# 특정 조항 확인
uv run regulation search "<규정명> 제N조"
```

---

## 체크리스트

### 기본 검증
- [ ] Phase 0: 시스템 상태 확인
- [ ] Phase 1: 정적 평가 완료

### 동적 테스트
- [ ] Phase 2: 페르소나 선택 (3~5개)
- [ ] Phase 2: 쿼리 생성 및 실행
- [ ] Phase 3: 답변 품질 검토 완료 (평균 ≥ 4.0)

### 멀티턴 테스트
- [ ] Phase 4: 멀티턴 시나리오 실행 (페르소나당 1개 이상)
- [ ] Phase 4: 맥락 유지 확인

### 개선 및 완료
- [ ] Phase 5: 개선 적용 (필요시)
- [ ] Phase 6: 재평가 완료
- [ ] Phase 7: 보고서 생성

---

## 부록: 샘플 테스트 세션

### 세션 예시

```
=== RAG 품질 테스트 세션 시작 ===
날짜: 2026-01-12
테스터: AI Agent

[Phase 0] 시스템 상태: OK (318개 규정, 17254개 조항)
[Phase 1] 정적 테스트: 50/50 통과 (100%)

[Phase 2] 동적 테스트
선택된 페르소나: 신입생, 대학원생, 불만있는 구성원

--- 페르소나: 신입생 ---
쿼리 1: "수강철회 어떻게 해요?"
결과: ✅ 성공 (학칙 제XX조 인용, 절차 설명 완전)

쿼리 2: "돈 없어서 학교 못 다닐 것 같아요"
결과: ⚠️ 부분 성공 (장학금 안내 OK, 등록금 분납 누락)

--- 페르소나: 대학원생 ---
쿼리 1: "논문 심사 일정이 어떻게 되나요?"
결과: ✅ 성공

[Phase 3] 품질 점수
- 정확성: 4.5/5.0
- 완전성: 4.0/5.0
- 관련성: 4.8/5.0
- 출처 명시: 4.2/5.0
- 이해 용이성: 4.3/5.0
- 평균: 4.36/5.0 ✅

[Phase 4] 멀티턴 테스트
시나리오 1: 휴학 → 장학금 → 복학
- Turn 1: ✅
- Turn 2: ✅ (맥락 유지)
- Turn 3: ✅ (일관성 유지)

[Phase 7] 최종 결과
- 정적 테스트: 100%
- 동적 쿼리 성공률: 85%
- 멀티턴 성공률: 100%
- 답변 품질: 4.36/5.0
- 회귀: 없음

결론: 모든 목표 달성 ✅
```
