#!/usr/bin/env python
"""
Dynamic Test Query Generator for RAG System Evaluation.

Generates diverse test queries using LLM for each persona type,
ensuring different queries on every run for comprehensive testing.
"""

from __future__ import annotations

import argparse
import json
import random
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


# Persona definitions with query generation guidance
PERSONAS = {
    "student": {
        "label": "í•™ìƒ",
        "description": "í•™ë¶€ìƒ ë˜ëŠ” ëŒ€í•™ì›ìƒ",
        "topics": [
            "íœ´í•™/ë³µí•™", "ì¡¸ì—…/í•™ìœ„", "ì¥í•™ê¸ˆ/ë“±ë¡ê¸ˆ", "ì „ê³¼/í¸ì…", 
            "í•™ì‚¬ê²½ê³ /ì„±ì ", "ìˆ˜ê°•ì‹ ì²­/í•™ì ", "ê¸°ìˆ™ì‚¬/ìƒí™œê´€", "ë™ì•„ë¦¬/í•™ìƒíšŒ"
        ],
        "styles": ["ì§ì ‘ì  ì§ˆë¬¸", "ê°„ì ‘ì  ì˜ë„", "ê°ì • í‘œí˜„", "ë¶ˆë§Œ í‘œí˜„"],
    },
    "faculty": {
        "label": "êµì›",
        "description": "êµìˆ˜, ê°•ì‚¬, ì—°êµ¬ì›",
        "topics": [
            "ì—°êµ¬ë…„/ì•ˆì‹ë…„", "ìŠ¹ì§„/ì¬ì„ìš©", "ê°•ì˜/ì±…ì„ì‹œìˆ˜", "ì—°êµ¬ìœ¤ë¦¬",
            "í•´ì™¸íŒŒê²¬/í•™íšŒ", "íœ´ì§/ë³‘ê°€", "í‡´ì§/ëª…ì˜ˆí‡´ì§", "ê²¸ì§/ê²¸ì„"
        ],
        "styles": ["ì§ì ‘ì  ì§ˆë¬¸", "ê°„ì ‘ì  ì˜ë„", "ì—…ë¬´ ê´€ë ¨", "ì œë„ ë¬¸ì˜"],
    },
    "staff": {
        "label": "ì§ì›",
        "description": "ì¼ë°˜ í–‰ì •ì§ì›",
        "topics": [
            "íœ´ê°€/ì—°ê°€", "í‡´ì§/í‡´ì§ê¸ˆ", "ìœ¡ì•„íœ´ì§/ìœ¡ì•„", "ìŠ¹ì§„/ì „ë³´",
            "ë³µë¬´/ê·¼ë¬´", "ì¸ì‚¬/í‰ê°€", "ìˆ˜ë‹¹/ê¸‰ì—¬"
        ],
        "styles": ["ì§ì ‘ì  ì§ˆë¬¸", "ê°„ì ‘ì  ì˜ë„", "í–‰ì • ì ˆì°¨"],
    },
    "common": {
        "label": "ê³µí†µ",
        "description": "ëª¨ë“  ëŒ€í•™ êµ¬ì„±ì›",
        "topics": [
            "ì„±í¬ë¡±/ì„±í­ë ¥ ì‹ ê³ ", "ì¸ê¶Œì„¼í„°/ê³ ì¶©ì²˜ë¦¬", "ì—°êµ¬ìœ¤ë¦¬ ìœ„ë°˜",
            "ì£¼ì°¨/ì‹œì„¤", "ë„ì„œê´€/í•™ìˆ ì •ë³´", "ì¥ì• í•™ìƒ ì§€ì›", "ê°‘ì§ˆ/ê´´ë¡­í˜"
        ],
        "styles": ["ì§ì ‘ì  ì§ˆë¬¸", "ì‹ ê³ /ë¬¸ì˜", "ì •ë³´ ìš”ì²­"],
    },
}

# Query generation prompt template
QUERY_GENERATION_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í•™ êµ¬ì„±ì›ì˜ ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì—­í• 
{persona_label} ({persona_desc}) ê´€ì ì—ì„œ ëŒ€í•™ ê·œì •ì— ëŒ€í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

## ìƒì„± ì¡°ê±´
1. **ì£¼ì œ**: {topic}
2. **ìŠ¤íƒ€ì¼**: {style}
3. **ë‹¤ì–‘ì„± í‚¤ì›Œë“œ**: {diversity_seed}
4. **ìƒì„± ê°œìˆ˜**: {count}ê°œ

## ìŠ¤íƒ€ì¼ ê°€ì´ë“œ
- ì§ì ‘ì  ì§ˆë¬¸: "~í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
- ê°„ì ‘ì  ì˜ë„: "~í•˜ê¸° ì‹«ì–´", "~í•˜ê³  ì‹¶ì–´"
- ê°ì • í‘œí˜„: "ë„ˆë¬´ í˜ë“¤ì–´", "ì–´ë–¡í•˜ì§€"
- ë¶ˆë§Œ í‘œí˜„: "ì™œ ì´ë ‡ê²Œ ë³µì¡í•´", "ë¶ˆê³µí‰í•´"

## ì¶œë ¥ í˜•ì‹ (JSON ë°°ì—´)
[
  {{"query": "ì§ˆë¬¸ ë‚´ìš©", "category": "ì¹´í…Œê³ ë¦¬", "intent_hint": "ì˜ë„ íŒíŠ¸"}},
  ...
]

ì¤‘ìš”: JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""


def setup_llm():
    """Initialize LLM client."""
    from src.rag.infrastructure.llm_adapter import LLMClientAdapter
    
    try:
        return LLMClientAdapter(provider="ollama")
    except Exception as e:
        print(f"Warning: LLM init failed ({e}). Using fallback templates.")
        return None


def load_existing_queries(dataset_path: Optional[str] = None) -> set:
    """Load existing queries from evaluation dataset to avoid duplicates."""
    path = Path(dataset_path or "data/config/evaluation_dataset.json")
    if not path.exists():
        return set()
    
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        return {tc["query"].lower().strip() for tc in data.get("test_cases", [])}
    except Exception:
        return set()


def generate_diversity_seed() -> str:
    """Generate a random seed phrase for query diversity."""
    adjectives = ["ê¸´ê¸‰í•œ", "ë³µì¡í•œ", "íŠ¹ìˆ˜í•œ", "ì¼ë°˜ì ì¸", "ì˜ˆì™¸ì ì¸", "ì„ì‹œ", "ì •ê·œ"]
    situations = ["ìƒí™©", "ê²½ìš°", "ì¡°ê±´", "ì‚¬ìœ ", "ì‚¬ë¡€"]
    emotions = ["ê±±ì •ë˜ëŠ”", "ê¶ê¸ˆí•œ", "ë‹µë‹µí•œ", "ê¸‰í•œ", "ì¤‘ìš”í•œ"]
    
    return f"{random.choice(adjectives)} {random.choice(situations)}, {random.choice(emotions)} ë§ˆìŒ"


def generate_queries_with_llm(
    llm_client,
    persona: str,
    count: int = 5,
    existing_queries: Optional[set] = None,
) -> List[Dict[str, Any]]:
    """Generate queries using LLM for a specific persona."""
    if persona not in PERSONAS:
        raise ValueError(f"Unknown persona: {persona}")
    
    persona_info = PERSONAS[persona]
    topic = random.choice(persona_info["topics"])
    style = random.choice(persona_info["styles"])
    diversity_seed = generate_diversity_seed()
    
    prompt = QUERY_GENERATION_PROMPT.format(
        persona_label=persona_info["label"],
        persona_desc=persona_info["description"],
        topic=topic,
        style=style,
        diversity_seed=diversity_seed,
        count=count,
    )
    
    try:
        response = llm_client.generate(
            system_prompt="ëŒ€í•™ ê·œì • ì§ˆë¬¸ ìƒì„±ê¸°. JSON ë°°ì—´ë§Œ ì¶œë ¥.",
            user_message=prompt,
            temperature=0.8,  # Higher temperature for diversity
        )
        
        # Extract JSON from response
        import re
        json_match = re.search(r'\[.*\]', response, re.DOTALL)
        if not json_match:
            print(f"  Warning: Could not parse JSON from LLM response")
            return []
        
        queries = json.loads(json_match.group())
        
        # Filter out existing queries
        if existing_queries:
            queries = [
                q for q in queries 
                if q.get("query", "").lower().strip() not in existing_queries
            ]
        
        # Add metadata
        for q in queries:
            q["persona"] = persona
            q["generated_at"] = datetime.now().isoformat()
            q["topic"] = topic
            q["style"] = style
        
        return queries
        
    except Exception as e:
        print(f"  Error generating queries: {e}")
        return []


def generate_fallback_queries(persona: str, count: int = 5) -> List[Dict[str, Any]]:
    """Generate queries using templates when LLM is unavailable."""
    if persona not in PERSONAS:
        return []
    
    persona_info = PERSONAS[persona]
    templates = {
        "student": [
            "{topic} ì–´ë–»ê²Œ í•´ì•¼ í•´?",
            "{topic} ê´€ë ¨ ê·œì • ì•Œë ¤ì¤˜",
            "ë‚˜ {topic} í•˜ê³  ì‹¶ì€ë°",
            "{topic} ë•Œë¬¸ì— ë„ˆë¬´ í˜ë“¤ì–´",
        ],
        "faculty": [
            "{topic} ì‹ ì²­í•˜ë ¤ë©´?",
            "{topic} ìê²© ìš”ê±´ì´ ë­ì•¼?",
            "{topic} ì ˆì°¨ ì•Œë ¤ì¤˜",
            "{topic} í•˜ê³  ì‹¶ì–´",
        ],
        "staff": [
            "{topic} ê·œì • ì•Œë ¤ì¤˜",
            "{topic} ì–´ë–»ê²Œ ì‹ ì²­í•´?",
            "{topic} ì“°ê³  ì‹¶ì–´",
        ],
        "common": [
            "{topic} ì–´ë–»ê²Œ í•´?",
            "{topic} ì–´ë””ì— ë¬¸ì˜í•´ì•¼ í•´?",
            "{topic} ì‹ ê³ í•˜ê³  ì‹¶ì–´",
        ],
    }
    
    queries = []
    persona_templates = templates.get(persona, templates["common"])
    
    for _ in range(count):
        topic = random.choice(persona_info["topics"])
        template = random.choice(persona_templates)
        query_text = template.format(topic=topic)
        
        queries.append({
            "query": query_text,
            "persona": persona,
            "category": topic,
            "generated_at": datetime.now().isoformat(),
            "method": "fallback_template",
        })
    
    return queries


def save_queries(queries: List[Dict[str, Any]], output_path: Optional[str] = None):
    """Save generated queries to JSON file."""
    if not output_path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"data/output/generated_queries_{timestamp}.json"
    
    path = Path(output_path)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    output = {
        "version": "1.0.0",
        "generated_at": datetime.now().isoformat(),
        "total_queries": len(queries),
        "queries": queries,
    }
    
    path.write_text(json.dumps(output, ensure_ascii=False, indent=2), encoding="utf-8")
    return path


def main():
    parser = argparse.ArgumentParser(
        description="ë™ì  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # í•™ìƒ í˜ë¥´ì†Œë‚˜ë¡œ 5ê°œ ì¿¼ë¦¬ ìƒì„±
  uv run python scripts/generate_test_queries.py --persona student --count 5
  
  # ëª¨ë“  í˜ë¥´ì†Œë‚˜ë¡œ ê° 3ê°œì”© ìƒì„±
  uv run python scripts/generate_test_queries.py --all --count 3
  
  # ê²°ê³¼ íŒŒì¼ ì§€ì •
  uv run python scripts/generate_test_queries.py --all -o data/output/my_queries.json
        """,
    )
    parser.add_argument(
        "--persona", "-p",
        choices=list(PERSONAS.keys()),
        help="ì¿¼ë¦¬ë¥¼ ìƒì„±í•  í˜ë¥´ì†Œë‚˜ (student, faculty, staff, common)",
    )
    parser.add_argument(
        "--all", "-a",
        action="store_true",
        help="ëª¨ë“  í˜ë¥´ì†Œë‚˜ì— ëŒ€í•´ ì¿¼ë¦¬ ìƒì„±",
    )
    parser.add_argument(
        "--count", "-n",
        type=int,
        default=5,
        help="í˜ë¥´ì†Œë‚˜ë‹¹ ìƒì„±í•  ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸ê°’: 5)",
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--no-dedup",
        action="store_true",
        help="ê¸°ì¡´ ì¿¼ë¦¬ì™€ì˜ ì¤‘ë³µ ê²€ì‚¬ ë¹„í™œì„±í™”",
    )
    
    args = parser.parse_args()
    
    if not args.persona and not args.all:
        parser.print_help()
        print("\nì˜¤ë¥˜: --persona ë˜ëŠ” --all ì˜µì…˜ì„ ì§€ì •í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # Setup
    print("ğŸš€ ë™ì  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±ê¸° ì‹œì‘")
    llm_client = setup_llm()
    existing_queries = set() if args.no_dedup else load_existing_queries()
    
    if existing_queries:
        print(f"ğŸ“‹ ê¸°ì¡´ ì¿¼ë¦¬ {len(existing_queries)}ê°œ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)")
    
    # Generate queries
    all_queries = []
    personas_to_process = list(PERSONAS.keys()) if args.all else [args.persona]
    
    for persona in personas_to_process:
        print(f"\nğŸ‘¤ {PERSONAS[persona]['label']} í˜ë¥´ì†Œë‚˜ ì¿¼ë¦¬ ìƒì„± ì¤‘...")
        
        queries = []
        if llm_client:
            queries = generate_queries_with_llm(
                llm_client, persona, args.count, existing_queries
            )
        
        # Fallback to templates if LLM failed or returned empty
        if not queries:
            print("   âš ï¸ LLM ìƒì„± ì‹¤íŒ¨, í…œí”Œë¦¿ ì‚¬ìš©")
            queries = generate_fallback_queries(persona, args.count)
        
        print(f"   âœ… {len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ")
        all_queries.extend(queries)
        
        # Update existing queries set to prevent duplicates across personas
        existing_queries.update(q["query"].lower().strip() for q in queries)
    
    # Save results
    if all_queries:
        output_path = save_queries(all_queries, args.output)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        print(f"ğŸ“Š ì´ {len(all_queries)}ê°œ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ")
        
        # Print sample
        print("\nğŸ“ ìƒì„±ëœ ì¿¼ë¦¬ ìƒ˜í”Œ:")
        for q in all_queries[:5]:
            print(f"   - [{q.get('persona', 'unknown')}] {q['query']}")
        if len(all_queries) > 5:
            print(f"   ... ì™¸ {len(all_queries) - 5}ê°œ")
    else:
        print("\nâš ï¸ ìƒì„±ëœ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)


if __name__ == "__main__":
    main()
