#!/usr/bin/env python3
"""
íŒ©íŠ¸ì²´í¬ ìë™í™” ë„êµ¬

LLM ë‹µë³€ì—ì„œ ê·œì •/ì¡°í•­ ì¸ìš©ì„ ì¶”ì¶œí•˜ê³ , ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ì¦í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ íŒŒì´í”„ë¡œ ì „ë‹¬
    echo "ã€Œíœ´í•™ê·œì •ã€ ì œ7ì¡°ì— ë”°ë¥´ë©´..." | uv run python scripts/factcheck.py
    
    # íŒŒì¼ì—ì„œ ì½ê¸°
    uv run python scripts/factcheck.py --file answer.txt
    
    # ì§ì ‘ í…ìŠ¤íŠ¸ ì…ë ¥
    uv run python scripts/factcheck.py --text "ã€Œêµì›ì¸ì‚¬ê·œì •ã€ ì œ36ì¡°ì— ë”°ë¥´ë©´ íœ´ì§ ê¸°ê°„ì€..."
"""

import argparse
import json
import re
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from rag.infrastructure.chroma_store import ChromaVectorStore


def extract_citations(text: str) -> list[dict]:
    """
    ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ ê·œì •/ì¡°í•­ ì¸ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì¶”ì¶œ íŒ¨í„´:
    - ã€Œê·œì •ëª…ã€ ì œNì¡°
    - ã€Œê·œì •ëª…ã€ ì œNì¡° ì œMí•­
    - ã€Œê·œì •ëª…ã€ ì œNì¡°ì˜N
    """
    patterns = [
        # ã€Œê·œì •ëª…ã€ ì œNì¡° ì œMí•­ Ní˜¸
        r"ã€Œ([^ã€]+)ã€\s*ì œ?(\d+)ì¡°(?:ì˜(\d+))?\s*(?:ì œ?(\d+)í•­)?(?:\s*(\d+)í˜¸)?",
        # ê·œì •ëª… ì œNì¡° (ë”°ì˜´í‘œ ì—†ì´)
        r"([ê°€-í£]+(?:ê·œì •|ê·œì¹™|ì„¸ì¹™|ì§€ì¹¨|í•™ì¹™))\s*ì œ?(\d+)ì¡°(?:ì˜(\d+))?\s*(?:ì œ?(\d+)í•­)?",
    ]

    citations = []
    seen = set()

    for pattern in patterns:
        for match in re.finditer(pattern, text):
            groups = match.groups()
            reg_name = groups[0]
            article = groups[1]
            article_sub = groups[2] if len(groups) > 2 else None
            paragraph = groups[3] if len(groups) > 3 else None

            # ì¤‘ë³µ ì œê±°ìš© í‚¤
            key = (reg_name, article, article_sub)
            if key in seen:
                continue
            seen.add(key)

            citation = {
                "regulation": reg_name,
                "article": article,
                "article_sub": article_sub,
                "paragraph": paragraph,
                "original": match.group(0),
            }
            citations.append(citation)

    return citations


def verify_citation(store: ChromaVectorStore, citation: dict) -> dict:
    """
    ì¸ìš©ì´ ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    reg_name = citation["regulation"]
    article = citation["article"]
    article_sub = citation.get("article_sub")

    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    if article_sub:
        query = f"{reg_name} ì œ{article}ì¡°ì˜{article_sub}"
    else:
        query = f"{reg_name} ì œ{article}ì¡°"

    # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
    from rag.domain.value_objects import Query
    results = store.search(Query(text=query), top_k=10)

    # ê²°ê³¼ ë¶„ì„
    found = False
    matched_chunks = []

    # ê·œì •ëª… ì •ê·œí™” (ë„ì–´ì“°ê¸° ë“± ì œê±°)
    reg_name_normalized = reg_name.replace(" ", "").replace("Â·", "")
    article_pattern = f"ì œ{article}ì¡°"

    for result in results:
        chunk = result.chunk
        # ê·œì •ëª… ë§¤ì¹­ í™•ì¸ (parent_path, title, textì—ì„œ í™•ì¸)
        chunk_text = f"{' '.join(chunk.parent_path)} {chunk.title} {chunk.text}"
        chunk_text_normalized = chunk_text.replace(" ", "").replace("Â·", "")

        # ê·œì •ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        reg_match = reg_name_normalized in chunk_text_normalized or reg_name in chunk_text

        # ì¡°í•­ ë²ˆí˜¸ ë§¤ì¹­ í™•ì¸
        article_match = article_pattern in chunk_text

        if reg_match and article_match:
            found = True
            chunk_reg = chunk.parent_path[0] if chunk.parent_path else "N/A"
            matched_chunks.append({
                "regulation": chunk_reg,
                "article": chunk.title,
                "content_preview": chunk.text[:200] if chunk.text else "",
                "score": result.score,
            })

    return {
        **citation,
        "verified": found,
        "matched_chunks": matched_chunks[:3],  # ìµœëŒ€ 3ê°œ
    }


def format_results(results: list[dict], verbose: bool = False) -> str:
    """ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    output = []
    output.append("=" * 60)
    output.append("ğŸ“‹ íŒ©íŠ¸ì²´í¬ ê²°ê³¼")
    output.append("=" * 60)

    verified_count = sum(1 for r in results if r["verified"])
    total_count = len(results)

    output.append(f"\nâœ… ê²€ì¦ ì™„ë£Œ: {verified_count}/{total_count} ({verified_count/total_count*100:.0f}%)\n")

    for i, result in enumerate(results, 1):
        status = "âœ…" if result["verified"] else "âŒ"
        output.append(f"{i}. {status} {result['original']}")

        if result["verified"]:
            if verbose and result["matched_chunks"]:
                chunk = result["matched_chunks"][0]
                output.append(f"   â†’ í™•ì¸ë¨: {chunk['regulation']} (ì ìˆ˜: {chunk['score']:.2f})")
                if chunk["content_preview"]:
                    preview = chunk["content_preview"][:100].replace("\n", " ")
                    output.append(f"   â†’ ë‚´ìš©: {preview}...")
        else:
            output.append("   â†’ âš ï¸ í•´ë‹¹ ê·œì •/ì¡°í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        output.append("")

    output.append("=" * 60)

    if verified_count < total_count:
        output.append("âš ï¸ ì¼ë¶€ ì¸ìš©ì´ ê²€ì¦ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í• ë£¨ì‹œë„¤ì´ì…˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    else:
        output.append("âœ… ëª¨ë“  ì¸ìš©ì´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return "\n".join(output)


def main():
    parser = argparse.ArgumentParser(
        description="LLM ë‹µë³€ì˜ ê·œì • ì¸ìš©ì„ íŒ©íŠ¸ì²´í¬í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("--text", "-t", help="ê²€ì¦í•  í…ìŠ¤íŠ¸")
    parser.add_argument("--file", "-f", help="ê²€ì¦í•  í…ìŠ¤íŠ¸ê°€ ë‹´ê¸´ íŒŒì¼")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ì¶œë ¥")
    parser.add_argument("--json", "-j", action="store_true", help="JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥")
    parser.add_argument("--db-path", default="data/chroma_db", help="ChromaDB ê²½ë¡œ")

    args = parser.parse_args()

    # í…ìŠ¤íŠ¸ ì…ë ¥ ë°›ê¸°
    if args.text:
        text = args.text
    elif args.file:
        text = Path(args.file).read_text(encoding="utf-8")
    elif not sys.stdin.isatty():
        text = sys.stdin.read()
    else:
        parser.print_help()
        print("\nâŒ ì˜¤ë¥˜: ê²€ì¦í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    # ì¸ìš© ì¶”ì¶œ
    citations = extract_citations(text)

    if not citations:
        print("â„¹ï¸ í…ìŠ¤íŠ¸ì—ì„œ ê·œì • ì¸ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    print(f"â„¹ï¸ {len(citations)}ê°œì˜ ì¸ìš©ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê²€ì¦ ì¤‘...")

    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    store = ChromaVectorStore(persist_directory=args.db_path)

    # ê° ì¸ìš© ê²€ì¦
    results = []
    for citation in citations:
        result = verify_citation(store, citation)
        results.append(result)

    # ê²°ê³¼ ì¶œë ¥
    if args.json:
        print(json.dumps(results, ensure_ascii=False, indent=2))
    else:
        print(format_results(results, verbose=args.verbose))

    # ê²€ì¦ ì‹¤íŒ¨ê°€ ìˆìœ¼ë©´ exit code 1
    if not all(r["verified"] for r in results):
        sys.exit(1)


if __name__ == "__main__":
    main()
