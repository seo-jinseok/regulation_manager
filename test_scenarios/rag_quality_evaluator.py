"""
Comprehensive RAG Quality Evaluator for University Regulation Manager.

This script executes diverse test queries through the RAG system and evaluates
the quality of responses across multiple dimensions:
- Intent Recognition
- Answer Accuracy
- Completeness
- Clarity
- Citation Quality
- Hallucination Detection

Executes queries simulating different user personas and query styles.
"""

import sys
import time
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

# Load .env file before importing project modules
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))


class IntentRecognitionScore(Enum):
    """Intent recognition quality levels."""

    PERFECT = 5
    GOOD = 4
    ACCEPTABLE = 3
    PARTIAL = 2
    POOR = 1


class AnswerQualityScore(Enum):
    """Answer quality levels."""

    EXCELLENT = 5
    GOOD = 4
    ACCEPTABLE = 3
    POOR = 2
    INCORRECT = 1


class UserExperienceScore(Enum):
    """User experience quality levels."""

    EXCELLENT = 5
    GOOD = 4
    ACCEPTABLE = 3
    POOR = 2
    FRUSTRATING = 1


@dataclass
class TestQuery:
    """A test query with metadata."""

    query: str
    persona: str
    query_style: str
    expertise: str
    expected_intent: str
    expected_keywords: List[str] = field(default_factory=list)


@dataclass
class EvaluationResult:
    """Result of evaluating a single query."""

    query: str
    persona: str
    query_style: str
    answer_text: str
    sources: List[Dict[str, Any]]
    confidence: float
    intent_score: int
    answer_score: int
    ux_score: int
    issues: List[str] = field(default_factory=list)
    strengths: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


# Diverse Test Queries for Quality Evaluation
TEST_QUERIES = [
    # ===== Freshman Student Queries =====
    TestQuery(
        query="ÏàòÍ∞ï Ïã†Ï≤≠ Ïñ∏Ï†úÍπåÏßÄÏïº?",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Íµ¨Ïñ¥Ï≤¥",
        expertise="Ï¥àÍ∏â",
        expected_intent="registration_deadline",
        expected_keywords=["ÏàòÍ∞ïÏã†Ï≤≠", "Í∏∞Í∞Ñ"],
    ),
    TestQuery(
        query="Ï°∏ÏóÖÌïòÎ†§Î©¥ ÌïôÏ†ê Î™á Ï†ê ÌïÑÏöîÌï¥?",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Ï†ïÌôï",
        expertise="Ï¥àÍ∏â",
        expected_intent="graduation_requirements",
        expected_keywords=["Ï°∏ÏóÖ", "ÌïôÏ†ê"],
    ),
    TestQuery(
        query="Ïû•ÌïôÍ∏à Ïã†Ï≤≠ÌïòÎäî Î≤ï",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Î™®Ìò∏",
        expertise="Ï¥àÍ∏â",
        expected_intent="scholarship_application",
        expected_keywords=["Ïû•ÌïôÍ∏à", "Ïã†Ï≤≠"],
    ),
    TestQuery(
        query="ÏïÑ Í∑∏Í≤å Î≠êÎÉêÎ©¥ ÌïôÏÉùÌöåÎπÑ ÎÇ©Î∂ÄÌïòÎäî Í±∞ Ïñ¥ÎîîÏÑú Ìï¥?",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Íµ¨Ïñ¥Ï≤¥/Í∏¥",
        expertise="Ï¥àÍ∏â",
        expected_intent="student_council_fee",
        expected_keywords=["ÌïôÏÉùÌöåÎπÑ", "ÎÇ©Î∂Ä"],
    ),
    TestQuery(
        query="Ìú¥ÌïôÌïòÍ≥† Ïã∂ÏùÄÎç∞ Ïñ¥ÎñªÍ≤å Ìï¥?",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Î™®Ìò∏",
        expertise="Ï¥àÍ∏â",
        expected_intent="leave_of_absence",
        expected_keywords=["Ìú¥Ìïô", "Ï†àÏ∞®"],
    ),
    # ===== Graduate Student Queries =====
    TestQuery(
        query="Î∞ïÏÇ¨Í≥ºÏ†ï Ïó∞Íµ¨Ïû•Î†§Í∏à ÏßÄÍ∏â Í∏∞Ï§ÄÍ≥º Ïã†Ï≤≠ ÏÑúÎ•òÍ∞Ä Í∂ÅÍ∏àÌï©ÎãàÎã§.",
        persona="ÎåÄÌïôÏõêÏÉù",
        query_style="Ï†ïÌôï",
        expertise="Ï§ëÍ∏â",
        expected_intent="research_grant",
        expected_keywords=["Ïó∞Íµ¨Ïû•Î†§Í∏à", "ÏßÄÍ∏âÍ∏∞Ï§Ä", "Ïã†Ï≤≠ÏÑúÎ•ò"],
    ),
    TestQuery(
        query="ÎÖºÎ¨∏ Ïã¨ÏÇ¨ ÏúÑÏõê ÏúÑÏ¥â Ï†àÏ∞®ÏôÄ Í∏∞Í∞ÑÏùÑ ÏïåÍ≥† Ïã∂ÏäµÎãàÎã§.",
        persona="ÎåÄÌïôÏõêÏÉù",
        query_style="Ï†ïÌôï",
        expertise="Ï§ëÍ∏â",
        expected_intent="thesis_committee",
        expected_keywords=["ÎÖºÎ¨∏Ïã¨ÏÇ¨", "ÏúÑÏõê", "ÏúÑÏ¥â"],
    ),
    TestQuery(
        query="Ï°∏ÏóÖ ÏöîÍ±¥ Ï§ë Ïô∏Íµ≠Ïñ¥ ÏÑ±Ï†Å Ï†úÏ∂úÏóê Í¥ÄÌïú Í∑úÏ†ï",
        persona="ÎåÄÌïôÏõêÏÉù",
        query_style="Ï†ïÌôï",
        expertise="Ï†ÑÎ¨∏Í∞Ä",
        expected_intent="graduation_requirements_language",
        expected_keywords=["Ï°∏ÏóÖÏöîÍ±¥", "Ïô∏Íµ≠Ïñ¥ÏÑ±Ï†Å"],
    ),
    # ===== Professor Queries =====
    TestQuery(
        query="ÍµêÏõê Ïù∏ÏÇ¨ ÌèâÍ∞Ä Ï†ïÏ±Ö Ï§ë Ïó∞Íµ¨ ÏÑ±Í≥º ÌèâÍ∞Ä Í∏∞Ï§Ä",
        persona="ÍµêÏàò",
        query_style="Ï†ïÌôï",
        expertise="Ï†ÑÎ¨∏Í∞Ä",
        expected_intent="faculty_evaluation",
        expected_keywords=["Ïù∏ÏÇ¨ÌèâÍ∞Ä", "Ïó∞Íµ¨ÏÑ±Í≥º", "ÌèâÍ∞ÄÍ∏∞Ï§Ä"],
    ),
    TestQuery(
        query="ÌïôÎ∂ÄÏÉù Ïó∞Íµ¨Ïõê Ï±ÑÏö© Ïãú ÌñâÏ†ï Ï†àÏ∞®",
        persona="ÍµêÏàò",
        query_style="Ï†ïÌôï",
        expertise="Ï§ëÍ∏â",
        expected_intent="undergraduate_researcher",
        expected_keywords=["ÌïôÎ∂ÄÏÉùÏó∞Íµ¨Ïõê", "Ï±ÑÏö©", "ÌñâÏ†ïÏ†àÏ∞®"],
    ),
    TestQuery(
        query="Ïó∞Íµ¨ÎπÑ ÏßëÌñâ Ïãú Ïú†ÏùòÌï¥Ïïº Ìï† Í∑úÏ†ï ÏÇ¨Ìï≠",
        persona="ÍµêÏàò",
        query_style="Ï†ïÌôï",
        expertise="Ï†ÑÎ¨∏Í∞Ä",
        expected_intent="research_expenditure",
        expected_keywords=["Ïó∞Íµ¨ÎπÑ", "ÏßëÌñâ", "Í∑úÏ†ï"],
    ),
    # ===== Staff Queries =====
    TestQuery(
        query="ÏßÅÏõê Î≥µÎ¨¥ Í∑úÏ†ï Ï§ë Ïó∞Ï∞® ÏÇ¨Ïö©Ïóê Í¥ÄÌïú Í∑úÏ†ï",
        persona="ÍµêÏßÅÏõê",
        query_style="Ï†ïÌôï",
        expertise="Ï§ëÍ∏â",
        expected_intent="annual_leave",
        expected_keywords=["Î≥µÎ¨¥Í∑úÏ†ï", "Ïó∞Ï∞®", "ÏÇ¨Ïö©"],
    ),
    TestQuery(
        query="Íµ¨Îß§ ÏûÖÏ∞∞ ÏßÑÌñâ Ï†àÏ∞®ÏôÄ ÌïÑÏöî ÏÑúÎ•ò",
        persona="ÍµêÏßÅÏõê",
        query_style="Ï†ïÌôï",
        expertise="Ï†ÑÎ¨∏Í∞Ä",
        expected_intent="procurement_procedure",
        expected_keywords=["Íµ¨Îß§ÏûÖÏ∞∞", "Ï†àÏ∞®", "ÏÑúÎ•ò"],
    ),
    # ===== Parent Queries =====
    TestQuery(
        query="ÌïôÏÉù Î≥µÏßÄ Ïπ¥Îìú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í≥≥Í≥º Ìï†Ïù∏ ÌòúÌÉù",
        persona="ÌïôÎ∂ÄÎ™®",
        query_style="Íµ¨Ïñ¥Ï≤¥",
        expertise="Ï¥àÍ∏â",
        expected_intent="student_welfare",
        expected_keywords=["Î≥µÏßÄÏπ¥Îìú", "Ìï†Ïù∏"],
    ),
    TestQuery(
        query="Í∏∞ÏàôÏÇ¨ ÎπÑÏö©Í≥º ÎÇ©Î∂Ä Î∞©Î≤ï",
        persona="ÌïôÎ∂ÄÎ™®",
        query_style="Ï†ïÌôï",
        expertise="Ï¥àÍ∏â",
        expected_intent="dormitory_fee",
        expected_keywords=["Í∏∞ÏàôÏÇ¨", "ÎπÑÏö©", "ÎÇ©Î∂Ä"],
    ),
    TestQuery(
        query="ÌïôÏÉùÏù¥ Ìú¥ÌïôÌïòÎ©¥ Îì±Î°ùÍ∏à ÌôòÎ∂àÎêòÎÇòÏöî?",
        persona="ÌïôÎ∂ÄÎ™®",
        query_style="Íµ¨Ïñ¥Ï≤¥",
        expertise="Ï¥àÍ∏â",
        expected_intent="tuition_refund",
        expected_keywords=["Ìú¥Ìïô", "Îì±Î°ùÍ∏à", "ÌôòÎ∂à"],
    ),
    # ===== Ambiguous Queries =====
    TestQuery(
        query="Ï°∏ÏóÖ",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Î™®Ìò∏",
        expertise="Ï¥àÍ∏â",
        expected_intent="graduation_requirements",
        expected_keywords=["Ï°∏ÏóÖ"],
    ),
    TestQuery(
        query="Îì±Î°ù",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Î™®Ìò∏",
        expertise="Ï¥àÍ∏â",
        expected_intent="registration",
        expected_keywords=["Îì±Î°ù"],
    ),
    # ===== Multi-part Queries =====
    TestQuery(
        query="ÏàòÍ∞ï Ïã†Ï≤≠ Í∏∞Í∞ÑÍ≥º Ï†ïÏ†ï Í∏∞Í∞Ñ, Í∑∏Î¶¨Í≥† Ï∑®ÏÜå Í∏∞Í∞ÑÏùÑ ÏïåÎ†§Ï£ºÏÑ∏Ïöî.",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Î≥µÌï©",
        expertise="Ï¥àÍ∏â",
        expected_intent="registration_periods",
        expected_keywords=["ÏàòÍ∞ïÏã†Ï≤≠", "Ï†ïÏ†ï", "Ï∑®ÏÜå", "Í∏∞Í∞Ñ"],
    ),
    TestQuery(
        query="Ïó∞Íµ¨Ïû•Î†§Í∏à Ïã†Ï≤≠ ÏûêÍ≤©Í≥º Ï†àÏ∞®, Í∑∏Î¶¨Í≥† Ï†úÏ∂ú ÏÑúÎ•òÍ∞Ä Î¨¥ÏóáÏù∏Í∞ÄÏöî?",
        persona="ÎåÄÌïôÏõêÏÉù",
        query_style="Î≥µÌï©",
        expertise="Ï§ëÍ∏â",
        expected_intent="research_grant_details",
        expected_keywords=["Ïó∞Íµ¨Ïû•Î†§Í∏à", "ÏûêÍ≤©", "Ï†àÏ∞®", "ÏÑúÎ•ò"],
    ),
    # ===== Incorrect Terminology =====
    TestQuery(
        query="ÌïôÍ∏∞ Îßê ÏãúÌóò ÏùºÏ†ï ÏïåÎ†§Ï§ò",
        persona="Ïã†ÏûÖÏÉù",
        query_style="ÏûòÎ™ªÎêú Ïö©Ïñ¥",
        expertise="Ï¥àÍ∏â",
        expected_intent="final_exam_schedule",
        expected_keywords=["ÏãúÌóò", "ÏùºÏ†ï"],
    ),
    TestQuery(
        query="ÌïôÍµê ÎèÑÏÑúÍ¥Ä ÎåÄÏ∂ú Ïó∞Ïû• Î∞©Î≤ï",
        persona="Ïã†ÏûÖÏÉù",
        query_style="ÏûòÎ™ªÎêú Ïö©Ïñ¥",
        expertise="Ï¥àÍ∏â",
        expected_intent="library_renewal",
        expected_keywords=["ÎèÑÏÑúÍ¥Ä", "ÎåÄÏ∂ú", "Ïó∞Ïû•"],
    ),
    # ===== Typos/Grammar Errors =====
    TestQuery(
        query="ÏÑ±Ï†Å Ïù¥Ïùò Ïã†Ï≤≠ÌïòÎäîÎ≤ï ÏïåÎ†§Ï§ò",
        persona="Ïã†ÏûÖÏÉù",
        query_style="Ïò§ÌÉÄ/Î¨∏Î≤ïÏò§Î•ò",
        expertise="Ï¥àÍ∏â",
        expected_intent="grade_appeal",
        expected_keywords=["ÏÑ±Ï†Å", "Ïù¥ÏùòÏã†Ï≤≠"],
    ),
    TestQuery(
        query="Ï°∏ÏóÖ ÎÖºÎ¨∏ Ï†úÏ∂ú ÎßàÍ∞êÏù¥ Ïñ∏Ï†úÏù∏Í∞ÄÏöî??",
        persona="ÎåÄÌïôÏõêÏÉù",
        query_style="Ïò§ÌÉÄ/Î¨∏Î≤ïÏò§Î•ò",
        expertise="Ï§ëÍ∏â",
        expected_intent="thesis_deadline",
        expected_keywords=["Ï°∏ÏóÖÎÖºÎ¨∏", "Ï†úÏ∂ú", "ÎßàÍ∞ê"],
    ),
    TestQuery(
        query="Ïó∞Íµ¨ÎπÑ ÏßëÌñâÏãú Ïú†ÏùòÏÇ¨Ìï≠Í≥º ÏòÅÏàòÏ¶ù Ï†úÏ∂úÎ∞©Î≤ï",
        persona="ÍµêÏàò",
        query_style="Ïò§ÌÉÄ/Î¨∏Î≤ïÏò§Î•ò",
        expertise="Ï†ÑÎ¨∏Í∞Ä",
        expected_intent="research_expenditure_receipt",
        expected_keywords=["Ïó∞Íµ¨ÎπÑ", "ÏßëÌñâ", "ÏòÅÏàòÏ¶ù"],
    ),
    # ===== International Student Queries =====
    TestQuery(
        query="How do I apply for leave of absence?",
        persona="Ïú†ÌïôÏÉù",
        query_style="ÏòÅÎ¨∏",
        expertise="Ï§ëÍ∏â",
        expected_intent="leave_of_absence",
        expected_keywords=["Ìú¥Ìïô", "Ïã†Ï≤≠", "Ï†àÏ∞®"],
    ),
    TestQuery(
        query="What is the tuition fee for international students?",
        persona="Ïú†ÌïôÏÉù",
        query_style="ÏòÅÎ¨∏",
        expertise="Ï§ëÍ∏â",
        expected_intent="international_tuition",
        expected_keywords=["Îì±Î°ùÍ∏à", "Ïú†ÌïôÏÉù", "ÎπÑÏö©"],
    ),
    TestQuery(
        query="ÎπÑÏûê Î∞úÍ∏âÏùÑ ÏúÑÌïú ÌïôÏÉù ÌôïÏù∏ Ï†àÏ∞®Í∞Ä Í∂ÅÍ∏àÌï©ÎãàÎã§.",
        persona="Ïú†ÌïôÏÉù",
        query_style="Íµ≠Î¨∏ÌòºÏö©",
        expertise="Ï§ëÍ∏â",
        expected_intent="visa_confirmation",
        expected_keywords=["ÎπÑÏûê", "ÌïôÏÉù ÌôïÏù∏", "Ïû¨ÌïôÏ¶ùÎ™Ö"],
    ),
    TestQuery(
        query="Í∏∞ÏàôÏÇ¨ Ïã†Ï≤≠ÌïòÎäî Î∞©Î≤ï ÏïåÎ†§Ï£ºÏÑ∏Ïöî. Can international students apply?",
        persona="Ïú†ÌïôÏÉù",
        query_style="Íµ≠Î¨∏ÌòºÏö©",
        expertise="Ï§ëÍ∏â",
        expected_intent="dormitory_application",
        expected_keywords=["Í∏∞ÏàôÏÇ¨", "Ïã†Ï≤≠", "Ïú†ÌïôÏÉù"],
    ),
    TestQuery(
        query="Where can I get English support for academic writing?",
        persona="Ïú†ÌïôÏÉù",
        query_style="ÏòÅÎ¨∏",
        expertise="Ï§ëÍ∏â",
        expected_intent="english_support",
        expected_keywords=["ÏòÅÏñ¥", "ÌïôÏà†ÏßÄÎèÑ", "ÏûëÏÑ±"],
    ),
]


class RAGQualityEvaluator:
    """Comprehensive RAG quality evaluator."""

    def __init__(self, db_path: str = "data/chroma_db"):
        """Initialize the evaluator."""
        from src.rag.config import get_config
        from src.rag.infrastructure.chroma_store import ChromaVectorStore
        from src.rag.infrastructure.llm_adapter import LLMClientAdapter
        from src.rag.interface.query_handler import QueryHandler, QueryOptions

        self.db_path = db_path
        self.config = get_config()
        self.results: List[EvaluationResult] = []

        # Initialize components
        self.store = ChromaVectorStore(persist_directory=db_path)
        self.llm_client = LLMClientAdapter(
            provider=self.config.llm_provider,
            model=self.config.llm_model,
            base_url=self.config.llm_base_url,
        )

        # Create QueryHandler
        self.query_handler = QueryHandler(
            store=self.store,
            llm_client=self.llm_client,
            use_reranker=True,
        )

        # Default query options
        self.default_options = QueryOptions(
            top_k=5,
            use_rerank=True,
        )

    def execute_query(self, query: str, answer_mode: bool = True) -> Dict[str, Any]:
        """
        Execute a query through the RAG system.

        Returns:
            Dict with answer_text, sources, confidence
        """
        try:
            from src.rag.interface.query_handler import QueryOptions

            # Try ask mode first, fall back to search mode if LLM fails
            options = QueryOptions(
                top_k=5,
                use_rerank=True,
                force_mode="ask" if answer_mode else "search",
            )

            result = self.query_handler.process_query(
                query=query,
                options=options,
            )

            # Extract answer and sources from QueryResult
            answer_text = result.content if result.success else ""
            sources = []
            confidence = 0.0

            # Try to extract structured data with multiple fallback paths
            if result.data:
                # Path 1: FunctionGemma tool results
                if "tool_results" in result.data:
                    for tool_result in result.data.get("tool_results", []):
                        if tool_result.get("tool_name") == "search_regulations":
                            # Get result dict with better error handling
                            result_data = tool_result.get("result")
                            if result_data and isinstance(result_data, dict):
                                search_results = result_data.get("results", [])
                                # Extract sources with flexible field mapping
                                sources = []
                                for r in search_results[:5]:
                                    if isinstance(r, dict):
                                        sources.append(
                                            {
                                                "title": r.get("title")
                                                or r.get("regulation_title", ""),
                                                "text": (
                                                    r.get("text", "")
                                                    or r.get("content", "")
                                                )[:200],
                                                "rule_code": r.get("rule_code", "")
                                                or r.get("rule_code", ""),
                                                "score": r.get("score", 0.0)
                                                or r.get("similarity", 0.0),
                                            }
                                        )
                                confidence = sources[0]["score"] if sources else 0.0
                                break
                            break

                # Path 2: Direct sources
                elif "sources" in result.data:
                    sources_data = result.data["sources"]
                    if sources_data:
                        sources = [
                            {
                                "title": s.get("title", ""),
                                "text": (s.get("text", "") or s.get("content", ""))[
                                    :200
                                ],
                                "rule_code": s.get("rule_code", ""),
                                "score": s.get("score", 0.0),
                            }
                            for s in sources_data[:5]
                            if isinstance(s, dict)
                        ]
                        confidence = sources[0]["score"] if sources else 0.0

                # Path 3: Search results in different format
                elif "search_results" in result.data:
                    search_results = result.data["search_results"]
                    if isinstance(search_results, list):
                        sources = [
                            {
                                "title": r.get("title", ""),
                                "text": (r.get("text", "") or r.get("content", ""))[
                                    :200
                                ],
                                "rule_code": r.get("rule_code", ""),
                                "score": r.get("score", 0.0),
                            }
                            for r in search_results[:5]
                            if isinstance(r, dict)
                        ]
                        confidence = sources[0]["score"] if sources else 0.0

            # If no sources extracted but answer contains regulation references,
            # try to extract from content
            if not sources and result.content:
                # Look for patterns like "ÍµêÏõêÏù∏ÏÇ¨Í∑úÏ†ï" or "Ï†úXÏ°∞" in content

                # Check if there's any regulation content
                if any(
                    term in result.content
                    for term in ["Í∑úÏ†ï", "Ï°∞", "Ìï≠", "Ïóê Îî∞Îùº", "Í¥ÄÎ†®ÌïòÏó¨"]
                ):
                    # Content exists but sources weren't extracted
                    # Mark as having content even if sources extraction failed
                    answer_text = result.content

            return {
                "answer_text": answer_text,
                "sources": sources,
                "confidence": confidence,
                "has_content": bool(answer_text),
            }

        except Exception as e:
            import traceback

            return {
                "answer_text": f"Error: {str(e)}\n{traceback.format_exc()}",
                "sources": [],
                "confidence": 0.0,
                "has_content": False,
            }

    def evaluate_query(self, test_query: TestQuery) -> EvaluationResult:
        """Evaluate a single test query."""
        result = self.execute_query(test_query.query)

        # Analyze the result
        issues = []
        strengths = []
        recommendations = []

        answer_text = result["answer_text"]
        sources = result["sources"]
        confidence = result["confidence"]

        # Intent Recognition Assessment
        intent_score = self._assess_intent_recognition(test_query, answer_text, sources)

        # Answer Quality Assessment
        answer_score = self._assess_answer_quality(test_query, answer_text, sources)

        # User Experience Assessment
        ux_score = self._assess_user_experience(
            test_query, answer_text, sources, confidence
        )

        # Identify issues
        if not answer_text or answer_text.startswith("Error:"):
            issues.append("ÏãúÏä§ÌÖú Ïò§Î•òÎ°ú ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®")
        elif len(answer_text) < 50:
            issues.append("ÎãµÎ≥ÄÏù¥ ÎÑàÎ¨¥ ÏßßÏùå")
        elif not sources:
            issues.append("Í¥ÄÎ†® Í∑úÏ†ïÏùÑ Ï∞æÏßÄ Î™ªÌï®")

        # Identify strengths
        if confidence > 0.8:
            strengths.append("ÎÜíÏùÄ Í≤ÄÏÉâ Ïã†Î¢∞ÎèÑ")
        if len(sources) >= 3:
            strengths.append("Îã§ÏñëÌïú Ï∞∏Í≥† Í∑úÏ†ï Ï†úÍ≥µ")
        if "Ï†ú" in answer_text or "Ï°∞" in answer_text:
            strengths.append("Íµ¨Ï≤¥Ï†ÅÏù∏ Ï°∞Î¨∏ Ïù∏Ïö©")

        # Generate recommendations
        if intent_score < 3:
            recommendations.append("ÏÇ¨Ïö©Ïûê ÏùòÎèÑ ÌååÏïÖ Í∞úÏÑ† ÌïÑÏöî")
        if answer_score < 3:
            recommendations.append("ÎãµÎ≥Ä Ï†ïÌôïÎèÑ Î∞è ÏôÑÍ≤∞ÏÑ± Í∞úÏÑ† ÌïÑÏöî")
        if not sources:
            recommendations.append("Í≤ÄÏÉâ ÌíàÏßà Í∞úÏÑ† ÌïÑÏöî")

        return EvaluationResult(
            query=test_query.query,
            persona=test_query.persona,
            query_style=test_query.query_style,
            answer_text=answer_text,
            sources=sources,
            confidence=confidence,
            intent_score=intent_score,
            answer_score=answer_score,
            ux_score=ux_score,
            issues=issues,
            strengths=strengths,
            recommendations=recommendations,
        )

    def _assess_intent_recognition(
        self, test_query: TestQuery, answer_text: str, sources: List[Dict]
    ) -> int:
        """Assess intent recognition quality (1-5)."""
        # Check if expected keywords are in answer or sources
        keyword_matches = sum(
            1
            for kw in test_query.expected_keywords
            if kw in answer_text or any(kw in s.get("text", "") for s in sources)
        )

        # Check if sources are relevant
        relevant_sources = sum(1 for s in sources if s.get("score", 0) > 0.5)

        if (
            keyword_matches >= len(test_query.expected_keywords)
            and relevant_sources >= 2
        ):
            return 5  # Perfect
        elif (
            keyword_matches >= len(test_query.expected_keywords) // 2
            and relevant_sources >= 1
        ):
            return 4  # Good
        elif keyword_matches >= 1:
            return 3  # Acceptable
        elif relevant_sources >= 1:
            return 2  # Partial
        else:
            return 1  # Poor

    def _assess_answer_quality(
        self, test_query: TestQuery, answer_text: str, sources: List[Dict]
    ) -> int:
        """Assess answer quality (1-5)."""
        if not answer_text or answer_text.startswith("Error:"):
            return 1

        # Check for hallucination indicators
        hallucination_terms = ["ÌïúÍµ≠Ïô∏Íµ≠Ïñ¥ÎåÄ", "ÏÑúÏö∏ÎåÄ", "02-XXXX", "ÏùºÎ∞òÏ†ÅÏúºÎ°ú"]
        has_hallucination = any(term in answer_text for term in hallucination_terms)

        if has_hallucination:
            return 1  # Hallucination detected

        # Check completeness
        if len(answer_text) < 100:
            return 2  # Too short

        # Check for specific regulation citations
        has_citations = any(
            term in answer_text for term in ["Ï†ú", "Ï°∞", "Ìï≠", "Í∑úÏ†ï", "Ïóê Îî∞Î•¥"]
        )

        if has_citations and len(answer_text) >= 200:
            return 5  # Excellent
        elif has_citations:
            return 4  # Good
        elif len(answer_text) >= 200:
            return 3  # Acceptable
        else:
            return 2  # Poor

    def _assess_user_experience(
        self,
        test_query: TestQuery,
        answer_text: str,
        sources: List[Dict],
        confidence: float,
    ) -> int:
        """Assess user experience quality (1-5)."""
        if not answer_text or answer_text.startswith("Error:"):
            return 1

        # Check if answer matches expertise level
        is_too_formal = test_query.expertise == "Ï¥àÍ∏â" and all(
            term in answer_text for term in ["Í∑ÄÌïò", "Í∑ÄÌïòÏùò", "ÌïòÏó¨Ïïº", "ÌïòÏó¨ÏïºÌïúÎã§"]
        )

        # Check if answer is clear and well-structured
        has_structure = any(marker in answer_text for marker in ["1.", "-", "‚Ä¢", "‚Äª"])

        if confidence > 0.8 and not is_too_formal and has_structure:
            return 5  # Excellent
        elif confidence > 0.6 and not is_too_formal:
            return 4  # Good
        elif confidence > 0.4:
            return 3  # Acceptable
        elif confidence > 0.2:
            return 2  # Poor
        else:
            return 1  # Frustrating

    def run_evaluation(self, limit: Optional[int] = None) -> List[EvaluationResult]:
        """Run the full evaluation."""
        queries_to_test = TEST_QUERIES[:limit] if limit else TEST_QUERIES

        print("üîç RAG Quality Evaluation Started")
        print(f"   Testing {len(queries_to_test)} queries...")
        print()

        for i, test_query in enumerate(queries_to_test, 1):
            print(f"[{i}/{len(queries_to_test)}] Testing: {test_query.query}")
            result = self.evaluate_query(test_query)
            self.results.append(result)
            time.sleep(0.5)  # Brief pause to avoid overwhelming

        print()
        print(f"‚úÖ Evaluation Complete: {len(self.results)} queries tested")
        return self.results

    def generate_report(self) -> str:
        """Generate comprehensive evaluation report."""
        if not self.results:
            return "No results to report."

        # Calculate statistics
        total = len(self.results)
        avg_intent = sum(r.intent_score for r in self.results) / total
        avg_answer = sum(r.answer_score for r in self.results) / total
        avg_ux = sum(r.ux_score for r in self.results) / total
        avg_confidence = sum(r.confidence for r in self.results) / total

        # Count by persona
        by_persona = {}
        for result in self.results:
            persona = result.persona
            if persona not in by_persona:
                by_persona[persona] = {"count": 0, "intent": 0, "answer": 0, "ux": 0}
            by_persona[persona]["count"] += 1
            by_persona[persona]["intent"] += result.intent_score
            by_persona[persona]["answer"] += result.answer_score
            by_persona[persona]["ux"] += result.ux_score

        # Count issues
        issue_counts = {}
        for result in self.results:
            for issue in result.issues:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1

        # Generate report
        report_lines = [
            "=" * 80,
            "RAG Quality Evaluation Report",
            "=" * 80,
            "",
            "## Test Summary",
            f"- Total queries tested: {total}",
            f"- Pass rate (Answer Score >= 3): {sum(1 for r in self.results if r.answer_score >= 3) / total:.1%}",
            "",
            "## Overall Scores",
            f"- Intent Recognition: {avg_intent:.2f}/5.0",
            f"- Answer Quality: {avg_answer:.2f}/5.0",
            f"- User Experience: {avg_ux:.2f}/5.0",
            f"- Average Confidence: {avg_confidence:.2%}",
            "",
            "## Results by Persona",
            "",
        ]

        for persona, stats in sorted(by_persona.items()):
            count = stats["count"]
            report_lines.extend(
                [
                    f"### {persona}",
                    f"  Queries: {count}",
                    f"  Intent: {stats['intent'] / count:.2f}/5.0",
                    f"  Answer: {stats['answer'] / count:.2f}/5.0",
                    f"  UX: {stats['ux'] / count:.2f}/5.0",
                    "",
                ]
            )

        # Issues section
        report_lines.extend(
            [
                "## Issues Found",
                "",
            ]
        )

        for issue, count in sorted(
            issue_counts.items(), key=lambda x: x[1], reverse=True
        ):
            report_lines.append(f"{count}x: {issue}")

        if issue_counts:
            report_lines.append("")

        # Recommendations
        all_recommendations = {}
        for result in self.results:
            for rec in result.recommendations:
                all_recommendations[rec] = all_recommendations.get(rec, 0) + 1

        report_lines.extend(
            [
                "## Improvement Recommendations",
                "",
            ]
        )

        for rec, count in sorted(
            all_recommendations.items(), key=lambda x: x[1], reverse=True
        ):
            report_lines.append(f"Priority {count}: {rec}")

        # Detailed results
        report_lines.extend(
            [
                "",
                "## Detailed Results",
                "",
            ]
        )

        for i, result in enumerate(self.results, 1):
            report_lines.extend(
                [
                    f"### {i}. {result.query} ({result.persona}, {result.query_style})",
                    f"**Scores:** Intent={result.intent_score}/5, Answer={result.answer_score}/5, UX={result.ux_score}/5",
                    f"**Confidence:** {result.confidence:.2%}",
                ]
            )

            if result.issues:
                report_lines.append(f"**Issues:** {', '.join(result.issues)}")
            if result.strengths:
                report_lines.append(f"**Strengths:** {', '.join(result.strengths)}")
            if result.recommendations:
                report_lines.append(
                    f"**Recommendations:** {', '.join(result.recommendations)}"
                )

            report_lines.extend(
                [
                    f"**Answer Preview:** {result.answer_text[:200]}...",
                    f"**Sources:** {len(result.sources)} found",
                    "",
                ]
            )

        report_lines.append("=" * 80)

        return "\n".join(report_lines)


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="RAG Quality Evaluator")
    parser.add_argument(
        "--limit", type=int, default=None, help="Limit number of queries to test"
    )
    parser.add_argument("--db-path", default="data/chroma_db", help="Path to ChromaDB")
    parser.add_argument("--output", default=None, help="Output report to file")
    parser.add_argument(
        "--use-llm-judge",
        action="store_true",
        help="Use new LLM-as-Judge 4-metric evaluation (Accuracy, Completeness, Citations, Context Relevance)",
    )
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="Use parallel persona evaluation (requires --use-llm-judge)",
    )
    parser.add_argument(
        "--queries-per-persona",
        type=int,
        default=5,
        help="Number of queries per persona for parallel evaluation",
    )

    args = parser.parse_args()

    # Check for parallel mode
    if args.parallel and not args.use_llm_judge:
        print(
            "‚ö†Ô∏è  --parallel requires --use-llm-judge. Enabling LLM-as-Judge evaluation."
        )
        args.use_llm_judge = True

    if args.use_llm_judge:
        # Use new LLM-as-Judge evaluation system
        from src.rag.config import get_config
        from src.rag.domain.evaluation import (
            LLMJudge,
            ParallelPersonaEvaluator,
        )
        from src.rag.infrastructure.llm_adapter import LLMClientAdapter

        config = get_config()
        llm_client = LLMClientAdapter(
            provider=config.llm_provider,
            model=config.llm_model,
            base_url=config.llm_base_url,
        )

        if args.parallel:
            # Parallel persona evaluation
            print("üöÄ Running parallel persona evaluation with LLM-as-Judge...")
            evaluator = ParallelPersonaEvaluator(
                db_path=args.db_path,
                llm_client=llm_client,
            )
            persona_results = evaluator.evaluate_parallel(
                queries_per_persona=args.queries_per_persona
            )

            # Generate and save report
            report = evaluator.generate_report()
            timestamp = time.strftime("%Y%m%d_%H%M%S")

            if args.output:
                output_path = args.output
            else:
                output_path = f"data/evaluations/parallel_eval_{timestamp}.md"

            with open(output_path, "w", encoding="utf-8") as f:
                f.write(report)

            # Save JSON results
            json_path = evaluator.save_results()

            print(f"‚úÖ Report saved to: {output_path}")
            print(f"‚úÖ JSON results saved to: {json_path}")
            print()

            # Print summary
            print("üìä Persona Summary:")
            for persona, result in persona_results.items():
                print(
                    f"  {persona}: {result.avg_score:.3f} avg, "
                    f"{result.pass_rate:.1%} pass rate"
                )

        else:
            # Single query LLM-as-Judge evaluation
            print("üîç Running LLM-as-Judge evaluation...")
            judge = LLMJudge(llm_client=llm_client)

            # Run evaluation using existing evaluator
            legacy_evaluator = RAGQualityEvaluator(db_path=args.db_path)
            legacy_evaluator.run_evaluation(limit=args.limit)

            # Re-evaluate results with LLM judge
            from src.rag.domain.evaluation import EvaluationBatch

            batch = EvaluationBatch(judge=judge)

            for legacy_result in legacy_evaluator.results:
                judge_result = judge.evaluate(
                    query=legacy_result.query,
                    answer=legacy_result.answer_text,
                    sources=legacy_result.sources,
                )
                batch.add_result(judge_result)

            # Generate report
            summary = batch.get_summary()

            report_lines = [
                "=" * 80,
                "RAG Quality Evaluation Report (LLM-as-Judge)",
                "=" * 80,
                "",
                "## Summary",
                f"- Total queries tested: {summary.total_queries}",
                f"- Passed: {summary.passed}",
                f"- Failed: {summary.failed}",
                f"- Pass rate: {summary.pass_rate:.1%}",
                "",
                "## Average Scores",
                f"- Overall: {summary.avg_overall_score:.3f}",
                f"- Accuracy: {summary.avg_accuracy:.3f}",
                f"- Completeness: {summary.avg_completeness:.3f}",
                f"- Citations: {summary.avg_citations:.3f}",
                f"- Context Relevance: {summary.avg_context_relevance:.3f}",
                "",
                "## Failure Patterns",
            ]

            for issue, count in sorted(
                summary.failure_patterns.items(), key=lambda x: x[1], reverse=True
            ):
                report_lines.append(f"- {issue}: {count}x")

            report_lines.extend(["", "## Detailed Results", ""])

            for i, result in enumerate(batch.results, 1):
                report_lines.extend(
                    [
                        f"### {i}. {result.query}",
                        f"**Score:** {result.overall_score:.3f} ({'PASS' if result.passed else 'FAIL'})",
                        f"**Metrics:** Acc={result.accuracy:.3f}, Comp={result.completeness:.3f}, "
                        f"Cit={result.citations:.3f}, Ctx={result.context_relevance:.3f}",
                    ]
                )

                if result.issues:
                    report_lines.append(f"**Issues:** {', '.join(result.issues)}")
                if result.strengths:
                    report_lines.append(f"**Strengths:** {', '.join(result.strengths)}")

                report_lines.extend(
                    [
                        f"**Answer:** {result.answer[:200]}...",
                        "",
                    ]
                )

            report_lines.append("=" * 80)
            report = "\n".join(report_lines)

            if args.output:
                with open(args.output, "w", encoding="utf-8") as f:
                    f.write(report)
                print(f"‚úÖ Report saved to: {args.output}")
            else:
                print(report)

    else:
        # Use legacy 3-point scale evaluation
        evaluator = RAGQualityEvaluator(db_path=args.db_path)
        evaluator.run_evaluation(limit=args.limit)

        report = evaluator.generate_report()

        if args.output:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(report)
            print(f"‚úÖ Report saved to: {args.output}")
        else:
            print(report)


if __name__ == "__main__":
    main()
