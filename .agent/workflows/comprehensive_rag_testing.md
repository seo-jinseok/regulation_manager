---
description: AI 에이전트 자율 실행을 위한 RAG 시스템 품질 심층 테스트 및 개선 워크플로우 (Strict Mode)
---

# RAG 시스템 품질 심층 테스트 및 개선 (Strict Mode)

이 워크플로우는 AI 에이전트가 **다양한 사용자 페르소나**를 시뮬레이션하여 RAG 시스템의 품질을 **엄격하게(Strictly)** 테스트하고, 답변 품질을 **비판적으로** 검토하여 개선하는 절차입니다.

> **⚠️ 평가자 마인드셋**
> 당신은 까다로운 품질 검수자입니다. 사용자 입장에서 "이게 정말 도움이 되는가?"를 냉정하게 판단하세요.
> *   ❌ "대체로 맞는 것 같다" → **Reject** (팩트체크 필수)
> *   ❌ "답변이 길고 상세하다" → **Reject** (핵심 정보 정확성 확인)
> *   ❌ "일반적으로 맞는 내용이다" → **Reject** (반드시 **이 학교**의 규정 확인)
> *   ❌ "절차를 설명했다" → **Reject** (구체적인 기한, 서류, 담당부서 명시 여부 확인)

---

## 성공 기준 (Strict Metrics)

| 메트릭 | 목표 | 엄격 모드 적용 사항 |
|--------|------|-------------------|
| 정적 테스트(Auto-Eval) 통과율 | ≥ 85% | - |
| 동적 쿼리 성공률 | ≥ 80% | **"부분 성공" = 실패**로 간주 |
| 멀티턴 대화 성공률 | ≥ 75% | 맥락 유지 실패 시 즉시 실패 |
| 답변 품질 점수 | ≥ 4.0/5.0 | 팩트체크 1개라도 실패 시 **최대 2.0점** |
| **팩트체크 수행률** | **100%** | 모든 답변에 대해 검증 쿼리 실행 필수 |

---

## Phase 0: 사전 검증

### 0.1 시스템 상태 및 데이터 확인

// turbo
```bash
uv run regulation status
```

데이터가 비어있거나 오래되었다면 동기화를 실행하세요:
```bash
uv run regulation sync data/output/규정집.json
```

### 0.2 LLM 연결 확인

// turbo
```bash
# .env의 LLM_PROVIDER에 따라 적절한 명령 실행
# Ollama 예시:
curl http://localhost:11434/api/tags 2>/dev/null || echo "Ollama 미실행"
```

---

## Phase 1: 정적 평가 (Baseline)

### 1.1 자동 평가 스크립트 실행

기존에 정의된 테스트셋(`evaluation_dataset.json`)을 사용하여 기초 성능을 측정합니다.

// turbo
```bash
uv run python scripts/auto_evaluate.py --run
```

### 1.2 결과 판정
*   **통과율 ≥ 85%**: Phase 2로 진행
*   **통과율 < 85%**: 즉시 Phase 5 (개선)로 이동하여 문제 해결 후 재시도

---

## Phase 2: 동적 쿼리 테스트 (핵심)

정해진 데이터셋이 아닌, **에이전트가 생성한 새로운 쿼리**로 테스트합니다.

### 2.1 페르소나 및 쿼리 선정

다음 페르소나 중 **3개를 무작위 선택**하고, 각 페르소나별로 **난이도별 쿼리를 1개씩(총 9개 이상)** 생성하세요.

**페르소나 목록**:
1. 🎓 **신입생** (비공식적 표현, 기초 정보: 휴학, 장학금)
2. 📚 **재학생** (졸업요건, 전과, 복수전공)
3. 🎓 **대학원생** (논문, 연구비, 조교)
4. 👨‍🏫 **교수** (연구년, 승진, 규정 해석)
5. 👔 **직원** (행정 절차, 예산, 감사)
6. 😡 **불만있는 구성원** (권리 주장, 민원성 질문)
7. 👪 **학부모** (등록금, 생활관)

**쿼리 난이도 구성**:
*   **쉬움 (30%)**: 단일 규정, 명확한 키워드 (예: "휴학 신청 기간")
*   **중간 (40%)**: 조건부 답변, 절차 (예: "장학금 받는데 휴학하면 어떻게 됨?")
*   **어려움 (30%)**: 모호함, 복합 질문, 감정적 (예: "돈이 없어서 학교 다니기 힘든데 어떡해")

### 2.2 쿼리 실행

선택한 각 쿼리에 대해 다음 명령을 실행하고 **결과를 기록**하세요.

```bash
uv run regulation search "<생성된_쿼리>" -a -n 5
```

---

## Phase 3: 답변 품질 심층 검토 (Rigorous Check)

**모든 동적 쿼리 답변에 대해 다음 과정을 수행해야 합니다. (생략 불가)**

### 3.1 팩트체크 루프 (필수)

답변에서 **핵심 주장 3가지**를 추출하고, 에이전트가 직접 터미널 명령으로 검증하세요.

**검증 매커니즘**:
1.  답변의 주장 식별 (예: "휴학은 수업일수 2/3선까지 가능하다")
2.  **검증 쿼리 실행**:
    ```bash
    uv run regulation search "휴학 수업일수 3분의 2" -n 3
    ```
3.  **대조 및 판정**: 검색된 규정 조항 원문과 답변이 일치하는지 확인.
    *   ✅ 일치: Pass
    *   ❌ 불일치/근거없음: **Fail (치명적 오류)**

### 3.2 품질 점수 산정

각 답변에 대해 5점 만점으로 평가합니다.

| 항목 | 기준 | 감점 요인 |
|------|------|----------|
| **정확성** (1.0) | 규정 일치 여부 | 팩트체크 실패 시 **-1.0** (0점) |
| **완전성** (1.0) | 질문의 모든 요소 답변 | 복합 질문 누락 시 **-0.5** |
| **관련성** (1.0) | 질문 의도 부합 | 동문서답 시 **-1.0** |
| **출처** (1.0) | 규정명/조항 번호 명시 | 출처 누락 시 **-1.0** |
| **실용성** (1.0) | 구체적 행동 지침(기간,서류) | 일반론만 나열 시 **-0.5** |

### 3.3 성공 여부 최종 판정
*   ✅ **성공**: 총점 **4.0 이상** AND 팩트체크 **All Pass**
*   ❌ **실패**: 총점 4.0 미만 OR 팩트체크 1건 이상 Fail

---

## Phase 4: 멀티턴 대화 테스트

단발성 질문이 아닌, 이어지는 대화 능력을 테스트합니다.

### 4.1 시나리오 실행

성공한 동적 쿼리 중 1개를 골라 꼬리에 꼬리를 무는 질문을 2회 더 수행합니다.

**실행 방법**:
```bash
uv run regulation  # 인터랙티브 모드 진입
```
(또는 `send_command_input` 도구 사용)

**시나리오 예시**:
1.  **Turn 1**: "휴학 신청 언제까지야?" (기본 질문)
2.  **Turn 2**: "그럼 필요한 서류는 뭐야?" (구체화)
3.  **Turn 3**: "질병 휴학도 똑같아?" (예외/조건 변경)

### 4.2 평가
*   **맥락 유지**: 앞선 대화의 문맥(휴학 등)을 유지하는가?
*   **정보 일관성**: 앞의 답변과 모순되지 않는가?

---

## Phase 5: 개선 및 피드백 적용

테스트에서 발견된 문제점(실패한 쿼리, 낮은 점수)을 해결합니다.

### 5.1 자동 개선 (scripts/auto_evaluate.py 결과)
Phase 1에서 생성된 `data/output/improvement_plan.json`을 확인하고 적용합니다.

```bash
cat data/output/improvement_plan.json
```
*   `intent`: `data/config/intents.json` 추가
*   `synonym`: `data/config/synonyms.json` 추가

### 5.2 수동 개선 (동적 테스트 결과)
Phase 2, 3, 4에서 발견한 실패 케이스(`WRONG_FACT`, `NO_SOURCE`, `IRRELEVANT`)에 대해 원인을 분석하고 수정합니다.

*   **검색 실패**: 핵심 키워드가 동의어 사전에 있나요? (`synonyms.json` 수정)
*   **의도 파악 실패**: Query Analyzer 패턴 보강 (`src/rag/infrastructure/query_analyzer.py`)
*   **답변 품질 저하**: 프롬프트 템플릿 검토 필요 가능성 (보고서에 기록)

### 5.3 변경사항 검증
수정 후 관련 단위 테스트를 실행합니다.
```bash
uv run pytest tests/rag/unit/infrastructure/test_query_analyzer.py
```

---

## Phase 6: 재평가 및 완료 보고

### 6.1 최종 검증
개선 사항 적용 후 Phase 1(정적 평가)을 다시 실행하여 점수가 향상되었는지, 회귀(Regression)는 없는지 확인합니다.

```bash
uv run python scripts/auto_evaluate.py --run
```

### 6.2 세션 요약 보고서 작성
다음 내용을 포함하여 Markdown 파일(`data/output/test_report_<날짜>.md`)로 저장합니다:

1.  **테스트 개요**: 실행 날짜, 선택된 페르소나
2.  **동적 테스트 결과**:
    *   실행한 쿼리 목록 및 난이도
    *   각 쿼리별 팩트체크 결과(성공/실패) 및 품질 점수
    *   최종 성공률 (목표 80% 달성 여부)
3.  **개선 내역**: 추가된 동의어, 인텐트, 코드 변경 사항
4.  **결론 및 제언**: 현재 시스템의 강점과 약점, 향후 개선 방향
